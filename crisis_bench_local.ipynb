{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e9ce422-6ab0-4409-8ac3-3a19d646c957",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "dcc470c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bdbb91-e226-4706-aff3-29d910ca910e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fba25f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "- `data/crisisbench/all_data_en`: all combined english dataset used for the experiments\n",
    "    - `crisis_consolidated_humanitarian_filtered_lang_en_dev.tsv`\n",
    "    - `crisis_consolidated_humanitarian_filtered_lang_en_test.tsv`\n",
    "    - `crisis_consolidated_humanitarian_filtered_lang_en_train.tsv`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a4e11d15",
   "metadata": {
    "id": "a4e11d15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train: N = 61164\n",
      "df_dev: N = 8935\n",
      "df_test: N = 17356\n"
     ]
    }
   ],
   "source": [
    "df = {}\n",
    "\n",
    "df[\"train\"] = pd.read_csv(\"./data/crisisbench/all_data_en/crisis_consolidated_humanitarian_filtered_lang_en_train.tsv\", sep=\"\\t\")\n",
    "print(f\"df_train: N = {len(df['train'])}\")\n",
    "df['dev'] = pd.read_csv(\"./data/crisisbench/all_data_en/crisis_consolidated_humanitarian_filtered_lang_en_dev.tsv\", sep=\"\\t\")\n",
    "print(f\"df_dev: N = {len(df['dev'])}\")\n",
    "df['test'] = pd.read_csv(\"./data/crisisbench/all_data_en/crisis_consolidated_humanitarian_filtered_lang_en_test.tsv\", sep=\"\\t\")\n",
    "print(f\"df_test: N = {len(df['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a2b97521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'affected_individual',\n",
       " 'caution_and_advice',\n",
       " 'displaced_and_evacuations',\n",
       " 'donation_and_volunteering',\n",
       " 'infrastructure_and_utilities_damage',\n",
       " 'injured_or_dead_people',\n",
       " 'missing_and_found_people',\n",
       " 'not_humanitarian',\n",
       " 'requests_or_needs',\n",
       " 'response_efforts',\n",
       " 'sympathy_and_support'}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_class_label = set(df[\"train\"][\"class_label\"])\n",
    "dev_class_label = set(df['dev'][\"class_label\"])\n",
    "test_class_label = set(df['test'][\"class_label\"])\n",
    "\n",
    "assert len(train_class_label) == len(dev_class_label) and  len(train_class_label) == len(test_class_label) \n",
    "\n",
    "train_class_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9dc13d",
   "metadata": {},
   "source": [
    "## Target Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd8381d",
   "metadata": {},
   "source": [
    "### Time-critical\n",
    "\n",
    "- 'affected_individual'\n",
    "  -  CrisisLexT26 (Affected individuals): Deaths, injuries, missing, found, or displaced `people`, and/or personal updates.\n",
    "  - examples \n",
    "    - Although one person confirmed  dead by police, BBC understands death toll at least three. #clutha #helicoptercrash htt…\n",
    "    - 4WABC-TV: FDNY confirms that there are fatalities in Metro North derailment. Other news outlets reporting 4 deaths.\n",
    "\n",
    "- 'caution_and_advice'\n",
    "  - CrisisLexT26 (Caution and advice) : If a message conveys/reports information about some `warning` or a piece of `advice` about a possible hazard of an incident.\n",
    "  - examples\n",
    "    - Be informed always. . . #RubyPH http://t.co/u1x521x0Is\n",
    "    - RT @ChileanProbs: 8.3 earthquake in the north of Chile! Tsunami alert up north, Peru and Ecuador!\n",
    "    - @JimFreund: Apparently we have no exclusivity.  The tornado watch is for all SE NY.  http://1.usa.gov/mSPGdf\ten\t1\tcaution_and_advice\n",
    "    - Japan issues tsunami alert after Chile quake, expecting no damage: Japan has issued a tsunami alert following ... http://t.co/GerjHpPaNN\n",
    "\n",
    "- 'displaced_and_evacuations'\n",
    "    - People who have relocated due to the crisis, even for a short time (includes evacuations)\n",
    "    - examples\n",
    "      - RT @rociolewis: @TheEllenShow Chile has gone through a recent earthquake and now a fire, thousands are homeless. Please spread the word foräó_\n",
    "      - RT @AnasMallick: More than 5 dozen #Earthquake victims, mostly women and children, shifted to #Karachi from #Awaran.\n",
    "      - Hurricane Odile hits Baja California - Click2Houston\n",
    "\n",
    "- 'infrastructure_and_utilities_damage'\n",
    "  - Houses, buildings, roads damaged or utilities such as water, electricity, interrupted\n",
    "  - Buildings or roads damaged or operational; utilities/services interrupted or restored\n",
    "  - Reports of damaged buildings, roads, bridges, or utilities/services interrupted or restored.\n",
    "\n",
    "- 'injured_or_dead_people'\n",
    "  - Reports of casualties and/or injured `people` due to the crisis.\n",
    "  - Injured and dead\n",
    "  - If a message reports the information about `casualties` or damage done by an incident.\n",
    "\n",
    "- 'missing_and_found_people'\n",
    "  - `Missing`, trapped, or found people—Questions and/or reports about missing or found people.\n",
    "  - People `missing` or found.\n",
    "  - If a message reports about the missing or found person effected by an incident or seen a celebrity visit on ground zero\n",
    "\n",
    "### Support and Relief\n",
    "\n",
    "- 'requests_or_needs'\n",
    "  - Needs of those affecte\n",
    "  - Something (e.g. food, water, shelter) or someone (e.g. volunteers, doctors) is needed\n",
    "  - examples\n",
    "    - These have warned that diphtheria, cholera and malaria could spread in an epidemic of \"apocalyptic proportions\" if medical, food, water and other types of aid are not allowed in, along with trained personnel to administer the support.\n",
    "\n",
    "- 'donation_and_volunteering'\n",
    "  - Reports of urgent needs or donations of shelter and/or supplies such as food, water, clothing, money, medical supplies or blood; and volunteering services\n",
    "  - Needs, requests, or offers of money, blood, shelter, supplies, and/or services by volunteers or professionals.\n",
    "  - Donations of money\n",
    "  - If a message speaks about money raised, donation offers, goods/services offered or asked by the victims of an incident.\n",
    "  - Donations of supplies and/or volunteer work\n",
    "  - Money requested, donated or spent\n",
    "  - Needs or donations of shelter and/or supplies such as food, water, clothing, medical supplies or blood\n",
    "  - Services needed or offered by volunteers or professionals\n",
    "  - examples\n",
    "    - \"You know me : I don't like giving away money. But Nepal needs our help. Donate to @decappeal today\"\n",
    "\n",
    "- 'response_efforts'\n",
    "  - Affected populations receiving food, water, shelter, medication, etc. from humanitarian/emergency response organizations\n",
    "  - All info about responders. Affected populations receiving food, water, shelter, medication, etc. from humanitarian/emergency response organizations.\n",
    "\n",
    "### Non-informative\n",
    "\n",
    "- 'not_humanitarian'\n",
    "  - Not applicable\n",
    "  -  Not related to this crisis\n",
    "  - Refers to the crisis, but does not contain useful information that helps you understand the situation; 2. Not related to the Typhoon, or not relevant for emergency/humanitarian response; 3. Related to the crisis, but not informative: if it refers to the crisis, but does not contain useful information that helps understand the situation.\n",
    "  - examples\n",
    "    - Had a long night. Time to sleep and rest for a while. I survived #RubyPH!\t\n",
    "    - #Baltimore is on fire and #Nepal death toll is rising....yet I still don't think people are paying attention\n",
    "    - A subtle pressure in the Force drew Jacen's attention to his aide, Orlopp. He turned to find the Jenet just looking up f\n",
    "    - IAF Planes Bring Back 546 Indians From Quake-hit Nepal | The New Indian Express http://t.co/8BPG5NCT2W | http://t.co/69mLhfefhr #AllTheNews\n",
    "    - HERO ALERT! please share á¼¼Dá½Š8âœ¨ https://t.co/UED0PojAPx #motorcycle https://t.co/6saBdgri4c\ten\tNA\tnot_humanitarian\n",
    "\n",
    "- 'sympathy_and_support'\n",
    "  - To hear about the state of Sardinia where I spent the majority of my summers, is extremely saddening. Hope they can get through it.#sardinia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a382b4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informative needed for handling urgent incidents \n",
    "time_critical = ['affected_individual', 'caution_and_advice', 'displaced_and_evacuations', 'infrastructure_and_utilities_damage', 'injured_or_dead_people', 'missing_and_found_people']\n",
    "\n",
    "# Helping the survivor\n",
    "support_and_relief = ['requests_or_needs', 'donation_and_volunteering', 'response_efforts']\n",
    " \n",
    "# Not solving the problem\n",
    "non_informative = ['not_humanitarian', 'sympathy_and_support']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "e8254f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {}\n",
    "for x in time_critical:\n",
    "    mapping[x] = 'time_critical'\n",
    "for x in support_and_relief:\n",
    "    mapping[x] = 'support_and_relief'\n",
    "for x in non_informative:\n",
    "    mapping[x] = 'non_informative'\n",
    "\n",
    "df[\"train\"]['class_label_group'] = df[\"train\"]['class_label'].map(mapping)\n",
    "df[\"dev\"]['class_label_group'] = df[\"dev\"]['class_label'].map(mapping)\n",
    "df[\"test\"]['class_label_group'] = df[\"test\"]['class_label'].map(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "ec189df7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'non_informative', 'support_and_relief', 'time_critical'}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_class_label = set(df[\"train\"][\"class_label_group\"])\n",
    "dev_class_label = set(df['dev'][\"class_label_group\"])\n",
    "test_class_label = set(df['test'][\"class_label_group\"])\n",
    "\n",
    "assert len(train_class_label) == len(dev_class_label) and  len(train_class_label) == len(test_class_label) \n",
    "\n",
    "train_class_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d54f6e",
   "metadata": {},
   "source": [
    "## Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb16e124",
   "metadata": {},
   "source": [
    "### Remove meaningless text\n",
    "\n",
    "\"Prior to the classification experiment, we preprocess tweets to remove symbols, emoticons, invisible and non-ASCII characters, punctuations (replaced with whitespace), numbers, URLs, and hashtag signs\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5233bc",
   "metadata": {},
   "source": [
    "####  URL removal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9138cc95",
   "metadata": {},
   "source": [
    "\"All URLs were removed from tweets, since the text of URL strings does not necessarily convey any relevant information, and can therefore be removed [39].\"\n",
    "\n",
    "- Roy, D.; Mitra, M.; Ganguly, D. To Clean or Not to Clean: Document Preprocessing and Reproducibility. J. Data Inf. Qual. (JDIQ)\n",
    "2018, 10, 18."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "95af60bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "God bless you... https://t.co/AnEy1ydkkz\n"
     ]
    }
   ],
   "source": [
    "print(df['train'].loc[1, 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "5889b37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in ['train', 'dev', 'test']:\n",
    "    df[d]['text'] = df[d]['text'].str.replace(r'http\\S+', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e738242e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "God bless you... \n"
     ]
    }
   ],
   "source": [
    "print(df['train'].loc[1, 'text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c67566",
   "metadata": {},
   "source": [
    "#### Remove hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "87ca43c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rescue effort expands in India, Pakistan as flood death toll tops 350   #india #asia\n",
      "RT @leanielsen: I hope everyone in Chile stays safe and are okay. Surrounding countries should watch out for the Tsunami alert. #PrayForChiäó_\n"
     ]
    }
   ],
   "source": [
    "print(df['train'].loc[4, 'text'])\n",
    "print(df['train'].loc[5, 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "d0560111",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in ['train', 'dev', 'test']:\n",
    "    df[d]['text'] = df[d]['text'].str.replace(r'#\\w+', '', regex=True).str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "c252d405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rescue effort expands in India, Pakistan as flood death toll tops 350\n",
      "RT @leanielsen: I hope everyone in Chile stays safe and are okay. Surrounding countries should watch out for the Tsunami alert.\n"
     ]
    }
   ],
   "source": [
    "print(df['train'].loc[4, 'text'])\n",
    "print(df['train'].loc[5, 'text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf46d3b",
   "metadata": {},
   "source": [
    "#### Remove username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "0b1145b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @perreaux: Cracked wine casks, damaged historical  buildings and coffee shops. This Napa earthquake is the biggest first world disaster â€¦\n",
      "I'm really just excited for new undies and pinkberry @mollymcnultzxo\n",
      "RT @leanielsen: I hope everyone in Chile stays safe and are okay. Surrounding countries should watch out for the Tsunami alert.\n"
     ]
    }
   ],
   "source": [
    "print(df['train'].loc[2, 'text'])\n",
    "print(df['train'].loc[3, 'text'])\n",
    "print(df['train'].loc[5, 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "5839cb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in ['train', 'dev', 'test']:\n",
    "    df[d]['text'] = df[d]['text'].str.replace(r'@\\w+', '', regex=True).str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "e2763353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT : Cracked wine casks, damaged historical  buildings and coffee shops. This Napa earthquake is the biggest first world disaster â€¦\n",
      "I'm really just excited for new undies and pinkberry\n",
      "RT : I hope everyone in Chile stays safe and are okay. Surrounding countries should watch out for the Tsunami alert.\n"
     ]
    }
   ],
   "source": [
    "print(df['train'].loc[2, 'text'])\n",
    "print(df['train'].loc[3, 'text'])\n",
    "print(df['train'].loc[5, 'text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bf12cf",
   "metadata": {},
   "source": [
    "#### Remove RT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "7e44189f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT : Cracked wine casks, damaged historical  buildings and coffee shops. This Napa earthquake is the biggest first world disaster â€¦\n",
      "RT : I hope everyone in Chile stays safe and are okay. Surrounding countries should watch out for the Tsunami alert.\n"
     ]
    }
   ],
   "source": [
    "print(df['train'].loc[2, 'text'])\n",
    "print(df['train'].loc[5, 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "0b994165",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in ['train', 'dev', 'test']:\n",
    "    df[d]['text'] = df[d]['text'].str.replace(r'\\bRT\\b', '', regex=True).str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "f2e07627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": Cracked wine casks, damaged historical  buildings and coffee shops. This Napa earthquake is the biggest first world disaster â€¦\n",
      ": I hope everyone in Chile stays safe and are okay. Surrounding countries should watch out for the Tsunami alert.\n"
     ]
    }
   ],
   "source": [
    "print(df['train'].loc[2, 'text'])\n",
    "print(df['train'].loc[5, 'text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c976af",
   "metadata": {},
   "source": [
    "#### Remove symbols, emoticons, invisible and non-ASCII characters, punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "207bf09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": Cracked wine casks, damaged historical  buildings and coffee shops. This Napa earthquake is the biggest first world disaster â€¦\n",
      "It���s a good thing that the government have done everything to avert any lost of lives from the onslaught of typhoon hagupit in the country.\n",
      "Hurricane Irma on collision course with Florida; 4 reported killed: 10 points\n",
      "News Corp Papers Compare The ABC To ISIS\n",
      "Traveling on Humanitarian Medical Mission to Puerto Rico ἟5἟7 hosted by\n",
      "Gym time!! Back to work!!\n",
      "STORMS A COMIN!!!!! I miss Fridays at your place.\n",
      "LIBTARDS RUIN EVERYTHING AND BLAME EVERYONE BUT THEMSELVES.\n",
      ": Found helicopters hovering above but none reached the ground for help where many are still waiting for food and shelter.â€¦\n"
     ]
    }
   ],
   "source": [
    "print(df['train'].loc[2, 'text'])\n",
    "print(df['train'].loc[7, 'text'])\n",
    "print(df['train'].loc[11, 'text'])\n",
    "print(df['train'].loc[12, 'text'])\n",
    "print(df['train'].loc[13, 'text'])\n",
    "print(df['train'].loc[14, 'text'])\n",
    "print(df['train'].loc[16, 'text'])\n",
    "print(df['train'].loc[17, 'text'])\n",
    "print(df['train'].loc[18, 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "4caf209d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_tweet(text):\n",
    "    # remove non-ASCII characters,\n",
    "    # 1) non-ASCII 제거 (이모티콘/특수문자/한글 등 모두 제거)\n",
    "    text = text.encode(\"ascii\", \"ignore\").decode()\n",
    "\n",
    "    # remove emoticons (e.g., :), :-D, XD)\n",
    "    text = re.sub(r'[:;=8xX][-~]?[)(DPpOo/\\\\]+', ' ', text)\n",
    "\n",
    "    # remove numbers\n",
    "    text = re.sub(r'\\d+', ' ', text)\n",
    "\n",
    "    # remove punctuations (replaced with whitespace)\n",
    "    text = re.sub(r'[.,!?;:/()\\\"\\'\\[\\]{}<>@#~`+=*&^%$|-]', ' ', text)\n",
    "\n",
    "    # remove invisible characters\n",
    "    text = re.sub(r'[\\u200B-\\u200D\\uFEFF]', '', text)\n",
    "\n",
    "    # remove duplicate spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "for d in ['train', 'dev', 'test']:\n",
    "    df[d]['text'] = df[d]['text'].apply(clean_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "6119ff3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cracked wine casks damaged historical buildings and coffee shops This Napa earthquake is the biggest first world disaster\n",
      "Its a good thing that the government have done everything to avert any lost of lives from the onslaught of typhoon hagupit in the country\n",
      "Hurricane Irma on collision course with Florida reported killed points\n",
      "News Corp Papers Compare The ABC ToISIS\n",
      "Traveling on Humanitarian Medical Mission to Puerto Rico hosted by\n",
      "Gym time Back to work\n",
      "STORMS A COMIN I miss Fridays at your place\n",
      "LIBTARDS RUIN EVERYTHING AND BLAME EVERYONE BUT THEMSELVES\n",
      "Found helicopters hovering above but none reached the ground for help where many are still waiting for food and shelter\n"
     ]
    }
   ],
   "source": [
    "print(df['train'].loc[2, 'text'])\n",
    "print(df['train'].loc[7, 'text'])\n",
    "print(df['train'].loc[11, 'text'])\n",
    "print(df['train'].loc[12, 'text'])\n",
    "print(df['train'].loc[13, 'text'])\n",
    "print(df['train'].loc[14, 'text'])\n",
    "print(df['train'].loc[16, 'text'])\n",
    "print(df['train'].loc[17, 'text'])\n",
    "print(df['train'].loc[18, 'text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02411611",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Text lowercasing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38675c13",
   "metadata": {},
   "source": [
    "All tweets were converted to lowercase; according to Hickman et al. [37], lowercasing tends to be beneficial because it reduces data dimensionality, thereby increasing statistical power, and usually does not reduce validity.\n",
    "\n",
    "- Hickman, L.; Thapa, S.; Tay, L.; Cao, M.; Srinivasan, P. Text Preprocessing for Text Mining in Organizational Research: Review\n",
    "and Recommendations. Organ. Res. Methods 2022, 25, 114–146."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "f5a48fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STORMS A COMIN I miss Fridays at your place\n",
      "LIBTARDS RUIN EVERYTHING AND BLAME EVERYONE BUT THEMSELVES\n"
     ]
    }
   ],
   "source": [
    "print(df['train'].loc[16, 'text'])\n",
    "print(df['train'].loc[17, 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "f74ca052",
   "metadata": {
    "executionInfo": {
     "elapsed": 153,
     "status": "ok",
     "timestamp": 1762294699522,
     "user": {
      "displayName": "Jade Kim",
      "userId": "13751362502685406790"
     },
     "user_tz": 300
    },
    "id": "f74ca052"
   },
   "outputs": [],
   "source": [
    "for d in ['train', 'dev', 'test']:\n",
    "    df[d]['text'] = df[d]['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "fbd795fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "storms a comin i miss fridays at your place\n",
      "libtards ruin everything and blame everyone but themselves\n"
     ]
    }
   ],
   "source": [
    "print(df['train'].loc[16, 'text'])\n",
    "print(df['train'].loc[17, 'text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c81a58e-77d1-4bc1-8a89-90b2a0bcf2a6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Remove Empty text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "4e22d7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 75\n",
      "dev 14\n",
      "test 21\n"
     ]
    }
   ],
   "source": [
    "for d in ['train', 'dev', 'test']:\n",
    "    mask = df[d][\"text\"].str.strip() == \"\"\n",
    "    count = mask.sum()\n",
    "    print(d, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "7c92b6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75 rows removed\n",
      "14 rows removed\n",
      "21 rows removed\n"
     ]
    }
   ],
   "source": [
    "for d in ['train', 'dev', 'test']:\n",
    "    prev_length = len(df[d])\n",
    "    df[d] = df[d][df[d][\"text\"].str.strip() != \"\"].reset_index(drop=True)\n",
    "    print(f\"{prev_length - len(df[d])} rows removed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61d56c3",
   "metadata": {},
   "source": [
    "## Save preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "ca060c6f-3eec-4b09-9a92-bac524b7b80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ./data/crisisbench/preprocessed_data_train.csv\n",
      "Saved: ./data/crisisbench/preprocessed_data_dev.csv\n",
      "Saved: ./data/crisisbench/preprocessed_data_test.csv\n"
     ]
    }
   ],
   "source": [
    "for d in ['train', 'dev', 'test']:\n",
    "    output_path = f\"./data/crisisbench/preprocessed_data_{d}.csv\"\n",
    "    df[d].to_csv(output_path, index=False)\n",
    "    print(\"Saved:\", output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a9ab23-f031-44a3-88cd-4d08f1f6381c",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733ec020-a4bb-430f-b561-1f0367e5f486",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "fc7346fe-d763-4501-9a45-1c4c7a9ab363",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "0869bbce-6ad5-4929-9439-c2f4bc6249f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "generator = torch.Generator()\n",
    "_ = generator.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "f83365a1-379c-4bc6-bbcc-eb8ca273209f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: ./data/crisisbench/preprocessed_data_train.csv\n",
      "Loading: ./data/crisisbench/preprocessed_data_dev.csv\n",
      "Loading: ./data/crisisbench/preprocessed_data_test.csv\n"
     ]
    }
   ],
   "source": [
    "def load_data():\n",
    "    df = {}\n",
    "    for d in ['train', 'dev', 'test']:\n",
    "        output_path = f\"./data/crisisbench/preprocessed_data_{d}.csv\"\n",
    "        df[d] = pd.read_csv(output_path).loc[:, ['text', 'class_label_group']]\n",
    "        print(\"Loading:\", output_path)\n",
    "    return df\n",
    "\n",
    "df = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "43775508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train: N=61164\n"
     ]
    }
   ],
   "source": [
    "df['train'].head()\n",
    "print(f\"df_train: N={len(df['train'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "12ce818d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_dev: N=8935\n"
     ]
    }
   ],
   "source": [
    "df['dev'].head()\n",
    "print(f\"df_dev: N={len(df['dev'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d208947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text   class_label_group\n",
      "0  staff at our feeding centre say chronic malnou...  support_and_relief\n",
      "1      you comin down for the summer semesters right     non_informative\n",
      "2         yea it s upstate i m like a few hours away     non_informative\n",
      "3  teach every pakistani that it is not enough to...     non_informative\n",
      "4  stay with for live cvg as typhoon hagupit slam...       time_critical\n",
      "df_test: N=17356\n"
     ]
    }
   ],
   "source": [
    "df['test'].head()\n",
    "print(f\"df_test: N={len(df['test'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736d27fb",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9ea0a0",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4a00974",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "from collections import Counter\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8566f4e",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "defa4f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MAX_VOCAB_SIZE = 20000\n",
    "MAX_SEQ_LEN = 64 # depends on tweet length\n",
    "EMBED_DIM = 50\n",
    "FILTER_SIZES = (3, 4, 5)\n",
    "NUM_FILTERS = 100\n",
    "DROPOUT = 0.5 # tune\n",
    "BATCH_SIZE = 64 # tune \n",
    "LR = 1e-3\n",
    "NUM_EPOCHS = 10\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "UNK_TOKEN = \"<unk>\"\n",
    "\n",
    "GLOVE_PATH = \"./data/crisisbench/glove_word_embeddings.txt\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fb1266",
   "metadata": {},
   "source": [
    "### Tokenizer and Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ecf53496",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_tokenize(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Splits on whitespace\n",
    "    \"\"\"\n",
    "    return text.strip().split()\n",
    "\n",
    "def build_vocab(\n",
    "    texts: List[str],\n",
    "    max_size: int,\n",
    "    min_freq: int = 1\n",
    ") -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Build a word -> index vocab from training texts.\n",
    "    Reserves index 0 for PAD and 1 for UNK.\n",
    "    \"\"\"\n",
    "    counter = Counter()\n",
    "    for text in texts:\n",
    "        tokens = simple_tokenize(text)\n",
    "        counter.update(tokens)\n",
    "\n",
    "    vocab = {PAD_TOKEN: 0, UNK_TOKEN: 1}\n",
    "    for word, freq in counter.most_common():\n",
    "        if freq < min_freq:\n",
    "            continue\n",
    "        if len(vocab) >= max_size:\n",
    "            break\n",
    "        vocab[word] = len(vocab)\n",
    "\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def encode_text(\n",
    "    text: str,\n",
    "    vocab: Dict[str, int],\n",
    "    max_len: int\n",
    ") -> List[int]:\n",
    "    tokens = simple_tokenize(text)\n",
    "    ids = [vocab.get(tok, vocab[UNK_TOKEN]) for tok in tokens][:max_len]\n",
    "    if len(ids) < max_len:\n",
    "        ids += [vocab[PAD_TOKEN]] * (max_len - len(ids))\n",
    "    return ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa81ae81",
   "metadata": {},
   "source": [
    "### Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c95f7d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        texts: List[str],\n",
    "        labels: List[int],\n",
    "        vocab: Dict[str, int],\n",
    "        max_len: int,\n",
    "    ):\n",
    "        assert len(texts) == len(labels)\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int]:\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        input_ids = encode_text(text, self.vocab, self.max_len)\n",
    "        return torch.tensor(input_ids, dtype=torch.long), label\n",
    "\n",
    "\n",
    "def create_dataloaders(\n",
    "    train_texts: List[str],\n",
    "    train_labels: List[int],\n",
    "    val_texts: List[str],\n",
    "    val_labels: List[int],\n",
    "    max_vocab_size: int,\n",
    "    max_seq_len: int,\n",
    "    batch_size: int,\n",
    ") -> Tuple[DataLoader, DataLoader, Dict[str, int], int]:\n",
    "    vocab = build_vocab(train_texts, max_vocab_size)\n",
    "    num_classes = len(set(train_labels))\n",
    "\n",
    "    train_dataset = TextDataset(train_texts, train_labels, vocab, max_seq_len)\n",
    "    val_dataset = TextDataset(val_texts, val_labels, vocab, max_seq_len)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, vocab, num_classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d02775",
   "metadata": {},
   "source": [
    "### Load GloVe & build embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1645084f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embeddings(\n",
    "    glove_path: str,\n",
    "    embed_dim: int,\n",
    ") -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Load GloVe file into a dict: word -> vector (torch.Tensor).\n",
    "    Expects each line: word val1 val2 ... valD\n",
    "    \"\"\"\n",
    "    embeddings = {}\n",
    "    with open(glove_path, \"r\", encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) != embed_dim + 1:\n",
    "                # ignore malformed lines\n",
    "                continue\n",
    "            word = parts[0]\n",
    "            vec = torch.tensor([float(x) for x in parts[1:]], dtype=torch.float32)\n",
    "            embeddings[word] = vec\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def build_embedding_matrix(\n",
    "    vocab: Dict[str, int],\n",
    "    glove_embeddings: Dict[str, torch.Tensor],\n",
    "    embed_dim: int,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create an embedding matrix of shape [vocab_size, embed_dim]\n",
    "    where row i is the vector for the word with index i.\n",
    "    Words not found in GloVe are randomly initialized (small normal).\n",
    "    \"\"\"\n",
    "    vocab_size = len(vocab)\n",
    "    embedding_matrix = torch.empty(vocab_size, embed_dim, dtype=torch.float32)\n",
    "\n",
    "    # Initialize OOV embeddings to small random values\n",
    "    torch.nn.init.normal_(embedding_matrix, mean=0.0, std=0.05)\n",
    "\n",
    "    # Set PAD embedding to zeros\n",
    "    pad_idx = vocab[PAD_TOKEN]\n",
    "    embedding_matrix[pad_idx] = torch.zeros(embed_dim, dtype=torch.float32)\n",
    "\n",
    "    oov_count = 0\n",
    "    for word, idx in vocab.items():\n",
    "        if word in (PAD_TOKEN, UNK_TOKEN):\n",
    "            continue\n",
    "        vec = glove_embeddings.get(word)\n",
    "        if vec is not None:\n",
    "            embedding_matrix[idx] = vec\n",
    "        else:\n",
    "            oov_count += 1\n",
    "\n",
    "    print(f\"GloVe OOV words: {oov_count}/{vocab_size}\")\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60863f21",
   "metadata": {},
   "source": [
    "### Text CNN model (with optional pretrained embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0f94c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        embed_dim: int,\n",
    "        num_classes: int,\n",
    "        pad_idx: int = 0,\n",
    "        num_filters: int = 100,\n",
    "        filter_sizes: Tuple[int, ...] = (3, 4, 5),\n",
    "        dropout: float = 0.5,\n",
    "        pretrained_embeddings: torch.Tensor | None = None,\n",
    "        freeze_embeddings: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embed_dim,\n",
    "            padding_idx=pad_idx,\n",
    "        )\n",
    "\n",
    "        if pretrained_embeddings is not None:\n",
    "            if pretrained_embeddings.shape != (vocab_size, embed_dim):\n",
    "                raise ValueError(\n",
    "                    f\"Pretrained embeddings shape {pretrained_embeddings.shape} \"\n",
    "                    f\"does not match (vocab_size, embed_dim)=({vocab_size}, {embed_dim})\"\n",
    "                )\n",
    "            self.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "            if freeze_embeddings:\n",
    "                self.embedding.weight.requires_grad = False\n",
    "\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(\n",
    "                in_channels=embed_dim,\n",
    "                out_channels=num_filters,\n",
    "                kernel_size=fs,\n",
    "            )\n",
    "            for fs in filter_sizes\n",
    "        ])\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(num_filters * len(filter_sizes), num_classes)\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        embedded = self.embedding(input_ids)          # [B, L, D]\n",
    "        embedded = embedded.transpose(1, 2)           # [B, D, L]\n",
    "\n",
    "        conv_outputs = []\n",
    "        for conv in self.convs:\n",
    "            x = conv(embedded)                        # [B, F, L']\n",
    "            x = F.relu(x)\n",
    "            x = F.max_pool1d(x, x.size(2)).squeeze(2) # [B, F]\n",
    "            conv_outputs.append(x)\n",
    "\n",
    "        cat = torch.cat(conv_outputs, dim=1)          # [B, F * len(filter_sizes)]\n",
    "        cat = self.dropout(cat)\n",
    "        logits = self.fc(cat)                         # [B, num_classes]\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93543e0d",
   "metadata": {},
   "source": [
    "### Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1baee17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    criterion: nn.Module,\n",
    "    device: torch.device,\n",
    ") -> Tuple[float, float]:\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for input_ids, labels in dataloader:\n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * input_ids.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total\n",
    "    accuracy = correct / total if total > 0 else 0.0\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    device: torch.device,\n",
    ") -> Tuple[float, float]:\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_ids, labels in dataloader:\n",
    "            input_ids = input_ids.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            logits = model(input_ids)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            total_loss += loss.item() * input_ids.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total\n",
    "    accuracy = correct / total if total > 0 else 0.0\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67776a78",
   "metadata": {},
   "source": [
    "### Main CNN Train Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "99044bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 20000, Num classes: 3\n",
      "Loading GloVe embeddings...\n",
      "GloVe OOV words: 882/20000\n",
      "Epoch 01 | Train Loss: 0.4595, Train Acc: 0.8258 | Val Loss: 0.3805, Val Acc: 0.8552\n",
      "Epoch 02 | Train Loss: 0.3572, Train Acc: 0.8675 | Val Loss: 0.3513, Val Acc: 0.8696\n",
      "Epoch 03 | Train Loss: 0.3088, Train Acc: 0.8865 | Val Loss: 0.3488, Val Acc: 0.8734\n",
      "Epoch 04 | Train Loss: 0.2718, Train Acc: 0.9027 | Val Loss: 0.3552, Val Acc: 0.8731\n",
      "Epoch 05 | Train Loss: 0.2348, Train Acc: 0.9148 | Val Loss: 0.3815, Val Acc: 0.8666\n",
      "Epoch 06 | Train Loss: 0.2045, Train Acc: 0.9268 | Val Loss: 0.3986, Val Acc: 0.8658\n",
      "Epoch 07 | Train Loss: 0.1774, Train Acc: 0.9370 | Val Loss: 0.4382, Val Acc: 0.8646\n",
      "Epoch 08 | Train Loss: 0.1547, Train Acc: 0.9453 | Val Loss: 0.4868, Val Acc: 0.8606\n",
      "Epoch 09 | Train Loss: 0.1365, Train Acc: 0.9517 | Val Loss: 0.5132, Val Acc: 0.8562\n",
      "Epoch 10 | Train Loss: 0.1208, Train Acc: 0.9579 | Val Loss: 0.5814, Val Acc: 0.8569\n",
      "Best validation accuracy: 0.8734\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_df = df['train'].dropna(subset=['text'])\n",
    "train_texts = train_df['text'].tolist()\n",
    "train_label_strs = train_df['class_label_group']\n",
    "\n",
    "val_df = df['dev'].dropna(subset=['text'])\n",
    "val_texts = val_df['text'].tolist()\n",
    "val_label_strs = val_df['class_label_group']\n",
    "\n",
    "all_label_strs = sorted(set(train_label_strs) | set(val_label_strs))\n",
    "label2id = {label: i for i, label in enumerate(all_label_strs)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "train_labels = [label2id[l] for l in train_label_strs]\n",
    "val_labels   = [label2id[l] for l in val_label_strs]\n",
    "# Create loaders and vocab\n",
    "train_loader, val_loader, vocab, num_classes = create_dataloaders(\n",
    "    train_texts=train_texts,\n",
    "    train_labels=train_labels,\n",
    "    val_texts=val_texts,\n",
    "    val_labels=val_labels,\n",
    "    max_vocab_size=MAX_VOCAB_SIZE,\n",
    "    max_seq_len=MAX_SEQ_LEN,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "\n",
    "print(f\"Vocab size: {len(vocab)}, Num classes: {num_classes}\")\n",
    "\n",
    "# Load GloVe embeddings\n",
    "print(\"Loading GloVe embeddings...\")\n",
    "glove_embeds = load_glove_embeddings(GLOVE_PATH, EMBED_DIM)\n",
    "embedding_matrix = build_embedding_matrix(vocab, glove_embeds, EMBED_DIM)\n",
    "\n",
    "# Initialize model with pretrained embeddings\n",
    "print(\"Model Initialization...\")\n",
    "model = TextCNN(\n",
    "    vocab_size=len(vocab),\n",
    "    embed_dim=EMBED_DIM,\n",
    "    num_classes=num_classes,\n",
    "    pad_idx=vocab[PAD_TOKEN],\n",
    "    num_filters=NUM_FILTERS,\n",
    "    filter_sizes=FILTER_SIZES,\n",
    "    dropout=DROPOUT,\n",
    "    pretrained_embeddings=embedding_matrix,\n",
    "    freeze_embeddings=False,   # set True if you want to freeze GloVe\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "best_val_acc = 0.0\n",
    "print(\"Training...\")\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    train_loss, train_acc = train_one_epoch(\n",
    "        model, train_loader, optimizer, criterion, device\n",
    "    )\n",
    "    val_loss, val_acc = evaluate(\n",
    "        model, val_loader, criterion, device\n",
    "    )\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"best_textcnn_glove.pt\")\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch:02d} | \"\n",
    "        f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "        f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\"\n",
    "    )\n",
    "\n",
    "print(f\"Best validation accuracy: {best_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774b973a",
   "metadata": {},
   "source": [
    "### Calculate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f1fdb0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics:\n",
      "accuracy: 0.8734\n",
      "precision: 0.8467\n",
      "recall: 0.8108\n",
      "f1: 0.8272\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Helper to get predictions + labels from a DataLoader\n",
    "def get_all_preds_and_labels(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    device: torch.device,\n",
    "):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_ids, labels in dataloader:\n",
    "            input_ids = input_ids.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            logits = model(input_ids)          # [B, num_classes]\n",
    "            preds = logits.argmax(dim=1)      # [B]\n",
    "\n",
    "            all_preds.extend(preds.cpu().tolist())\n",
    "            all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "    return all_preds, all_labels\n",
    "\n",
    "# compute accuracy, precision, recall, F1 (macro)\n",
    "def compute_classification_metrics(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    device: torch.device,\n",
    "    average: str = \"macro\",   # \"macro\", \"micro\", or \"weighted\"\n",
    "):\n",
    "    preds, labels = get_all_preds_and_labels(model, dataloader, device)\n",
    "\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels,\n",
    "        preds,\n",
    "        average=average,\n",
    "        zero_division=0,\n",
    "    )\n",
    "\n",
    "    metrics = {\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "model.load_state_dict(torch.load(\"best_textcnn_glove.pt\", map_location=device))\n",
    "\n",
    "# For validation metrics\n",
    "val_metrics = compute_classification_metrics(model, val_loader, device, average=\"macro\")\n",
    "print(\"Validation metrics:\")\n",
    "for k, v in val_metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f946d83a",
   "metadata": {},
   "source": [
    "## Transformer (Deberta-V3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a34723",
   "metadata": {},
   "source": [
    "### Build Label Mappings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd741eb",
   "metadata": {},
   "source": [
    "convert class_label_group values from string to int (0,1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "e3f3253c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original labels=['non_informative', 'support_and_relief', 'time_critical']\n",
      "original labels=['non_informative', 'support_and_relief', 'time_critical']\n",
      "original labels=['non_informative', 'support_and_relief', 'time_critical']\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "texts = {}\n",
    "labels = {}\n",
    "\n",
    "for d in [\"train\", \"dev\", \"test\"]:\n",
    "    df_tmp = df['train'].dropna(subset=['text'])\n",
    "    texts[d] = df_tmp['text'].tolist()\n",
    "    label_strs = df_tmp['class_label_group']\n",
    "\n",
    "    all_label_strs = sorted(set(label_strs))\n",
    "    print(f\"original labels={all_label_strs}\")\n",
    "    label2id = {label: i for i, label in enumerate(all_label_strs)}\n",
    "    id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "    labels[d] = [label2id[l] for l in label_strs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "8e9e478e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2}\n",
      "{0, 1, 2}\n",
      "{0, 1, 2}\n"
     ]
    }
   ],
   "source": [
    "for d in [\"train\", \"dev\", \"test\"]:\n",
    "    print(set(labels[d]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8657a561",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42a1cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"microsoft/deberta-v3-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "max_length = 64 # adjust based on the maximum input size?\n",
    "\n",
    "# 1. Tokenize directly (no map, no progress bars)\n",
    "train_encodings = tokenizer(\n",
    "    texts['train'],\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=max_length,\n",
    ")\n",
    "\n",
    "val_encodings = tokenizer(\n",
    "    texts['dev'],\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=max_length,\n",
    ")\n",
    "\n",
    "# 2. Build HF Datasets from encoded inputs + labels\n",
    "train_dataset = Dataset.from_dict({\n",
    "    \"input_ids\": train_encodings[\"input_ids\"],\n",
    "    \"attention_mask\": train_encodings[\"attention_mask\"],\n",
    "    \"label\": train_labels,\n",
    "})\n",
    "\n",
    "val_dataset = Dataset.from_dict({\n",
    "    \"input_ids\": val_encodings[\"input_ids\"],\n",
    "    \"attention_mask\": val_encodings[\"attention_mask\"],\n",
    "    \"label\": val_labels,\n",
    "})\n",
    "\n",
    "tokenized_datasets = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": val_dataset,\n",
    "})\n",
    "\n",
    "# 3. Set format for PyTorch\n",
    "tokenized_datasets.set_format(\n",
    "    type=\"torch\",\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"label\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26156b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check appropriate token size\n",
    "tmp_train = tokenizer(texts['train'], truncation=False, add_special_tokens=True)\n",
    "lens_train = [len(ids) for ids in tmp_train[\"input_ids\"]]\n",
    "\n",
    "tmp_dev = tokenizer(texts['dev'], truncation=False, add_special_tokens=True)\n",
    "lens_dev = [len(ids) for ids in tmp_dev[\"input_ids\"]]\n",
    "\n",
    "lengths = lens_train + lens_dev\n",
    "\n",
    "print(\"median:\", np.median(lengths))\n",
    "print(\"mean:\", np.mean(lengths))\n",
    "print(\"95th percentile:\", np.percentile(lengths, 95))\n",
    "print(\"99th percentile:\", np.percentile(lengths, 99))\n",
    "print(\"max:\", np.max(lengths))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c52b5f1",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0f71a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(label2id),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels,\n",
    "        preds,\n",
    "        average=\"macro\",\n",
    "        zero_division=0,\n",
    "    )\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "    }\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./deberta-v3-crisis\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "eval_results = trainer.evaluate(tokenized_datasets[\"validation\"])\n",
    "print(eval_results)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V6E1",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "crisisbench-dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
