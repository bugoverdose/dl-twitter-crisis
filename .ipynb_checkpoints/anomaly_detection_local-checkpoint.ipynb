{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e9ce422-6ab0-4409-8ac3-3a19d646c957",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dcc470c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bdbb91-e226-4706-aff3-29d910ca910e",
   "metadata": {},
   "source": [
    "## Cleanse raw dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca11d51",
   "metadata": {},
   "source": [
    "\n",
    "Dataset: [Turkey and Syria Earthquake Tweets](https://www.kaggle.com/datasets/swaptr/turkey-earthquake-tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fba25f",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a4e11d15",
   "metadata": {
    "id": "a4e11d15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 189626\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>like_count</th>\n",
       "      <th>rt_count</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>isVerified</th>\n",
       "      <th>language</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>place</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-21 03:29:07+00:00</td>\n",
       "      <td>New search &amp;amp; rescue work is in progress in...</td>\n",
       "      <td>['Hatay', 'earthquakes', 'Türkiye', 'TurkiyeQu...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5697.0</td>\n",
       "      <td>True</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Twitter Web App</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-21 03:29:04+00:00</td>\n",
       "      <td>Can't imagine those who still haven't recovere...</td>\n",
       "      <td>['Turkey', 'earthquake', 'turkeyearthquake2023...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Twitter for Android</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-02-21 03:28:06+00:00</td>\n",
       "      <td>its a highkey sign for all of us to ponder ove...</td>\n",
       "      <td>['turkeyearthquake2023', 'earthquake', 'Syria']</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Twitter for Android</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>2023-02-21 03:27:27+00:00</td>\n",
       "      <td>See how strong was the #Earthquake of Feb 20, ...</td>\n",
       "      <td>['Earthquake', 'Hatay', 'Turkey', 'turkeyearth...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21836.0</td>\n",
       "      <td>True</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Twitter for Android</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>2023-02-21 03:27:11+00:00</td>\n",
       "      <td>More difficult news today on top of struggles ...</td>\n",
       "      <td>['Türkiye', 'Syria', 'earthquake', 'Canadians']</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>675.0</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                       date  \\\n",
       "0           1  2023-02-21 03:29:07+00:00   \n",
       "1           2  2023-02-21 03:29:04+00:00   \n",
       "2           3  2023-02-21 03:28:06+00:00   \n",
       "3           5  2023-02-21 03:27:27+00:00   \n",
       "4           6  2023-02-21 03:27:11+00:00   \n",
       "\n",
       "                                             content  \\\n",
       "0  New search &amp; rescue work is in progress in...   \n",
       "1  Can't imagine those who still haven't recovere...   \n",
       "2  its a highkey sign for all of us to ponder ove...   \n",
       "3  See how strong was the #Earthquake of Feb 20, ...   \n",
       "4  More difficult news today on top of struggles ...   \n",
       "\n",
       "                                            hashtags  like_count  rt_count  \\\n",
       "0  ['Hatay', 'earthquakes', 'Türkiye', 'TurkiyeQu...         1.0       0.0   \n",
       "1  ['Turkey', 'earthquake', 'turkeyearthquake2023...         0.0       0.0   \n",
       "2    ['turkeyearthquake2023', 'earthquake', 'Syria']         0.0       0.0   \n",
       "3  ['Earthquake', 'Hatay', 'Turkey', 'turkeyearth...         0.0       0.0   \n",
       "4    ['Türkiye', 'Syria', 'earthquake', 'Canadians']         1.0       0.0   \n",
       "\n",
       "   followers_count  isVerified language coordinates place               source  \n",
       "0           5697.0        True       en         NaN   NaN      Twitter Web App  \n",
       "1              1.0       False       en         NaN   NaN  Twitter for Android  \n",
       "2              3.0       False       en         NaN   NaN  Twitter for Android  \n",
       "3          21836.0        True       en         NaN   NaN  Twitter for Android  \n",
       "4            675.0       False       en         NaN   NaN   Twitter for iPhone  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # NOTE: unzip ./data/turkey_syria_earthquake_tweets/archive.zip before running the below code. `tweets.csv` file is too big.\n",
    "# full_data = pd.read_csv(\"./data/turkey_syria_earthquake_tweets/tweets.csv\")\n",
    "# full_data.head()\n",
    "\n",
    "# # Invalid data\n",
    "# # NOTE: full_data.iloc[:, 6] contains NaN\n",
    "# # DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
    "# print(full_data.iloc[:, 6].apply(type).value_counts())\n",
    "# mask = full_data['isVerified'].apply(lambda v: isinstance(v, float))\n",
    "# print(full_data.loc[mask, 'isVerified'])\n",
    "\n",
    "df = pd.read_csv(\"./data/turkey_syria_earthquake_tweets/tweets_en.csv\")\n",
    "print(f\"N = {len(df)}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a2b97521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New search &amp; rescue work is in progress in #Hatay after two more #earthquakes hit #Türkiye’s southeastern province.  #TurkiyeQuakes #Turkey-#Syria  #Earthquake #turkeyearthquake2023  https://t.co/sd4WHByiQs\n"
     ]
    }
   ],
   "source": [
    "print(df.loc[0, 'content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02411611",
   "metadata": {},
   "source": [
    "### Text lowercasing\n",
    "\n",
    "All tweets were converted to lowercase; according to Hickman et al. [37], lowercasing tends to be beneficial because it reduces data dimensionality, thereby increasing statistical power, and usually does not reduce validity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f74ca052",
   "metadata": {
    "executionInfo": {
     "elapsed": 153,
     "status": "ok",
     "timestamp": 1762294699522,
     "user": {
      "displayName": "Jade Kim",
      "userId": "13751362502685406790"
     },
     "user_tz": 300
    },
    "id": "f74ca052"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new search &amp; rescue work is in progress in #hatay after two more #earthquakes hit #türkiye’s southeastern province.  #turkiyequakes #turkey-#syria  #earthquake #turkeyearthquake2023  https://t.co/sd4whbyiqs\n"
     ]
    }
   ],
   "source": [
    "df['content'] = df['content'].str.lower()\n",
    "print(df.loc[0, 'content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162319cc",
   "metadata": {},
   "source": [
    "### Stop word removal\n",
    "\n",
    "Stop word removal was useful in traditional NLP models, but not so effective for DL models. \n",
    "\n",
    "The original paper compared the traditional NLP models with DL models so they used it. \n",
    "\n",
    "\"common English (function) words such as “and”, “is”, “I”, “am”, “what”, “of”, etc. were removed by using the Natural Language Toolkit (NLTK). Stop word removal has the advantages of reducing the size of the stored dataset and improving the overall efficiency and effectiveness of the analysis [38].\"\n",
    "\n",
    "However, **we are only training DL models in our analysis so we are not applying it**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5800ed7d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21153,
     "status": "ok",
     "timestamp": 1762294724071,
     "user": {
      "displayName": "Jade Kim",
      "userId": "13751362502685406790"
     },
     "user_tz": 300
    },
    "id": "5800ed7d",
    "outputId": "d3dc1c32-711a-4e81-d3c0-d607c1c5439d"
   },
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.tokenize import word_tokenize\n",
    "\n",
    "# NLTK_LOCAL_PATH = \"./nltk_data\"\n",
    "# os.makedirs(NLTK_LOCAL_PATH, exist_ok=True)\n",
    "# nltk.data.path.append(NLTK_LOCAL_PATH)\n",
    "# # nltk.download('stopwords', download_dir=NLTK_LOCAL_PATH)\n",
    "# # nltk.download('punkt', download_dir=NLTK_LOCAL_PATH)\n",
    "\n",
    "# # Get English stop words\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# # Function to remove stop words from text\n",
    "# def remove_stopwords(text):\n",
    "#     if pd.isna(text):\n",
    "#         return text\n",
    "#     # Tokenize the text\n",
    "#     words = word_tokenize(text)\n",
    "#     # Remove stop words and return as string\n",
    "#     filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "#     return ' '.join(filtered_words)\n",
    "\n",
    "# df['content'] = df['content'].apply(remove_stopwords)\n",
    "# print(df.loc[0, 'content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fed112",
   "metadata": {},
   "source": [
    "### URLs removal\n",
    "\n",
    "All URLs were removed from tweets, since the text of URL strings does not necessarily convey any relevant information,  and can therefore be removed [39]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "71530407",
   "metadata": {
    "executionInfo": {
     "elapsed": 142,
     "status": "ok",
     "timestamp": 1762294724216,
     "user": {
      "displayName": "Jade Kim",
      "userId": "13751362502685406790"
     },
     "user_tz": 300
    },
    "id": "71530407"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new search &amp; rescue work is in progress in #hatay after two more #earthquakes hit #türkiye’s southeastern province.  #turkiyequakes #turkey-#syria  #earthquake #turkeyearthquake2023  \n"
     ]
    }
   ],
   "source": [
    "df['content'] = df['content'].str.replace(r'http\\S+', '', regex=True)\n",
    "print(df.loc[0, 'content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f64f950",
   "metadata": {},
   "source": [
    "### Duplicate removal\n",
    "\n",
    "All duplicate tweets were removed to eliminate redundancy and possible skewing of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6f7b55b3",
   "metadata": {
    "executionInfo": {
     "elapsed": 81,
     "status": "ok",
     "timestamp": 1762294724300,
     "user": {
      "displayName": "Jade Kim",
      "userId": "13751362502685406790"
     },
     "user_tz": 300
    },
    "id": "6f7b55b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 189626\n",
      "N = 180915\n"
     ]
    }
   ],
   "source": [
    "print(f\"N = {len(df)}\")\n",
    "df = df.drop_duplicates(subset='content', keep='first')\n",
    "print(f\"N = {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6b4793",
   "metadata": {},
   "source": [
    "### Remove unncessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "73dfe962-df76-4e72-bd53-225e7be2a22a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>like_count</th>\n",
       "      <th>rt_count</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>isVerified</th>\n",
       "      <th>language</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>place</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-02-21 03:29:07+00:00</td>\n",
       "      <td>new search &amp;amp; rescue work is in progress in...</td>\n",
       "      <td>['Hatay', 'earthquakes', 'Türkiye', 'TurkiyeQu...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5697.0</td>\n",
       "      <td>True</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Twitter Web App</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-02-21 03:29:04+00:00</td>\n",
       "      <td>can't imagine those who still haven't recovere...</td>\n",
       "      <td>['Turkey', 'earthquake', 'turkeyearthquake2023...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Twitter for Android</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-02-21 03:28:06+00:00</td>\n",
       "      <td>its a highkey sign for all of us to ponder ove...</td>\n",
       "      <td>['turkeyearthquake2023', 'earthquake', 'Syria']</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Twitter for Android</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-02-21 03:27:27+00:00</td>\n",
       "      <td>see how strong was the #earthquake of feb 20, ...</td>\n",
       "      <td>['Earthquake', 'Hatay', 'Turkey', 'turkeyearth...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21836.0</td>\n",
       "      <td>True</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Twitter for Android</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-02-21 03:27:11+00:00</td>\n",
       "      <td>more difficult news today on top of struggles ...</td>\n",
       "      <td>['Türkiye', 'Syria', 'earthquake', 'Canadians']</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>675.0</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        date  \\\n",
       "0  2023-02-21 03:29:07+00:00   \n",
       "1  2023-02-21 03:29:04+00:00   \n",
       "2  2023-02-21 03:28:06+00:00   \n",
       "3  2023-02-21 03:27:27+00:00   \n",
       "4  2023-02-21 03:27:11+00:00   \n",
       "\n",
       "                                             content  \\\n",
       "0  new search &amp; rescue work is in progress in...   \n",
       "1  can't imagine those who still haven't recovere...   \n",
       "2  its a highkey sign for all of us to ponder ove...   \n",
       "3  see how strong was the #earthquake of feb 20, ...   \n",
       "4  more difficult news today on top of struggles ...   \n",
       "\n",
       "                                            hashtags  like_count  rt_count  \\\n",
       "0  ['Hatay', 'earthquakes', 'Türkiye', 'TurkiyeQu...         1.0       0.0   \n",
       "1  ['Turkey', 'earthquake', 'turkeyearthquake2023...         0.0       0.0   \n",
       "2    ['turkeyearthquake2023', 'earthquake', 'Syria']         0.0       0.0   \n",
       "3  ['Earthquake', 'Hatay', 'Turkey', 'turkeyearth...         0.0       0.0   \n",
       "4    ['Türkiye', 'Syria', 'earthquake', 'Canadians']         1.0       0.0   \n",
       "\n",
       "   followers_count  isVerified language coordinates place               source  \n",
       "0           5697.0        True       en         NaN   NaN      Twitter Web App  \n",
       "1              1.0       False       en         NaN   NaN  Twitter for Android  \n",
       "2              3.0       False       en         NaN   NaN  Twitter for Android  \n",
       "3          21836.0        True       en         NaN   NaN  Twitter for Android  \n",
       "4            675.0       False       en         NaN   NaN   Twitter for iPhone  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(columns=['Unnamed: 0'], errors='ignore')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27389518-1c56-4901-8cca-d5b16675cde2",
   "metadata": {},
   "source": [
    "#### Exclude location info\n",
    "\n",
    "96% of the tweets lacked geolocation, drop if exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5866eb9e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1762294724336,
     "user": {
      "displayName": "Jade Kim",
      "userId": "13751362502685406790"
     },
     "user_tz": 300
    },
    "id": "5866eb9e",
    "outputId": "254ef374-e187-4e3e-db31-5541da746211"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>like_count</th>\n",
       "      <th>rt_count</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>isVerified</th>\n",
       "      <th>language</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-02-21 03:29:07+00:00</td>\n",
       "      <td>new search &amp;amp; rescue work is in progress in...</td>\n",
       "      <td>['Hatay', 'earthquakes', 'Türkiye', 'TurkiyeQu...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5697.0</td>\n",
       "      <td>True</td>\n",
       "      <td>en</td>\n",
       "      <td>Twitter Web App</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-02-21 03:29:04+00:00</td>\n",
       "      <td>can't imagine those who still haven't recovere...</td>\n",
       "      <td>['Turkey', 'earthquake', 'turkeyearthquake2023...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>Twitter for Android</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-02-21 03:28:06+00:00</td>\n",
       "      <td>its a highkey sign for all of us to ponder ove...</td>\n",
       "      <td>['turkeyearthquake2023', 'earthquake', 'Syria']</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>Twitter for Android</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-02-21 03:27:27+00:00</td>\n",
       "      <td>see how strong was the #earthquake of feb 20, ...</td>\n",
       "      <td>['Earthquake', 'Hatay', 'Turkey', 'turkeyearth...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21836.0</td>\n",
       "      <td>True</td>\n",
       "      <td>en</td>\n",
       "      <td>Twitter for Android</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-02-21 03:27:11+00:00</td>\n",
       "      <td>more difficult news today on top of struggles ...</td>\n",
       "      <td>['Türkiye', 'Syria', 'earthquake', 'Canadians']</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>675.0</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        date  \\\n",
       "0  2023-02-21 03:29:07+00:00   \n",
       "1  2023-02-21 03:29:04+00:00   \n",
       "2  2023-02-21 03:28:06+00:00   \n",
       "3  2023-02-21 03:27:27+00:00   \n",
       "4  2023-02-21 03:27:11+00:00   \n",
       "\n",
       "                                             content  \\\n",
       "0  new search &amp; rescue work is in progress in...   \n",
       "1  can't imagine those who still haven't recovere...   \n",
       "2  its a highkey sign for all of us to ponder ove...   \n",
       "3  see how strong was the #earthquake of feb 20, ...   \n",
       "4  more difficult news today on top of struggles ...   \n",
       "\n",
       "                                            hashtags  like_count  rt_count  \\\n",
       "0  ['Hatay', 'earthquakes', 'Türkiye', 'TurkiyeQu...         1.0       0.0   \n",
       "1  ['Turkey', 'earthquake', 'turkeyearthquake2023...         0.0       0.0   \n",
       "2    ['turkeyearthquake2023', 'earthquake', 'Syria']         0.0       0.0   \n",
       "3  ['Earthquake', 'Hatay', 'Turkey', 'turkeyearth...         0.0       0.0   \n",
       "4    ['Türkiye', 'Syria', 'earthquake', 'Canadians']         1.0       0.0   \n",
       "\n",
       "   followers_count  isVerified language               source  \n",
       "0           5697.0        True       en      Twitter Web App  \n",
       "1              1.0       False       en  Twitter for Android  \n",
       "2              3.0       False       en  Twitter for Android  \n",
       "3          21836.0        True       en  Twitter for Android  \n",
       "4            675.0       False       en   Twitter for iPhone  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(columns=['coordinates', 'place'], errors='ignore')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7180724",
   "metadata": {
    "id": "c7180724"
   },
   "source": [
    "## Sentiment Analysis\n",
    "\n",
    "- use pre-trained transformer-based `BERT` model to add a sentiment score fore each sentence "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5a9cd2",
   "metadata": {},
   "source": [
    "\"Sentiment analysis was performed utilizing a pre-trained transformer-based BERT model, specifically the `nlptown/bert-base-multilingual-uncased-sentiment`. This model is a fine-tuned version of `bert-base-multilingual-uncased`, which is optimized for sentiment analysis across six languages: English, Dutch, German, French, Spanish and Italian [40].\"\n",
    "\n",
    "#### Reference\n",
    "[40] Lakhanpal, S.; Gupta, A.; Agrawal, R. Leveraging Explainable AI to Analyze Researchers’ Aspect-Based Sentiment About ChatGPT. In Proceedings of the 15th International Conference on Intelligent Human Computer Interaction (IHCI 2023), Daegu, Republic of Korea, 8–10 November 2023; pp. 281–290."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "20c75d25",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10640,
     "status": "ok",
     "timestamp": 1762294735008,
     "user": {
      "displayName": "Jade Kim",
      "userId": "13751362502685406790"
     },
     "user_tz": 300
    },
    "id": "20c75d25",
    "outputId": "7e06ead2-ed3e-4af0-faa0-90a0aeee00be"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "save_dir = \"./local_bert_multilingual\"\n",
    "\n",
    "# Download once\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# tokenizer.save_pretrained(save_dir)\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "# model.save_pretrained(save_dir)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_dir)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(save_dir)\n",
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0247df-a6c2-433d-a867-cf9d11fb9833",
   "metadata": {},
   "source": [
    "\"Tweets were tokenized using the AutoTokenizer from HuggingFace Transformers, truncated to a maximum length of 512 tokens [41]. The model predicted sentiment scores across five classes representing very negative to very positive sentiments. These categorical outputs were then converted to a continuous polarity scale ranging from −1 (strongly negative) to +1 (strongly positive) to facilitate the temporal analysis of sentiment fluctuations.\"\n",
    "\n",
    "- polarity = (star_rating - 3) / 2\n",
    "- discrete star_rating  = \\[0,1,2,3,4\\] => continuous polarity = (-1.0, -0.5, 0.0, 0.5, 1.0)\n",
    "\n",
    "#### Reference\n",
    "[41] Hussain, Z.; Binz, M.; Mata, R.; Wulff, D.U. A tutorial on open-source large language models for behavioral science. Behav. Res. 2024, 56, 8214–8237."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "20c15946-cc98-4925-9a18-b2e85fecb325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 180915\n",
      "batch_size: 32\n",
      "5653 batchs + 19\n"
     ]
    }
   ],
   "source": [
    "inputs = df['content'].tolist()\n",
    "print(f\"N = {len(inputs)}\")\n",
    "batch_size = 32\n",
    "print(f\"batch_size: {batch_size}\")\n",
    "print(f\"{len(inputs)//batch_size} batchs + {len(inputs)%batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d-XBPpxs99DR",
   "metadata": {
    "executionInfo": {
     "elapsed": 43109,
     "status": "ok",
     "timestamp": 1762294778119,
     "user": {
      "displayName": "Jade Kim",
      "userId": "13751362502685406790"
     },
     "user_tz": 300
    },
    "id": "d-XBPpxs99DR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading ./data/turkey_syria_earthquake_tweets/polarity_scores.pt\n",
      "torch.Size([180915])\n"
     ]
    }
   ],
   "source": [
    "polarity_scores_path = \"./data/turkey_syria_earthquake_tweets/polarity_scores.pt\"\n",
    "outputs_dir = \"./data/turkey_syria_earthquake_tweets/polarity_batches\"\n",
    "os.makedirs(outputs_dir, exist_ok=True)\n",
    "\n",
    "if not os.path.exists(polarity_scores_path):\n",
    "    all_polarities = []\n",
    "    \n",
    "    num_inputs = len(inputs)\n",
    "    num_batches = (num_inputs + batch_size - 1) // batch_size\n",
    "    \n",
    "    for batch_idx, start_idx in enumerate(range(0, num_inputs, batch_size)):\n",
    "        end_idx = min(start_idx + batch_size, num_inputs)\n",
    "        batch_path = os.path.join(outputs_dir, f\"{int(batch_idx)}.pt\")\n",
    "    \n",
    "        if os.path.exists(batch_path):\n",
    "            print(f\"[skip] {batch_idx+1}/{num_batches} ({start_idx}~{end_idx}) already exists\")\n",
    "            continue\n",
    "        print(f\"[compute] {batch_idx+1}/{num_batches}: {start_idx}~{end_idx}\")\n",
    "    \n",
    "        batch = inputs[start_idx:end_idx]\n",
    "        model_inputs = tokenizer(batch, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "        \n",
    "        # Call BERT and get predicted labels\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**model_inputs)\n",
    "    \n",
    "        logits = outputs.logits\n",
    "        probabilities = torch.softmax(logits, dim=-1)\n",
    "        predicted_labels = torch.argmax(probabilities, dim=-1)\n",
    "    \n",
    "        # Convert to polarity scale as stated in the paper\n",
    "        star_ratings = predicted_labels + 1\n",
    "        polarity_scores = (star_ratings - 3) / 2.0\n",
    "        all_polarities.append(polarity_scores)\n",
    "    \n",
    "        # save each batch\n",
    "        torch.save(polarity_scores, batch_path)\n",
    "        print(f\"saved: {batch_path}\")\n",
    "    \n",
    "    polarity_scores = torch.cat(all_polarities, dim=0)\n",
    "    torch.save(polarity_scores, polarity_scores_path)\n",
    "    print(f\"saved to {polarity_scores_path}\")\n",
    "else:\n",
    "    print(f\"loading {polarity_scores_path}\")\n",
    "    polarity_scores = torch.load(polarity_scores_path, weights_only=True)\n",
    "\n",
    "print(polarity_scores.shape)\n",
    "assert len(polarity_scores) == len(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304ba44e-354a-488a-b65c-cd01fa17f6cc",
   "metadata": {},
   "source": [
    "Assign sentiment polarity (-1 ~ +1) to each tweet & add normalized polarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c8c59066-64a4-49e9-8b92-b2d390c6f7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of NA data: 0\n",
      "interpolation not needed\n"
     ]
    }
   ],
   "source": [
    "df['sentiment_polarity'] = polarity_scores.numpy()\n",
    "print(f\"number of NA data: {df['sentiment_polarity'].isna().sum()}\")\n",
    "if df['sentiment_polarity'].isna().sum() > 0:\n",
    "    pol = pol.interpolate(limit_direction=\"both\")\n",
    "else:\n",
    "    print(\"interpolation not needed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "97a2fbe2-80e3-4f88-9545-c1c47587ca10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>like_count</th>\n",
       "      <th>rt_count</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>isVerified</th>\n",
       "      <th>language</th>\n",
       "      <th>source</th>\n",
       "      <th>sentiment_polarity</th>\n",
       "      <th>sentiment_polarity_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-02-21 03:29:07+00:00</td>\n",
       "      <td>new search &amp;amp; rescue work is in progress in...</td>\n",
       "      <td>['Hatay', 'earthquakes', 'Türkiye', 'TurkiyeQu...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5697.0</td>\n",
       "      <td>True</td>\n",
       "      <td>en</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.011136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-02-21 03:29:04+00:00</td>\n",
       "      <td>can't imagine those who still haven't recovere...</td>\n",
       "      <td>['Turkey', 'earthquake', 'turkeyearthquake2023...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.689723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-02-21 03:28:06+00:00</td>\n",
       "      <td>its a highkey sign for all of us to ponder ove...</td>\n",
       "      <td>['turkeyearthquake2023', 'earthquake', 'Syria']</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.578089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-02-21 03:27:27+00:00</td>\n",
       "      <td>see how strong was the #earthquake of feb 20, ...</td>\n",
       "      <td>['Earthquake', 'Hatay', 'Turkey', 'turkeyearth...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21836.0</td>\n",
       "      <td>True</td>\n",
       "      <td>en</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.689723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-02-21 03:27:11+00:00</td>\n",
       "      <td>more difficult news today on top of struggles ...</td>\n",
       "      <td>['Türkiye', 'Syria', 'earthquake', 'Canadians']</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>675.0</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.122770</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        date  \\\n",
       "0  2023-02-21 03:29:07+00:00   \n",
       "1  2023-02-21 03:29:04+00:00   \n",
       "2  2023-02-21 03:28:06+00:00   \n",
       "3  2023-02-21 03:27:27+00:00   \n",
       "4  2023-02-21 03:27:11+00:00   \n",
       "\n",
       "                                             content  \\\n",
       "0  new search &amp; rescue work is in progress in...   \n",
       "1  can't imagine those who still haven't recovere...   \n",
       "2  its a highkey sign for all of us to ponder ove...   \n",
       "3  see how strong was the #earthquake of feb 20, ...   \n",
       "4  more difficult news today on top of struggles ...   \n",
       "\n",
       "                                            hashtags  like_count  rt_count  \\\n",
       "0  ['Hatay', 'earthquakes', 'Türkiye', 'TurkiyeQu...         1.0       0.0   \n",
       "1  ['Turkey', 'earthquake', 'turkeyearthquake2023...         0.0       0.0   \n",
       "2    ['turkeyearthquake2023', 'earthquake', 'Syria']         0.0       0.0   \n",
       "3  ['Earthquake', 'Hatay', 'Turkey', 'turkeyearth...         0.0       0.0   \n",
       "4    ['Türkiye', 'Syria', 'earthquake', 'Canadians']         1.0       0.0   \n",
       "\n",
       "   followers_count  isVerified language               source  \\\n",
       "0           5697.0        True       en      Twitter Web App   \n",
       "1              1.0       False       en  Twitter for Android   \n",
       "2              3.0       False       en  Twitter for Android   \n",
       "3          21836.0        True       en  Twitter for Android   \n",
       "4            675.0       False       en   Twitter for iPhone   \n",
       "\n",
       "   sentiment_polarity  sentiment_polarity_norm  \n",
       "0                 0.5                 1.011136  \n",
       "1                -1.0                -0.689723  \n",
       "2                 1.0                 1.578089  \n",
       "3                -1.0                -0.689723  \n",
       "4                -0.5                -0.122770  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean and normalize polarity data\n",
    "pol = df['sentiment_polarity'].astype(float).copy()\n",
    "pol_mean, pol_std = pol.mean(), pol.std() if pol.std() > 0 else 1.0\n",
    "df['sentiment_polarity_norm'] = (pol - pol_mean) / pol_std\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c81a58e-77d1-4bc1-8a89-90b2a0bcf2a6",
   "metadata": {},
   "source": [
    "## Save preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ca060c6f-3eec-4b09-9a92-bac524b7b80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ./data/turkey_syria_earthquake_tweets/preprocessed_data.csv\n"
     ]
    }
   ],
   "source": [
    "output_path = \"./data/turkey_syria_earthquake_tweets/preprocessed_data.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "print(\"Saved:\", output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a523e0",
   "metadata": {
    "id": "d0a523e0"
   },
   "source": [
    "# DL: Sentiment Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efbdbca-a3f2-4d8e-924e-359646d05330",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "efed0e4b-094f-4c48-8d86-6faba16d8429",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5fd705ff-2b32-4c8c-9f7c-a7992900efbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 180915\n",
      "date is descending (future => past)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>like_count</th>\n",
       "      <th>rt_count</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>isVerified</th>\n",
       "      <th>language</th>\n",
       "      <th>source</th>\n",
       "      <th>sentiment_polarity</th>\n",
       "      <th>sentiment_polarity_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-02-21 03:29:07+00:00</td>\n",
       "      <td>new search &amp;amp; rescue work is in progress in...</td>\n",
       "      <td>['Hatay', 'earthquakes', 'Türkiye', 'TurkiyeQu...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5697.0</td>\n",
       "      <td>True</td>\n",
       "      <td>en</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.011136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-02-21 03:29:04+00:00</td>\n",
       "      <td>can't imagine those who still haven't recovere...</td>\n",
       "      <td>['Turkey', 'earthquake', 'turkeyearthquake2023...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.689723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-02-21 03:28:06+00:00</td>\n",
       "      <td>its a highkey sign for all of us to ponder ove...</td>\n",
       "      <td>['turkeyearthquake2023', 'earthquake', 'Syria']</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.578089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-02-21 03:27:27+00:00</td>\n",
       "      <td>see how strong was the #earthquake of feb 20, ...</td>\n",
       "      <td>['Earthquake', 'Hatay', 'Turkey', 'turkeyearth...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21836.0</td>\n",
       "      <td>True</td>\n",
       "      <td>en</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.689723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-02-21 03:27:11+00:00</td>\n",
       "      <td>more difficult news today on top of struggles ...</td>\n",
       "      <td>['Türkiye', 'Syria', 'earthquake', 'Canadians']</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>675.0</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.122770</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        date  \\\n",
       "0  2023-02-21 03:29:07+00:00   \n",
       "1  2023-02-21 03:29:04+00:00   \n",
       "2  2023-02-21 03:28:06+00:00   \n",
       "3  2023-02-21 03:27:27+00:00   \n",
       "4  2023-02-21 03:27:11+00:00   \n",
       "\n",
       "                                             content  \\\n",
       "0  new search &amp; rescue work is in progress in...   \n",
       "1  can't imagine those who still haven't recovere...   \n",
       "2  its a highkey sign for all of us to ponder ove...   \n",
       "3  see how strong was the #earthquake of feb 20, ...   \n",
       "4  more difficult news today on top of struggles ...   \n",
       "\n",
       "                                            hashtags  like_count  rt_count  \\\n",
       "0  ['Hatay', 'earthquakes', 'Türkiye', 'TurkiyeQu...         1.0       0.0   \n",
       "1  ['Turkey', 'earthquake', 'turkeyearthquake2023...         0.0       0.0   \n",
       "2    ['turkeyearthquake2023', 'earthquake', 'Syria']         0.0       0.0   \n",
       "3  ['Earthquake', 'Hatay', 'Turkey', 'turkeyearth...         0.0       0.0   \n",
       "4    ['Türkiye', 'Syria', 'earthquake', 'Canadians']         1.0       0.0   \n",
       "\n",
       "   followers_count  isVerified language               source  \\\n",
       "0           5697.0        True       en      Twitter Web App   \n",
       "1              1.0       False       en  Twitter for Android   \n",
       "2              3.0       False       en  Twitter for Android   \n",
       "3          21836.0        True       en  Twitter for Android   \n",
       "4            675.0       False       en   Twitter for iPhone   \n",
       "\n",
       "   sentiment_polarity  sentiment_polarity_norm  \n",
       "0                 0.5                 1.011136  \n",
       "1                -1.0                -0.689723  \n",
       "2                 1.0                 1.578089  \n",
       "3                -1.0                -0.689723  \n",
       "4                -0.5                -0.122770  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_path = \"./data/turkey_syria_earthquake_tweets/preprocessed_data.csv\"\n",
    "df = pd.read_csv(output_path)\n",
    "print(f\"N = {len(df)}\")\n",
    "\n",
    "if df['date'].is_monotonic_increasing:\n",
    "    print(\"date is ascending (past => future)\")\n",
    "if df['date'].is_monotonic_decreasing:\n",
    "    print(\"date is descending (future => past)\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec095b8-b805-43d6-9d25-362e3cc168ac",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9144abe8-0f20-4fde-9eca-4698c1a7342c",
   "metadata": {},
   "source": [
    "Implementation\n",
    "- An autoencoder neural network was designed and trained to detect anomalies based on deviations in tweet sentiment patterns.\n",
    "- The input data was structured into sequences of polarity scores.\n",
    "- The autoencoder was implemented as a fully connected feedforward network with a three-layer encoder and symmetric decoder.\n",
    "- The encoder consisted of a hidden layer with 64 neurons followed by a 16-neuron bottleneck, using rectified linear unit (ReLU) activations for encoding and decoding [42].\n",
    "- Reconstruction errors (mean squared error between actual and reconstructed sequences) were calculated, and tweets with errors above the 95th percentile threshold were flagged as anomalies.\n",
    "\n",
    "Common config\n",
    "- Both models were trained for 10 epochs using the Adam optimizer (learning rate was set to 0.001), with a batch size of 32 and mean squared error (MSE) loss.\n",
    "- Sentiment polarity scores were normalized using MinMax scaling to the [0,1] range. The model’s output was a prediction of subsequent sentiment scores.\n",
    "- Anomalies were identified when prediction errors exceeded a threshold set at the 95th percentile, highlighting sudden or extreme shifts (changes) in sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70xxm9V0bYT2",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1762294778134,
     "user": {
      "displayName": "Jade Kim",
      "userId": "13751362502685406790"
     },
     "user_tz": 300
    },
    "id": "70xxm9V0bYT2"
   },
   "outputs": [],
   "source": [
    "# Create Autoencoder neural network\n",
    "\n",
    "SEQ_LEN   = 64\n",
    "STRIDE    = 1\n",
    "BATCH_SIZE= 128\n",
    "EPOCHS    = 20\n",
    "LR        = 1e-3\n",
    "DEVICE    = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "RNG       = np.random.default_rng(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "yWs_nrwH5VNv",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1762294778138,
     "user": {
      "displayName": "Jade Kim",
      "userId": "13751362502685406790"
     },
     "user_tz": 300
    },
    "id": "yWs_nrwH5VNv"
   },
   "outputs": [],
   "source": [
    "# construct input data as sequences of polarity scores. Note that I used GPT for this section\n",
    "\n",
    "series = pol_norm.to_numpy().astype(np.float32)\n",
    "N = len(series)\n",
    "windows = []\n",
    "indices = []  # store ending index of each window for mapping anomalies back\n",
    "for start in range(0, N - SEQ_LEN + 1, STRIDE):\n",
    "    end = start + SEQ_LEN\n",
    "    windows.append(series[start:end])\n",
    "    indices.append(end - 1)  # align anomaly decision at window end\n",
    "X = np.stack(windows, axis=0)  # (num_windows, SEQ_LEN)\n",
    "idx_map = np.array(indices)\n",
    "\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, X):\n",
    "        self.X = torch.from_numpy(X)\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    def __getitem__(self, i):\n",
    "        x = self.X[i]\n",
    "        return x, x  # autoencoder: input == target\n",
    "\n",
    "dataset = SeqDataset(X)\n",
    "\n",
    "# train/test split the dataset\n",
    "val_size = int(0.2 * len(dataset))\n",
    "train_size = len(dataset) - val_size\n",
    "train_ds, val_ds = random_split(dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "val_dl   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "vG0ZSCXkuS-3",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1762294778142,
     "user": {
      "displayName": "Jade Kim",
      "userId": "13751362502685406790"
     },
     "user_tz": 300
    },
    "id": "vG0ZSCXkuS-3"
   },
   "outputs": [],
   "source": [
    "# declare AutoEncoder class with layers specified in the paper\n",
    "class AE(nn.Module):\n",
    "    def __init__(self, in_dim=SEQ_LEN, h1=128, h2=64, bottleneck=16):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(in_dim, h1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h1, h2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h2, bottleneck),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(bottleneck, h2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h2, h1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h1, in_dim)  # final layer (no activation for regression)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        out = self.decoder(z)\n",
    "        return out\n",
    "\n",
    "model = AE(in_dim=SEQ_LEN, h1=128, h2=64, bottleneck=16).to(DEVICE)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "UmsAK2mCn-2l",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 571,
     "status": "ok",
     "timestamp": 1762294778715,
     "user": {
      "displayName": "Jade Kim",
      "userId": "13751362502685406790"
     },
     "user_tz": 300
    },
    "id": "UmsAK2mCn-2l",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "e4b09677-1115-4c26-8f85-2141d5ab6148"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train MSE: 0.986858 | val MSE: 0.978194\n",
      "Epoch 02 | train MSE: 0.975218 | val MSE: 0.961202\n",
      "Epoch 03 | train MSE: 0.944968 | val MSE: 0.920235\n",
      "Epoch 04 | train MSE: 0.896460 | val MSE: 0.878052\n",
      "Epoch 05 | train MSE: 0.853866 | val MSE: 0.846852\n",
      "Epoch 06 | train MSE: 0.821692 | val MSE: 0.824133\n",
      "Epoch 07 | train MSE: 0.798710 | val MSE: 0.810321\n",
      "Epoch 08 | train MSE: 0.785143 | val MSE: 0.804307\n",
      "Epoch 09 | train MSE: 0.771090 | val MSE: 0.791600\n",
      "Epoch 10 | train MSE: 0.753499 | val MSE: 0.780109\n",
      "Epoch 11 | train MSE: 0.737869 | val MSE: 0.769325\n",
      "Epoch 12 | train MSE: 0.723167 | val MSE: 0.756426\n",
      "Epoch 13 | train MSE: 0.706743 | val MSE: 0.746167\n",
      "Epoch 14 | train MSE: 0.693631 | val MSE: 0.741966\n",
      "Epoch 15 | train MSE: 0.682895 | val MSE: 0.732714\n",
      "Epoch 16 | train MSE: 0.669968 | val MSE: 0.723534\n",
      "Epoch 17 | train MSE: 0.658414 | val MSE: 0.714663\n",
      "Epoch 18 | train MSE: 0.647138 | val MSE: 0.710504\n",
      "Epoch 19 | train MSE: 0.637369 | val MSE: 0.701968\n",
      "Epoch 20 | train MSE: 0.624954 | val MSE: 0.692304\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AE(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=64, out_features=16, bias=True)\n",
       "    (5): ReLU()\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=16, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=128, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train loop\n",
    "def run_epoch(dl, train=True):\n",
    "    model.train(train)\n",
    "    total, count = 0.0, 0\n",
    "    for xb, yb in dl:\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "        if train:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "        preds = model(xb)\n",
    "        loss = criterion(preds, yb)\n",
    "        if train:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        total += loss.item() * xb.size(0)\n",
    "        count += xb.size(0)\n",
    "    return total / max(count, 1)\n",
    "\n",
    "best_val = float(\"inf\")\n",
    "patience, bad = 5, 0 # used to prevent overfitting, tracks if validation loss stops decreasing or starts increasing\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    tr = run_epoch(train_dl, train=True)\n",
    "    va = run_epoch(val_dl, train=False)\n",
    "    print(f\"Epoch {epoch:02d} | train MSE: {tr:.6f} | val MSE: {va:.6f}\")\n",
    "    if va + 1e-6 < best_val:\n",
    "        best_val = va\n",
    "        bad = 0\n",
    "        best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "    else:\n",
    "        bad += 1\n",
    "        if bad >= patience:\n",
    "            print(\"Early stopping.\")\n",
    "            break\n",
    "\n",
    "# Restore best model\n",
    "model.load_state_dict(best_state)\n",
    "model.to(DEVICE)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ne-kXE27LL-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1762295616630,
     "user": {
      "displayName": "Jade Kim",
      "userId": "13751362502685406790"
     },
     "user_tz": 300
    },
    "id": "3ne-kXE27LL-",
    "outputId": "3f7ae055-a465-4386-f697-c11f1a3e38bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold (95th pct): 1.322654\n",
      "Detected anomalies: 122 / 2437 windows\n",
      "                                               content  sentiment_polarity\n",
      "166  @ lefkosaturkbld , present information disaste...                -1.0\n",
      "168  africa also reported splitting two . # turkey ...                -1.0\n",
      "317  heart aching feel helpless # helpsyria # syria...                -1.0\n",
      "319  idlib health directorate : dozens emergency ca...                -1.0\n",
      "320  🔔 # earthquake ( # deprem ) m2.7 occurred 20 k...                -1.0\n",
      "329  best option choose btw ➡️ # livrma match  : //...                 1.0\n",
      "336  footage shaking adana city hospital magnitude ...                -1.0\n",
      "339  # almayadeen 's correspondent # aleppo said 3 ...                -1.0\n",
      "356  friend vietnam predicted another # earthquake ...                -1.0\n",
      "628  interior minister soylu : 3 people lost lives ...                -1.0\n",
      "                                             content  sentiment_polarity\n",
      "0  new search & amp ; rescue work progress # hata...                 0.5\n",
      "1  ca n't imagine still n't recovered previous tr...                -1.0\n",
      "2  highkey sign us ponder actions return merciful...                -1.0\n",
      "3  see strong # earthquake feb 20 , 2023 # hatay ...                -1.0\n",
      "4  difficult news today top struggles already fac...                -1.0\n",
      "5  another # earthquake southern # turkey .  : //...                -1.0\n",
      "6  three people confirmed died 6.4-magnitude # ea...                -1.0\n",
      "7  6.4 magnitude quake shakes turkey -syria borde...                -1.0\n",
      "8  new 6.3 # earthquake hits # turkey , # syria b...                -1.0\n",
      "9  latest # earthquake # turkey impact # disaster...                -1.0\n"
     ]
    }
   ],
   "source": [
    "# compute recon errors\n",
    "with torch.no_grad():\n",
    "    X_tensor = torch.from_numpy(X).to(DEVICE)\n",
    "    recon = model(X_tensor)\n",
    "    mse = ((recon - X_tensor) ** 2).mean(dim=1).detach().cpu().numpy()  # per-window MSE\n",
    "\n",
    "# 95th percentile threshold\n",
    "threshold = np.percentile(mse, 95.0)\n",
    "\n",
    "# Flag anomalies: windows with error above threshold\n",
    "is_anom = mse > threshold\n",
    "\n",
    "# Map window anomalies back to tweet-level indices\n",
    "anom_indices = idx_map[is_anom]  # indices in the original df (row positions)\n",
    "df_subset['ae_recon_error'] = np.nan\n",
    "df_subset.loc[idx_map, 'ae_recon_error'] = mse  # assign errors to window-end rows\n",
    "df_subset['ae_anomaly_95p'] = False\n",
    "df_subset.loc[anom_indices, 'ae_anomaly_95p'] = True\n",
    "\n",
    "\n",
    "print(f\"Detected anomalies: {is_anom.sum()} / {len(is_anom)} windows\")\n",
    "anomalies = df_subset.loc[df_subset['ae_anomaly_95p'] == True, ['content', 'sentiment_polarity']]\n",
    "# non_anomalies = df_subset.loc[df_subset['ae_anomaly_95p'] == False, ['content', 'sentiment_polarity']]\n",
    "print(anomalies.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f523f80-0254-42d0-b7bf-dfd22d828d7f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## LSTM with Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54eed613-cfe5-4c78-813e-8528fc665b80",
   "metadata": {},
   "source": [
    "Implementation\n",
    "- An LSTM neural network with an integrated attention mechanism was implemented to detect anomalies based on prediction errors.\n",
    "- Input sequences of polarity scores were processed through LSTM layers, and attention layers were applied to selectively weigh temporal dependencies within the sequences.\n",
    "- The LSTM with attention included a single-layer LSTM model with a hidden size of 32, followed by an attention mechanism.\n",
    "\n",
    "Common config\n",
    "- Both models were trained for 10 epochs using the Adam optimizer (learning rate was set to 0.001), with a batch size of 32 and mean squared error (MSE) loss.\n",
    "- Sentiment polarity scores were normalized using MinMax scaling to the [0,1] range. The model’s output was a prediction of subsequent sentiment scores.\n",
    "- Anomalies were identified when prediction errors exceeded a threshold set at the 95th percentile, highlighting sudden or extreme shifts (changes) in sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e7d859-98a8-4b0e-ba23-b45a63acdc15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V6E1",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
