{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2751f992",
   "metadata": {
    "id": "2751f992"
   },
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "- [Dataset](https://www.kaggle.com/datasets/swaptr/turkey-earthquake-tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "852916e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fba25f",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a4e11d15",
   "metadata": {
    "id": "a4e11d15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 189626\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>like_count</th>\n",
       "      <th>rt_count</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>isVerified</th>\n",
       "      <th>language</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>place</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-21 03:29:07+00:00</td>\n",
       "      <td>New search &amp;amp; rescue work is in progress in...</td>\n",
       "      <td>['Hatay', 'earthquakes', 'T√ºrkiye', 'TurkiyeQu...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5697.0</td>\n",
       "      <td>True</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Twitter Web App</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-21 03:29:04+00:00</td>\n",
       "      <td>Can't imagine those who still haven't recovere...</td>\n",
       "      <td>['Turkey', 'earthquake', 'turkeyearthquake2023...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Twitter for Android</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-02-21 03:28:06+00:00</td>\n",
       "      <td>its a highkey sign for all of us to ponder ove...</td>\n",
       "      <td>['turkeyearthquake2023', 'earthquake', 'Syria']</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Twitter for Android</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>2023-02-21 03:27:27+00:00</td>\n",
       "      <td>See how strong was the #Earthquake of Feb 20, ...</td>\n",
       "      <td>['Earthquake', 'Hatay', 'Turkey', 'turkeyearth...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21836.0</td>\n",
       "      <td>True</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Twitter for Android</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>2023-02-21 03:27:11+00:00</td>\n",
       "      <td>More difficult news today on top of struggles ...</td>\n",
       "      <td>['T√ºrkiye', 'Syria', 'earthquake', 'Canadians']</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>675.0</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                       date  \\\n",
       "0           1  2023-02-21 03:29:07+00:00   \n",
       "1           2  2023-02-21 03:29:04+00:00   \n",
       "2           3  2023-02-21 03:28:06+00:00   \n",
       "3           5  2023-02-21 03:27:27+00:00   \n",
       "4           6  2023-02-21 03:27:11+00:00   \n",
       "\n",
       "                                             content  \\\n",
       "0  New search &amp; rescue work is in progress in...   \n",
       "1  Can't imagine those who still haven't recovere...   \n",
       "2  its a highkey sign for all of us to ponder ove...   \n",
       "3  See how strong was the #Earthquake of Feb 20, ...   \n",
       "4  More difficult news today on top of struggles ...   \n",
       "\n",
       "                                            hashtags  like_count  rt_count  \\\n",
       "0  ['Hatay', 'earthquakes', 'T√ºrkiye', 'TurkiyeQu...         1.0       0.0   \n",
       "1  ['Turkey', 'earthquake', 'turkeyearthquake2023...         0.0       0.0   \n",
       "2    ['turkeyearthquake2023', 'earthquake', 'Syria']         0.0       0.0   \n",
       "3  ['Earthquake', 'Hatay', 'Turkey', 'turkeyearth...         0.0       0.0   \n",
       "4    ['T√ºrkiye', 'Syria', 'earthquake', 'Canadians']         1.0       0.0   \n",
       "\n",
       "   followers_count  isVerified language coordinates place               source  \n",
       "0           5697.0        True       en         NaN   NaN      Twitter Web App  \n",
       "1              1.0       False       en         NaN   NaN  Twitter for Android  \n",
       "2              3.0       False       en         NaN   NaN  Twitter for Android  \n",
       "3          21836.0        True       en         NaN   NaN  Twitter for Android  \n",
       "4            675.0       False       en         NaN   NaN   Twitter for iPhone  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # NOTE: unzip ./data/turkey_syria_earthquake_tweets/archive.zip before running the below code. `tweets.csv` file is too big.\n",
    "# full_data = pd.read_csv(\"./data/turkey_syria_earthquake_tweets/tweets.csv\")\n",
    "# full_data.head()\n",
    "\n",
    "# # Invalid data\n",
    "# # NOTE: full_data.iloc[:, 6] contains NaN\n",
    "# # DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
    "# print(full_data.iloc[:, 6].apply(type).value_counts())\n",
    "# mask = full_data['isVerified'].apply(lambda v: isinstance(v, float))\n",
    "# print(full_data.loc[mask, 'isVerified'])\n",
    "\n",
    "df = pd.read_csv(\"./data/turkey_syria_earthquake_tweets/tweets_en.csv\")\n",
    "print(f\"N = {len(df)}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a2b97521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New search &amp; rescue work is in progress in #Hatay after two more #earthquakes hit #T√ºrkiye‚Äôs southeastern province.  #TurkiyeQuakes #Turkey-#Syria  #Earthquake #turkeyearthquake2023  https://t.co/sd4WHByiQs\n"
     ]
    }
   ],
   "source": [
    "print(df.loc[0, 'content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02411611",
   "metadata": {},
   "source": [
    "### Text lowercasing\n",
    "\n",
    "All tweets were converted to lowercase; according to Hickman et al. [37], lowercasing tends to be beneficial because it reduces data dimensionality, thereby increasing statistical power, and usually does not reduce validity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f74ca052",
   "metadata": {
    "executionInfo": {
     "elapsed": 153,
     "status": "ok",
     "timestamp": 1762294699522,
     "user": {
      "displayName": "Jade Kim",
      "userId": "13751362502685406790"
     },
     "user_tz": 300
    },
    "id": "f74ca052"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new search &amp; rescue work is in progress in #hatay after two more #earthquakes hit #t√ºrkiye‚Äôs southeastern province.  #turkiyequakes #turkey-#syria  #earthquake #turkeyearthquake2023  https://t.co/sd4whbyiqs\n"
     ]
    }
   ],
   "source": [
    "df['content'] = df['content'].str.lower()\n",
    "print(df.loc[0, 'content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162319cc",
   "metadata": {},
   "source": [
    "### Stop word removal\n",
    "\n",
    "Stop word removal was useful in traditional NLP models, but not so effective for DL models. \n",
    "\n",
    "The original paper compared the traditional NLP models with DL models so they used it. \n",
    "\n",
    "\"common English (function) words such as ‚Äúand‚Äù, ‚Äúis‚Äù, ‚ÄúI‚Äù, ‚Äúam‚Äù, ‚Äúwhat‚Äù, ‚Äúof‚Äù, etc. were removed by using the Natural Language Toolkit (NLTK). Stop word removal has the advantages of reducing the size of the stored dataset and improving the overall efficiency and effectiveness of the analysis [38].\"\n",
    "\n",
    "However, **we are only training DL models in our analysis so we are not applying it**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5800ed7d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21153,
     "status": "ok",
     "timestamp": 1762294724071,
     "user": {
      "displayName": "Jade Kim",
      "userId": "13751362502685406790"
     },
     "user_tz": 300
    },
    "id": "5800ed7d",
    "outputId": "d3dc1c32-711a-4e81-d3c0-d607c1c5439d"
   },
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.tokenize import word_tokenize\n",
    "\n",
    "# NLTK_LOCAL_PATH = \"./nltk_data\"\n",
    "# os.makedirs(NLTK_LOCAL_PATH, exist_ok=True)\n",
    "# nltk.data.path.append(NLTK_LOCAL_PATH)\n",
    "# # nltk.download('stopwords', download_dir=NLTK_LOCAL_PATH)\n",
    "# # nltk.download('punkt', download_dir=NLTK_LOCAL_PATH)\n",
    "\n",
    "# # Get English stop words\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# # Function to remove stop words from text\n",
    "# def remove_stopwords(text):\n",
    "#     if pd.isna(text):\n",
    "#         return text\n",
    "#     # Tokenize the text\n",
    "#     words = word_tokenize(text)\n",
    "#     # Remove stop words and return as string\n",
    "#     filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "#     return ' '.join(filtered_words)\n",
    "\n",
    "# df['content'] = df['content'].apply(remove_stopwords)\n",
    "# print(df.loc[0, 'content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fed112",
   "metadata": {},
   "source": [
    "### URLs removal\n",
    "\n",
    "All URLs were removed from tweets, since the text of URL strings does not necessarily convey any relevant information,  and can therefore be removed [39]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "71530407",
   "metadata": {
    "executionInfo": {
     "elapsed": 142,
     "status": "ok",
     "timestamp": 1762294724216,
     "user": {
      "displayName": "Jade Kim",
      "userId": "13751362502685406790"
     },
     "user_tz": 300
    },
    "id": "71530407"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new search &amp; rescue work is in progress in #hatay after two more #earthquakes hit #t√ºrkiye‚Äôs southeastern province.  #turkiyequakes #turkey-#syria  #earthquake #turkeyearthquake2023  \n"
     ]
    }
   ],
   "source": [
    "df['content'] = df['content'].str.replace(r'http\\S+', '', regex=True)\n",
    "print(df.loc[0, 'content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f64f950",
   "metadata": {},
   "source": [
    "### Duplicate removal\n",
    "\n",
    "All duplicate tweets were removed to eliminate redundancy and possible skewing of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6f7b55b3",
   "metadata": {
    "executionInfo": {
     "elapsed": 81,
     "status": "ok",
     "timestamp": 1762294724300,
     "user": {
      "displayName": "Jade Kim",
      "userId": "13751362502685406790"
     },
     "user_tz": 300
    },
    "id": "6f7b55b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189626\n",
      "180915\n"
     ]
    }
   ],
   "source": [
    "print(len(df))\n",
    "df = df.drop_duplicates(subset='content', keep='first')\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6b4793",
   "metadata": {},
   "source": [
    "### Exclude location info\n",
    "\n",
    "96% of the tweets lacked geolocation, drop if exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5866eb9e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1762294724336,
     "user": {
      "displayName": "Jade Kim",
      "userId": "13751362502685406790"
     },
     "user_tz": 300
    },
    "id": "5866eb9e",
    "outputId": "254ef374-e187-4e3e-db31-5541da746211"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>like_count</th>\n",
       "      <th>rt_count</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>isVerified</th>\n",
       "      <th>language</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-21 03:29:07+00:00</td>\n",
       "      <td>new search &amp;amp; rescue work is in progress in...</td>\n",
       "      <td>['Hatay', 'earthquakes', 'T√ºrkiye', 'TurkiyeQu...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5697.0</td>\n",
       "      <td>True</td>\n",
       "      <td>en</td>\n",
       "      <td>Twitter Web App</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-21 03:29:04+00:00</td>\n",
       "      <td>can't imagine those who still haven't recovere...</td>\n",
       "      <td>['Turkey', 'earthquake', 'turkeyearthquake2023...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>Twitter for Android</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-02-21 03:28:06+00:00</td>\n",
       "      <td>its a highkey sign for all of us to ponder ove...</td>\n",
       "      <td>['turkeyearthquake2023', 'earthquake', 'Syria']</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>Twitter for Android</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>2023-02-21 03:27:27+00:00</td>\n",
       "      <td>see how strong was the #earthquake of feb 20, ...</td>\n",
       "      <td>['Earthquake', 'Hatay', 'Turkey', 'turkeyearth...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21836.0</td>\n",
       "      <td>True</td>\n",
       "      <td>en</td>\n",
       "      <td>Twitter for Android</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>2023-02-21 03:27:11+00:00</td>\n",
       "      <td>more difficult news today on top of struggles ...</td>\n",
       "      <td>['T√ºrkiye', 'Syria', 'earthquake', 'Canadians']</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>675.0</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                       date  \\\n",
       "0           1  2023-02-21 03:29:07+00:00   \n",
       "1           2  2023-02-21 03:29:04+00:00   \n",
       "2           3  2023-02-21 03:28:06+00:00   \n",
       "3           5  2023-02-21 03:27:27+00:00   \n",
       "4           6  2023-02-21 03:27:11+00:00   \n",
       "\n",
       "                                             content  \\\n",
       "0  new search &amp; rescue work is in progress in...   \n",
       "1  can't imagine those who still haven't recovere...   \n",
       "2  its a highkey sign for all of us to ponder ove...   \n",
       "3  see how strong was the #earthquake of feb 20, ...   \n",
       "4  more difficult news today on top of struggles ...   \n",
       "\n",
       "                                            hashtags  like_count  rt_count  \\\n",
       "0  ['Hatay', 'earthquakes', 'T√ºrkiye', 'TurkiyeQu...         1.0       0.0   \n",
       "1  ['Turkey', 'earthquake', 'turkeyearthquake2023...         0.0       0.0   \n",
       "2    ['turkeyearthquake2023', 'earthquake', 'Syria']         0.0       0.0   \n",
       "3  ['Earthquake', 'Hatay', 'Turkey', 'turkeyearth...         0.0       0.0   \n",
       "4    ['T√ºrkiye', 'Syria', 'earthquake', 'Canadians']         1.0       0.0   \n",
       "\n",
       "   followers_count  isVerified language               source  \n",
       "0           5697.0        True       en      Twitter Web App  \n",
       "1              1.0       False       en  Twitter for Android  \n",
       "2              3.0       False       en  Twitter for Android  \n",
       "3          21836.0        True       en  Twitter for Android  \n",
       "4            675.0       False       en   Twitter for iPhone  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(columns=['coordinates', 'place'], errors='ignore')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a653ab",
   "metadata": {
    "id": "50a653ab"
   },
   "source": [
    "# Neural Network Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6900cfd9",
   "metadata": {},
   "source": [
    "- Sentiment Analysis\n",
    "  - pre-trained transformer-based `BERT` model\n",
    "\n",
    "- Anomaly Detection\n",
    "  - `autoencoder`\n",
    "  - `LSTM with Attention`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7180724",
   "metadata": {
    "id": "c7180724"
   },
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5a9cd2",
   "metadata": {},
   "source": [
    "- `nlptown/bert-base-multilingual-uncased-sentiment` :  fine-tuned version of `bert-base-multilingual-uncased`, which is optimized for sentiment analysis across six languages: English, Dutch, German, French, Spanish and Italian.\n",
    "- Reference: Lakhanpal, S.; Gupta, A.; Agrawal, R. Leveraging Explainable AI to Analyze Researchers‚Äô Aspect-Based Sentiment About ChatGPT. In Proceedings of the 15th International Conference on Intelligent Human Computer Interaction (IHCI 2023), Daegu, Republic of Korea, 8‚Äì10 November 2023; pp. 281‚Äì290.\n",
    "\n",
    "- Can be seen as part of preprocessing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c75d25",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 10640,
     "status": "ok",
     "timestamp": 1762294735008,
     "user": {
      "displayName": "Jade Kim",
      "userId": "13751362502685406790"
     },
     "user_tz": 300
    },
    "id": "20c75d25",
    "outputId": "7e06ead2-ed3e-4af0-faa0-90a0aeee00be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.10.23)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/jax/_src/cloud_tpu_init.py:86: UserWarning: Transparent hugepages are not enabled. TPU runtime startup and shutdown time should be significantly improved on TPU v5e and newer. If not already set, you may need to enable transparent hugepages in your VM image (sudo sh -c \"echo always > /sys/kernel/mm/transparent_hugepage/enabled\")\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Tokenize inputs\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# pipe = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "\n",
    "inputs = df['content'].tolist()\n",
    "\n",
    "inputs = inputs[:2500]\n",
    "model_inputs = tokenizer(inputs, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d-XBPpxs99DR",
   "metadata": {
    "collapsed": true,
    "executionInfo": {
     "elapsed": 43109,
     "status": "ok",
     "timestamp": 1762294778119,
     "user": {
      "displayName": "Jade Kim",
      "userId": "13751362502685406790"
     },
     "user_tz": 300
    },
    "id": "d-XBPpxs99DR"
   },
   "outputs": [],
   "source": [
    "# Call BERT and get predicted labels\n",
    "outputs = model(**model_inputs)\n",
    "logits = outputs.logits\n",
    "probabilities = torch.softmax(logits, dim=-1)\n",
    "predicted_labels = torch.argmax(probabilities, dim=-1)\n",
    "\n",
    "# Convert to polarity scale as stated in the paper\n",
    "star_ratings = predicted_labels + 1\n",
    "polarity_scores = (star_ratings - 3) / 2.0\n",
    "# Now each tweet has a sentiment polarity ‚àà [-1, +1]\n",
    "# Note that I had to create a subset because of resource limitations\n",
    "df_subset = df.iloc[:2500].copy()\n",
    "df_subset['sentiment_polarity'] = polarity_scores.numpy()\n",
    "\n",
    "\n",
    "# clean and normalize polarity data\n",
    "pol = df_subset['sentiment_polarity'].astype(float).copy()\n",
    "pol = pol.interpolate(limit_direction=\"both\")  # fill occasional gaps\n",
    "pol_mean, pol_std = pol.mean(), pol.std() if pol.std() > 0 else 1.0\n",
    "pol_norm = (pol - pol_mean) / pol_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70xxm9V0bYT2",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1762294778134,
     "user": {
      "displayName": "Jade Kim",
      "userId": "13751362502685406790"
     },
     "user_tz": 300
    },
    "id": "70xxm9V0bYT2"
   },
   "outputs": [],
   "source": [
    "# Create Autoencoder neural network\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# hyperparameters\n",
    "SEQ_LEN   = 64\n",
    "STRIDE    = 1\n",
    "BATCH_SIZE= 128\n",
    "EPOCHS    = 20\n",
    "LR        = 1e-3\n",
    "DEVICE    = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "RNG       = np.random.default_rng(42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "yWs_nrwH5VNv",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1762294778138,
     "user": {
      "displayName": "Jade Kim",
      "userId": "13751362502685406790"
     },
     "user_tz": 300
    },
    "id": "yWs_nrwH5VNv"
   },
   "outputs": [],
   "source": [
    "# construct input data as sequences of polarity scores. Note that I used GPT\n",
    "# for this section\n",
    "\n",
    "series = pol_norm.to_numpy().astype(np.float32)\n",
    "N = len(series)\n",
    "windows = []\n",
    "indices = []  # store ending index of each window for mapping anomalies back\n",
    "for start in range(0, N - SEQ_LEN + 1, STRIDE):\n",
    "    end = start + SEQ_LEN\n",
    "    windows.append(series[start:end])\n",
    "    indices.append(end - 1)  # align anomaly decision at window end\n",
    "X = np.stack(windows, axis=0)  # (num_windows, SEQ_LEN)\n",
    "idx_map = np.array(indices)\n",
    "\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, X):\n",
    "        self.X = torch.from_numpy(X)\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    def __getitem__(self, i):\n",
    "        x = self.X[i]\n",
    "        return x, x  # autoencoder: input == target\n",
    "\n",
    "dataset = SeqDataset(X)\n",
    "\n",
    "# train/test split the dataset\n",
    "val_size = int(0.2 * len(dataset))\n",
    "train_size = len(dataset) - val_size\n",
    "train_ds, val_ds = random_split(dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "val_dl   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "vG0ZSCXkuS-3",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1762294778142,
     "user": {
      "displayName": "Jade Kim",
      "userId": "13751362502685406790"
     },
     "user_tz": 300
    },
    "id": "vG0ZSCXkuS-3"
   },
   "outputs": [],
   "source": [
    "# declare AutoEncoder class with layers specified in the paper\n",
    "class AE(nn.Module):\n",
    "    def __init__(self, in_dim=SEQ_LEN, h1=128, h2=64, bottleneck=16):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(in_dim, h1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h1, h2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h2, bottleneck),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(bottleneck, h2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h2, h1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h1, in_dim)  # final layer (no activation for regression)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        out = self.decoder(z)\n",
    "        return out\n",
    "\n",
    "model = AE(in_dim=SEQ_LEN, h1=128, h2=64, bottleneck=16).to(DEVICE)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "UmsAK2mCn-2l",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 571,
     "status": "ok",
     "timestamp": 1762294778715,
     "user": {
      "displayName": "Jade Kim",
      "userId": "13751362502685406790"
     },
     "user_tz": 300
    },
    "id": "UmsAK2mCn-2l",
    "outputId": "e4b09677-1115-4c26-8f85-2141d5ab6148"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train MSE: 0.986858 | val MSE: 0.978194\n",
      "Epoch 02 | train MSE: 0.975218 | val MSE: 0.961202\n",
      "Epoch 03 | train MSE: 0.944968 | val MSE: 0.920235\n",
      "Epoch 04 | train MSE: 0.896460 | val MSE: 0.878052\n",
      "Epoch 05 | train MSE: 0.853866 | val MSE: 0.846852\n",
      "Epoch 06 | train MSE: 0.821692 | val MSE: 0.824133\n",
      "Epoch 07 | train MSE: 0.798710 | val MSE: 0.810321\n",
      "Epoch 08 | train MSE: 0.785143 | val MSE: 0.804307\n",
      "Epoch 09 | train MSE: 0.771090 | val MSE: 0.791600\n",
      "Epoch 10 | train MSE: 0.753499 | val MSE: 0.780109\n",
      "Epoch 11 | train MSE: 0.737869 | val MSE: 0.769325\n",
      "Epoch 12 | train MSE: 0.723167 | val MSE: 0.756426\n",
      "Epoch 13 | train MSE: 0.706743 | val MSE: 0.746167\n",
      "Epoch 14 | train MSE: 0.693631 | val MSE: 0.741966\n",
      "Epoch 15 | train MSE: 0.682895 | val MSE: 0.732714\n",
      "Epoch 16 | train MSE: 0.669968 | val MSE: 0.723534\n",
      "Epoch 17 | train MSE: 0.658414 | val MSE: 0.714663\n",
      "Epoch 18 | train MSE: 0.647138 | val MSE: 0.710504\n",
      "Epoch 19 | train MSE: 0.637369 | val MSE: 0.701968\n",
      "Epoch 20 | train MSE: 0.624954 | val MSE: 0.692304\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AE(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=64, out_features=16, bias=True)\n",
       "    (5): ReLU()\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=16, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=128, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train loop\n",
    "def run_epoch(dl, train=True):\n",
    "    model.train(train)\n",
    "    total, count = 0.0, 0\n",
    "    for xb, yb in dl:\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "        if train:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "        preds = model(xb)\n",
    "        loss = criterion(preds, yb)\n",
    "        if train:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        total += loss.item() * xb.size(0)\n",
    "        count += xb.size(0)\n",
    "    return total / max(count, 1)\n",
    "\n",
    "best_val = float(\"inf\")\n",
    "patience, bad = 5, 0 # used to prevent overfitting, tracks if validation loss stops decreasing or starts increasing\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    tr = run_epoch(train_dl, train=True)\n",
    "    va = run_epoch(val_dl, train=False)\n",
    "    print(f\"Epoch {epoch:02d} | train MSE: {tr:.6f} | val MSE: {va:.6f}\")\n",
    "    if va + 1e-6 < best_val:\n",
    "        best_val = va\n",
    "        bad = 0\n",
    "        best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "    else:\n",
    "        bad += 1\n",
    "        if bad >= patience:\n",
    "            print(\"Early stopping.\")\n",
    "            break\n",
    "\n",
    "# Restore best model\n",
    "model.load_state_dict(best_state)\n",
    "model.to(DEVICE)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ne-kXE27LL-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1762295616630,
     "user": {
      "displayName": "Jade Kim",
      "userId": "13751362502685406790"
     },
     "user_tz": 300
    },
    "id": "3ne-kXE27LL-",
    "outputId": "3f7ae055-a465-4386-f697-c11f1a3e38bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold (95th pct): 1.322654\n",
      "Detected anomalies: 122 / 2437 windows\n",
      "                                               content  sentiment_polarity\n",
      "166  @ lefkosaturkbld , present information disaste...                -1.0\n",
      "168  africa also reported splitting two . # turkey ...                -1.0\n",
      "317  heart aching feel helpless # helpsyria # syria...                -1.0\n",
      "319  idlib health directorate : dozens emergency ca...                -1.0\n",
      "320  üîî # earthquake ( # deprem ) m2.7 occurred 20 k...                -1.0\n",
      "329  best option choose btw ‚û°Ô∏è # livrma match  : //...                 1.0\n",
      "336  footage shaking adana city hospital magnitude ...                -1.0\n",
      "339  # almayadeen 's correspondent # aleppo said 3 ...                -1.0\n",
      "356  friend vietnam predicted another # earthquake ...                -1.0\n",
      "628  interior minister soylu : 3 people lost lives ...                -1.0\n",
      "                                             content  sentiment_polarity\n",
      "0  new search & amp ; rescue work progress # hata...                 0.5\n",
      "1  ca n't imagine still n't recovered previous tr...                -1.0\n",
      "2  highkey sign us ponder actions return merciful...                -1.0\n",
      "3  see strong # earthquake feb 20 , 2023 # hatay ...                -1.0\n",
      "4  difficult news today top struggles already fac...                -1.0\n",
      "5  another # earthquake southern # turkey .  : //...                -1.0\n",
      "6  three people confirmed died 6.4-magnitude # ea...                -1.0\n",
      "7  6.4 magnitude quake shakes turkey -syria borde...                -1.0\n",
      "8  new 6.3 # earthquake hits # turkey , # syria b...                -1.0\n",
      "9  latest # earthquake # turkey impact # disaster...                -1.0\n"
     ]
    }
   ],
   "source": [
    "# compute recon errors\n",
    "with torch.no_grad():\n",
    "    X_tensor = torch.from_numpy(X).to(DEVICE)\n",
    "    recon = model(X_tensor)\n",
    "    mse = ((recon - X_tensor) ** 2).mean(dim=1).detach().cpu().numpy()  # per-window MSE\n",
    "\n",
    "# 95th percentile threshold\n",
    "threshold = np.percentile(mse, 95.0)\n",
    "\n",
    "# Flag anomalies: windows with error above threshold\n",
    "is_anom = mse > threshold\n",
    "\n",
    "# Map window anomalies back to tweet-level indices\n",
    "anom_indices = idx_map[is_anom]  # indices in the original df (row positions)\n",
    "df_subset['ae_recon_error'] = np.nan\n",
    "df_subset.loc[idx_map, 'ae_recon_error'] = mse  # assign errors to window-end rows\n",
    "df_subset['ae_anomaly_95p'] = False\n",
    "df_subset.loc[anom_indices, 'ae_anomaly_95p'] = True\n",
    "\n",
    "\n",
    "print(f\"Detected anomalies: {is_anom.sum()} / {len(is_anom)} windows\")\n",
    "anomalies = df_subset.loc[df_subset['ae_anomaly_95p'] == True, ['content', 'sentiment_polarity']]\n",
    "# non_anomalies = df_subset.loc[df_subset['ae_anomaly_95p'] == False, ['content', 'sentiment_polarity']]\n",
    "print(anomalies.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a523e0",
   "metadata": {
    "id": "d0a523e0"
   },
   "source": [
    "## Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250d2ca7",
   "metadata": {},
   "source": [
    "- `autoencoder`\n",
    "  - An autoencoder neural network was designed and trained to detect anomalies based on deviations in tweet sentiment patterns.\n",
    "  - The input data was structured into sequences of polarity scores.\n",
    "  - The autoencoder was implemented as a fully connected feedforward network with a three-layer encoder and symmetric decoder.\n",
    "  - The encoder consisted of a hidden layer with 64 neurons followed by a 16-neuron bottleneck, using rectified linear unit (ReLU) activations for encoding and decoding [ 42 ].\n",
    "  - Reconstruction errors (mean squared error between actual and reconstructed sequences) were calculated, and tweets with errors above the 95th percentile threshold were flagged as anomalies.\n",
    "\n",
    "- `LSTM with Attention`\n",
    "  - An LSTM neural network with an integrated attention mechanism was implemented to detect anomalies based on prediction errors.\n",
    "  - Input sequences of polarity scores were processed through LSTM layers, and attention layers were applied to selectively weigh temporal dependencies within the sequences.\n",
    "  - The LSTM with attention included a single-layer LSTM model with a hidden size of 32, followed by an attention mechanism.\n",
    "\n",
    "- Common config\n",
    "  - Both models were trained for 10 epochs using the Adam optimizer (learning rate was set to 0.001), with a batch size of 32 and mean squared error (MSE) loss.\n",
    "  - Sentiment polarity scores were normalized using MinMax scaling to the [0,1] range. The model‚Äôs output was a prediction of subsequent sentiment scores.\n",
    "  - Anomalies were identified when prediction errors exceeded a threshold set at the 95th percentile, highlighting sudden or extreme shifts (changes) in sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa78c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TBA"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V6E1",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "crisisbench-dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
