{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e9ce422-6ab0-4409-8ac3-3a19d646c957",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dcc470c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bdbb91-e226-4706-aff3-29d910ca910e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Cleanse raw dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca11d51",
   "metadata": {},
   "source": [
    "Dataset: [Turkey and Syria Earthquake Tweets](https://www.kaggle.com/datasets/swaptr/turkey-earthquake-tweets)\n",
    "\n",
    "\"For the purpose of this study, the authors used the Turkey and Syria Earthquake Tweets dataset, which is available on Kaggle [35]. The dataset contains 472,399 tweets relating to the earthquake that struck Turkey and Syria on 6 February 2023, and ends with tweets on 21 February 2023. The dataset captures real-time, user-generated tweets reflecting interactions, public responses, and reactions during the event. All tweets included a language metadata field. The dataset was filtered to include only English-language tweets, yielding 189,626 tweets for this analysis (the filtering was conducted via the provided language label)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fba25f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Load data\n",
    "\n",
    "- \"Filtering English language: After this filtering, 189,626 tweets out of 472,399 tweets were filtered as English text.\" \n",
    "- Simply used the `tweets_en.csv` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a4e11d15",
   "metadata": {
    "id": "a4e11d15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 189626\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>like_count</th>\n",
       "      <th>rt_count</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>isVerified</th>\n",
       "      <th>language</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>place</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-21 03:29:07+00:00</td>\n",
       "      <td>New search &amp;amp; rescue work is in progress in...</td>\n",
       "      <td>['Hatay', 'earthquakes', 'Türkiye', 'TurkiyeQu...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5697.0</td>\n",
       "      <td>True</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Twitter Web App</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-21 03:29:04+00:00</td>\n",
       "      <td>Can't imagine those who still haven't recovere...</td>\n",
       "      <td>['Turkey', 'earthquake', 'turkeyearthquake2023...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Twitter for Android</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-02-21 03:28:06+00:00</td>\n",
       "      <td>its a highkey sign for all of us to ponder ove...</td>\n",
       "      <td>['turkeyearthquake2023', 'earthquake', 'Syria']</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Twitter for Android</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>2023-02-21 03:27:27+00:00</td>\n",
       "      <td>See how strong was the #Earthquake of Feb 20, ...</td>\n",
       "      <td>['Earthquake', 'Hatay', 'Turkey', 'turkeyearth...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21836.0</td>\n",
       "      <td>True</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Twitter for Android</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>2023-02-21 03:27:11+00:00</td>\n",
       "      <td>More difficult news today on top of struggles ...</td>\n",
       "      <td>['Türkiye', 'Syria', 'earthquake', 'Canadians']</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>675.0</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                       date  \\\n",
       "0           1  2023-02-21 03:29:07+00:00   \n",
       "1           2  2023-02-21 03:29:04+00:00   \n",
       "2           3  2023-02-21 03:28:06+00:00   \n",
       "3           5  2023-02-21 03:27:27+00:00   \n",
       "4           6  2023-02-21 03:27:11+00:00   \n",
       "\n",
       "                                             content  \\\n",
       "0  New search &amp; rescue work is in progress in...   \n",
       "1  Can't imagine those who still haven't recovere...   \n",
       "2  its a highkey sign for all of us to ponder ove...   \n",
       "3  See how strong was the #Earthquake of Feb 20, ...   \n",
       "4  More difficult news today on top of struggles ...   \n",
       "\n",
       "                                            hashtags  like_count  rt_count  \\\n",
       "0  ['Hatay', 'earthquakes', 'Türkiye', 'TurkiyeQu...         1.0       0.0   \n",
       "1  ['Turkey', 'earthquake', 'turkeyearthquake2023...         0.0       0.0   \n",
       "2    ['turkeyearthquake2023', 'earthquake', 'Syria']         0.0       0.0   \n",
       "3  ['Earthquake', 'Hatay', 'Turkey', 'turkeyearth...         0.0       0.0   \n",
       "4    ['Türkiye', 'Syria', 'earthquake', 'Canadians']         1.0       0.0   \n",
       "\n",
       "   followers_count  isVerified language coordinates place               source  \n",
       "0           5697.0        True       en         NaN   NaN      Twitter Web App  \n",
       "1              1.0       False       en         NaN   NaN  Twitter for Android  \n",
       "2              3.0       False       en         NaN   NaN  Twitter for Android  \n",
       "3          21836.0        True       en         NaN   NaN  Twitter for Android  \n",
       "4            675.0       False       en         NaN   NaN   Twitter for iPhone  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/turkey_syria_earthquake_tweets/tweets_en.csv\")\n",
    "print(f\"N = {len(df)}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a2b97521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New search &amp; rescue work is in progress in #Hatay after two more #earthquakes hit #Türkiye’s southeastern province.  #TurkiyeQuakes #Turkey-#Syria  #Earthquake #turkeyearthquake2023  https://t.co/sd4WHByiQs\n"
     ]
    }
   ],
   "source": [
    "print(df.loc[0, 'content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02411611",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Text lowercasing\n",
    "\n",
    "\"All tweets were converted to lowercase; according to Hickman et al. [37], lowercasing tends to be beneficial because it reduces data dimensionality, thereby increasing statistical power, and usually does not reduce validity.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f74ca052",
   "metadata": {
    "executionInfo": {
     "elapsed": 153,
     "status": "ok",
     "timestamp": 1762294699522,
     "user": {
      "displayName": "Jade Kim",
      "userId": "13751362502685406790"
     },
     "user_tz": 300
    },
    "id": "f74ca052"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new search &amp; rescue work is in progress in #hatay after two more #earthquakes hit #türkiye’s southeastern province.  #turkiyequakes #turkey-#syria  #earthquake #turkeyearthquake2023  https://t.co/sd4whbyiqs\n"
     ]
    }
   ],
   "source": [
    "df['content'] = df['content'].str.lower()\n",
    "print(df.loc[0, 'content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162319cc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Stop word removal\n",
    "\n",
    "- Stop word removal was useful in traditional NLP models, but not so effective for DL models. \n",
    "- The original paper compared the traditional NLP models with DL models so they used it. \n",
    "  - \"common English (function) words such as “and”, “is”, “I”, “am”, “what”, “of”, etc. were removed by using the Natural Language Toolkit (NLTK). Stop word removal has the advantages of reducing the size of the stored dataset and improving the overall efficiency and effectiveness of the analysis [38].\"\n",
    "\n",
    "- However, **we are only training DL models in our analysis so we are not applying it**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5800ed7d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21153,
     "status": "ok",
     "timestamp": 1762294724071,
     "user": {
      "displayName": "Jade Kim",
      "userId": "13751362502685406790"
     },
     "user_tz": 300
    },
    "id": "5800ed7d",
    "outputId": "d3dc1c32-711a-4e81-d3c0-d607c1c5439d"
   },
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.tokenize import word_tokenize\n",
    "\n",
    "# NLTK_LOCAL_PATH = \"./nltk_data\"\n",
    "# os.makedirs(NLTK_LOCAL_PATH, exist_ok=True)\n",
    "# nltk.data.path.append(NLTK_LOCAL_PATH)\n",
    "# # nltk.download('stopwords', download_dir=NLTK_LOCAL_PATH)\n",
    "# # nltk.download('punkt', download_dir=NLTK_LOCAL_PATH)\n",
    "\n",
    "# # Get English stop words\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# # Function to remove stop words from text\n",
    "# def remove_stopwords(text):\n",
    "#     if pd.isna(text):\n",
    "#         return text\n",
    "#     # Tokenize the text\n",
    "#     words = word_tokenize(text)\n",
    "#     # Remove stop words and return as string\n",
    "#     filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "#     return ' '.join(filtered_words)\n",
    "\n",
    "# df['content'] = df['content'].apply(remove_stopwords)\n",
    "# print(df.loc[0, 'content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fed112",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### URLs removal\n",
    "\n",
    "\"All URLs were removed from tweets, since the text of URL strings does not necessarily convey any relevant information,  and can therefore be removed [39].\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "71530407",
   "metadata": {
    "executionInfo": {
     "elapsed": 142,
     "status": "ok",
     "timestamp": 1762294724216,
     "user": {
      "displayName": "Jade Kim",
      "userId": "13751362502685406790"
     },
     "user_tz": 300
    },
    "id": "71530407"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new search &amp; rescue work is in progress in #hatay after two more #earthquakes hit #türkiye’s southeastern province.  #turkiyequakes #turkey-#syria  #earthquake #turkeyearthquake2023  \n"
     ]
    }
   ],
   "source": [
    "df['content'] = df['content'].str.replace(r'http\\S+', '', regex=True)\n",
    "print(df.loc[0, 'content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f64f950",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Duplicate removal\n",
    "\n",
    "\"All duplicate tweets were removed to eliminate redundancy and possible skewing of the results.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6f7b55b3",
   "metadata": {
    "executionInfo": {
     "elapsed": 81,
     "status": "ok",
     "timestamp": 1762294724300,
     "user": {
      "displayName": "Jade Kim",
      "userId": "13751362502685406790"
     },
     "user_tz": 300
    },
    "id": "6f7b55b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 189626\n",
      "N = 180915\n"
     ]
    }
   ],
   "source": [
    "print(f\"N = {len(df)}\")\n",
    "df = df.drop_duplicates(subset='content', keep='first')\n",
    "print(f\"N = {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6b4793",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Remove unncessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "73dfe962-df76-4e72-bd53-225e7be2a22a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>like_count</th>\n",
       "      <th>rt_count</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>isVerified</th>\n",
       "      <th>language</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>place</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-02-21 03:29:07+00:00</td>\n",
       "      <td>new search &amp;amp; rescue work is in progress in...</td>\n",
       "      <td>['Hatay', 'earthquakes', 'Türkiye', 'TurkiyeQu...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5697.0</td>\n",
       "      <td>True</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Twitter Web App</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-02-21 03:29:04+00:00</td>\n",
       "      <td>can't imagine those who still haven't recovere...</td>\n",
       "      <td>['Turkey', 'earthquake', 'turkeyearthquake2023...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Twitter for Android</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-02-21 03:28:06+00:00</td>\n",
       "      <td>its a highkey sign for all of us to ponder ove...</td>\n",
       "      <td>['turkeyearthquake2023', 'earthquake', 'Syria']</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Twitter for Android</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-02-21 03:27:27+00:00</td>\n",
       "      <td>see how strong was the #earthquake of feb 20, ...</td>\n",
       "      <td>['Earthquake', 'Hatay', 'Turkey', 'turkeyearth...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21836.0</td>\n",
       "      <td>True</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Twitter for Android</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-02-21 03:27:11+00:00</td>\n",
       "      <td>more difficult news today on top of struggles ...</td>\n",
       "      <td>['Türkiye', 'Syria', 'earthquake', 'Canadians']</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>675.0</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        date  \\\n",
       "0  2023-02-21 03:29:07+00:00   \n",
       "1  2023-02-21 03:29:04+00:00   \n",
       "2  2023-02-21 03:28:06+00:00   \n",
       "3  2023-02-21 03:27:27+00:00   \n",
       "4  2023-02-21 03:27:11+00:00   \n",
       "\n",
       "                                             content  \\\n",
       "0  new search &amp; rescue work is in progress in...   \n",
       "1  can't imagine those who still haven't recovere...   \n",
       "2  its a highkey sign for all of us to ponder ove...   \n",
       "3  see how strong was the #earthquake of feb 20, ...   \n",
       "4  more difficult news today on top of struggles ...   \n",
       "\n",
       "                                            hashtags  like_count  rt_count  \\\n",
       "0  ['Hatay', 'earthquakes', 'Türkiye', 'TurkiyeQu...         1.0       0.0   \n",
       "1  ['Turkey', 'earthquake', 'turkeyearthquake2023...         0.0       0.0   \n",
       "2    ['turkeyearthquake2023', 'earthquake', 'Syria']         0.0       0.0   \n",
       "3  ['Earthquake', 'Hatay', 'Turkey', 'turkeyearth...         0.0       0.0   \n",
       "4    ['Türkiye', 'Syria', 'earthquake', 'Canadians']         1.0       0.0   \n",
       "\n",
       "   followers_count  isVerified language coordinates place               source  \n",
       "0           5697.0        True       en         NaN   NaN      Twitter Web App  \n",
       "1              1.0       False       en         NaN   NaN  Twitter for Android  \n",
       "2              3.0       False       en         NaN   NaN  Twitter for Android  \n",
       "3          21836.0        True       en         NaN   NaN  Twitter for Android  \n",
       "4            675.0       False       en         NaN   NaN   Twitter for iPhone  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(columns=['Unnamed: 0'], errors='ignore')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27389518-1c56-4901-8cca-d5b16675cde2",
   "metadata": {},
   "source": [
    "#### Exclude location info\n",
    "\n",
    "\"Location information was excluded from all subsequent modelling because 96% of the tweets lacked geolocation.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5866eb9e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1762294724336,
     "user": {
      "displayName": "Jade Kim",
      "userId": "13751362502685406790"
     },
     "user_tz": 300
    },
    "id": "5866eb9e",
    "outputId": "254ef374-e187-4e3e-db31-5541da746211"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>like_count</th>\n",
       "      <th>rt_count</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>isVerified</th>\n",
       "      <th>language</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-02-21 03:29:07+00:00</td>\n",
       "      <td>new search &amp;amp; rescue work is in progress in...</td>\n",
       "      <td>['Hatay', 'earthquakes', 'Türkiye', 'TurkiyeQu...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5697.0</td>\n",
       "      <td>True</td>\n",
       "      <td>en</td>\n",
       "      <td>Twitter Web App</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-02-21 03:29:04+00:00</td>\n",
       "      <td>can't imagine those who still haven't recovere...</td>\n",
       "      <td>['Turkey', 'earthquake', 'turkeyearthquake2023...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>Twitter for Android</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-02-21 03:28:06+00:00</td>\n",
       "      <td>its a highkey sign for all of us to ponder ove...</td>\n",
       "      <td>['turkeyearthquake2023', 'earthquake', 'Syria']</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>Twitter for Android</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-02-21 03:27:27+00:00</td>\n",
       "      <td>see how strong was the #earthquake of feb 20, ...</td>\n",
       "      <td>['Earthquake', 'Hatay', 'Turkey', 'turkeyearth...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21836.0</td>\n",
       "      <td>True</td>\n",
       "      <td>en</td>\n",
       "      <td>Twitter for Android</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-02-21 03:27:11+00:00</td>\n",
       "      <td>more difficult news today on top of struggles ...</td>\n",
       "      <td>['Türkiye', 'Syria', 'earthquake', 'Canadians']</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>675.0</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        date  \\\n",
       "0  2023-02-21 03:29:07+00:00   \n",
       "1  2023-02-21 03:29:04+00:00   \n",
       "2  2023-02-21 03:28:06+00:00   \n",
       "3  2023-02-21 03:27:27+00:00   \n",
       "4  2023-02-21 03:27:11+00:00   \n",
       "\n",
       "                                             content  \\\n",
       "0  new search &amp; rescue work is in progress in...   \n",
       "1  can't imagine those who still haven't recovere...   \n",
       "2  its a highkey sign for all of us to ponder ove...   \n",
       "3  see how strong was the #earthquake of feb 20, ...   \n",
       "4  more difficult news today on top of struggles ...   \n",
       "\n",
       "                                            hashtags  like_count  rt_count  \\\n",
       "0  ['Hatay', 'earthquakes', 'Türkiye', 'TurkiyeQu...         1.0       0.0   \n",
       "1  ['Turkey', 'earthquake', 'turkeyearthquake2023...         0.0       0.0   \n",
       "2    ['turkeyearthquake2023', 'earthquake', 'Syria']         0.0       0.0   \n",
       "3  ['Earthquake', 'Hatay', 'Turkey', 'turkeyearth...         0.0       0.0   \n",
       "4    ['Türkiye', 'Syria', 'earthquake', 'Canadians']         1.0       0.0   \n",
       "\n",
       "   followers_count  isVerified language               source  \n",
       "0           5697.0        True       en      Twitter Web App  \n",
       "1              1.0       False       en  Twitter for Android  \n",
       "2              3.0       False       en  Twitter for Android  \n",
       "3          21836.0        True       en  Twitter for Android  \n",
       "4            675.0       False       en   Twitter for iPhone  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(columns=['coordinates', 'place'], errors='ignore')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7180724",
   "metadata": {
    "id": "c7180724",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Sentiment Analysis\n",
    "\n",
    "- \"It should be noted that this dataset did not come with sentiment annotations. Therefore, sentiment labels were assigned using a pre-trained BERT sentiment model, rather than manual human annotations.\"\n",
    "- use pre-trained transformer-based `BERT` model to add a sentiment score fore each sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5a9cd2",
   "metadata": {},
   "source": [
    "\"Sentiment analysis was performed utilizing a pre-trained transformer-based BERT model, specifically the `nlptown/bert-base-multilingual-uncased-sentiment`. This model is a fine-tuned version of `bert-base-multilingual-uncased`, which is optimized for sentiment analysis across six languages: English, Dutch, German, French, Spanish and Italian [40].\"\n",
    "\n",
    "#### Reference\n",
    "[40] Lakhanpal, S.; Gupta, A.; Agrawal, R. Leveraging Explainable AI to Analyze Researchers’ Aspect-Based Sentiment About ChatGPT. In Proceedings of the 15th International Conference on Intelligent Human Computer Interaction (IHCI 2023), Daegu, Republic of Korea, 8–10 November 2023; pp. 281–290."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "20c75d25",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10640,
     "status": "ok",
     "timestamp": 1762294735008,
     "user": {
      "displayName": "Jade Kim",
      "userId": "13751362502685406790"
     },
     "user_tz": 300
    },
    "id": "20c75d25",
    "outputId": "7e06ead2-ed3e-4af0-faa0-90a0aeee00be"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "save_dir = \"./local_bert_multilingual\"\n",
    "\n",
    "# Download once\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# tokenizer.save_pretrained(save_dir)\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "# model.save_pretrained(save_dir)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_dir)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(save_dir)\n",
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0247df-a6c2-433d-a867-cf9d11fb9833",
   "metadata": {},
   "source": [
    "\"Tweets were tokenized using the AutoTokenizer from HuggingFace Transformers, truncated to a maximum length of 512 tokens [41]. The model predicted sentiment scores across five classes representing very negative to very positive sentiments. These categorical outputs were then converted to a continuous polarity scale ranging from −1 (strongly negative) to +1 (strongly positive) to facilitate the temporal analysis of sentiment fluctuations.\"\n",
    "\n",
    "- polarity = (star_rating - 3) / 2\n",
    "- discrete star_rating  = \\[0,1,2,3,4\\] => continuous polarity = (-1.0, -0.5, 0.0, 0.5, 1.0)\n",
    "\n",
    "#### Reference\n",
    "[41] Hussain, Z.; Binz, M.; Mata, R.; Wulff, D.U. A tutorial on open-source large language models for behavioral science. Behav. Res. 2024, 56, 8214–8237."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "20c15946-cc98-4925-9a18-b2e85fecb325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 180915\n",
      "batch_size: 32\n",
      "5653 batchs + 19\n"
     ]
    }
   ],
   "source": [
    "inputs = df['content'].tolist()\n",
    "print(f\"N = {len(inputs)}\")\n",
    "batch_size = 32\n",
    "print(f\"batch_size: {batch_size}\")\n",
    "print(f\"{len(inputs)//batch_size} batchs + {len(inputs)%batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d-XBPpxs99DR",
   "metadata": {
    "executionInfo": {
     "elapsed": 43109,
     "status": "ok",
     "timestamp": 1762294778119,
     "user": {
      "displayName": "Jade Kim",
      "userId": "13751362502685406790"
     },
     "user_tz": 300
    },
    "id": "d-XBPpxs99DR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading ./data/turkey_syria_earthquake_tweets/polarity_scores.pt\n",
      "torch.Size([180915])\n"
     ]
    }
   ],
   "source": [
    "polarity_scores_path = \"./data/turkey_syria_earthquake_tweets/polarity_scores.pt\"\n",
    "outputs_dir = \"./data/turkey_syria_earthquake_tweets/polarity_batches\"\n",
    "os.makedirs(outputs_dir, exist_ok=True)\n",
    "\n",
    "if not os.path.exists(polarity_scores_path):\n",
    "    all_polarities = []\n",
    "    \n",
    "    num_inputs = len(inputs)\n",
    "    num_batches = (num_inputs + batch_size - 1) // batch_size\n",
    "    \n",
    "    for batch_idx, start_idx in enumerate(range(0, num_inputs, batch_size)):\n",
    "        end_idx = min(start_idx + batch_size, num_inputs)\n",
    "        batch_path = os.path.join(outputs_dir, f\"{int(batch_idx)}.pt\")\n",
    "    \n",
    "        if os.path.exists(batch_path):\n",
    "            print(f\"[skip] {batch_idx+1}/{num_batches} ({start_idx}~{end_idx}) already exists\")\n",
    "            continue\n",
    "        print(f\"[compute] {batch_idx+1}/{num_batches}: {start_idx}~{end_idx}\")\n",
    "    \n",
    "        batch = inputs[start_idx:end_idx]\n",
    "        model_inputs = tokenizer(batch, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "        \n",
    "        # Call BERT and get predicted labels\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**model_inputs)\n",
    "    \n",
    "        logits = outputs.logits\n",
    "        probabilities = torch.softmax(logits, dim=-1)\n",
    "        predicted_labels = torch.argmax(probabilities, dim=-1)\n",
    "    \n",
    "        # Convert to polarity scale as stated in the paper\n",
    "        star_ratings = predicted_labels + 1\n",
    "        polarity_scores = (star_ratings - 3) / 2.0\n",
    "        all_polarities.append(polarity_scores)\n",
    "    \n",
    "        # save each batch\n",
    "        torch.save(polarity_scores, batch_path)\n",
    "        print(f\"saved: {batch_path}\")\n",
    "    \n",
    "    polarity_scores = torch.cat(all_polarities, dim=0)\n",
    "    torch.save(polarity_scores, polarity_scores_path)\n",
    "    print(f\"saved to {polarity_scores_path}\")\n",
    "else:\n",
    "    print(f\"loading {polarity_scores_path}\")\n",
    "    polarity_scores = torch.load(polarity_scores_path, weights_only=True)\n",
    "\n",
    "print(polarity_scores.shape)\n",
    "assert len(polarity_scores) == len(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304ba44e-354a-488a-b65c-cd01fa17f6cc",
   "metadata": {},
   "source": [
    "Assign sentiment polarity (-1 ~ +1) to each tweet & add normalized polarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c8c59066-64a4-49e9-8b92-b2d390c6f7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of NA data: 0\n",
      "interpolation not needed\n"
     ]
    }
   ],
   "source": [
    "df['sentiment_polarity'] = polarity_scores.numpy()\n",
    "print(f\"number of NA data: {df['sentiment_polarity'].isna().sum()}\")\n",
    "if df['sentiment_polarity'].isna().sum() > 0:\n",
    "    pol = pol.interpolate(limit_direction=\"both\")\n",
    "else:\n",
    "    print(\"interpolation not needed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0973a2dd-65a6-4604-95ee-9b63ae7cd2f0",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "\"Sentiment polarity scores were normalized using MinMax scaling to the [0,1] range.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "97a2fbe2-80e3-4f88-9545-c1c47587ca10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>like_count</th>\n",
       "      <th>rt_count</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>isVerified</th>\n",
       "      <th>language</th>\n",
       "      <th>source</th>\n",
       "      <th>sentiment_polarity</th>\n",
       "      <th>sentiment_polarity_norm</th>\n",
       "      <th>sentiment_polarity_min_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-02-21 03:29:07+00:00</td>\n",
       "      <td>new search &amp;amp; rescue work is in progress in...</td>\n",
       "      <td>['Hatay', 'earthquakes', 'Türkiye', 'TurkiyeQu...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5697.0</td>\n",
       "      <td>True</td>\n",
       "      <td>en</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.011136</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-02-21 03:29:04+00:00</td>\n",
       "      <td>can't imagine those who still haven't recovere...</td>\n",
       "      <td>['Turkey', 'earthquake', 'turkeyearthquake2023...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.689723</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-02-21 03:28:06+00:00</td>\n",
       "      <td>its a highkey sign for all of us to ponder ove...</td>\n",
       "      <td>['turkeyearthquake2023', 'earthquake', 'Syria']</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.578089</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-02-21 03:27:27+00:00</td>\n",
       "      <td>see how strong was the #earthquake of feb 20, ...</td>\n",
       "      <td>['Earthquake', 'Hatay', 'Turkey', 'turkeyearth...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21836.0</td>\n",
       "      <td>True</td>\n",
       "      <td>en</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.689723</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-02-21 03:27:11+00:00</td>\n",
       "      <td>more difficult news today on top of struggles ...</td>\n",
       "      <td>['Türkiye', 'Syria', 'earthquake', 'Canadians']</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>675.0</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.122770</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        date  \\\n",
       "0  2023-02-21 03:29:07+00:00   \n",
       "1  2023-02-21 03:29:04+00:00   \n",
       "2  2023-02-21 03:28:06+00:00   \n",
       "3  2023-02-21 03:27:27+00:00   \n",
       "4  2023-02-21 03:27:11+00:00   \n",
       "\n",
       "                                             content  \\\n",
       "0  new search &amp; rescue work is in progress in...   \n",
       "1  can't imagine those who still haven't recovere...   \n",
       "2  its a highkey sign for all of us to ponder ove...   \n",
       "3  see how strong was the #earthquake of feb 20, ...   \n",
       "4  more difficult news today on top of struggles ...   \n",
       "\n",
       "                                            hashtags  like_count  rt_count  \\\n",
       "0  ['Hatay', 'earthquakes', 'Türkiye', 'TurkiyeQu...         1.0       0.0   \n",
       "1  ['Turkey', 'earthquake', 'turkeyearthquake2023...         0.0       0.0   \n",
       "2    ['turkeyearthquake2023', 'earthquake', 'Syria']         0.0       0.0   \n",
       "3  ['Earthquake', 'Hatay', 'Turkey', 'turkeyearth...         0.0       0.0   \n",
       "4    ['Türkiye', 'Syria', 'earthquake', 'Canadians']         1.0       0.0   \n",
       "\n",
       "   followers_count  isVerified language               source  \\\n",
       "0           5697.0        True       en      Twitter Web App   \n",
       "1              1.0       False       en  Twitter for Android   \n",
       "2              3.0       False       en  Twitter for Android   \n",
       "3          21836.0        True       en  Twitter for Android   \n",
       "4            675.0       False       en   Twitter for iPhone   \n",
       "\n",
       "   sentiment_polarity  sentiment_polarity_norm  sentiment_polarity_min_max  \n",
       "0                 0.5                 1.011136                        0.75  \n",
       "1                -1.0                -0.689723                        0.00  \n",
       "2                 1.0                 1.578089                        1.00  \n",
       "3                -1.0                -0.689723                        0.00  \n",
       "4                -0.5                -0.122770                        0.25  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean and normalize polarity data\n",
    "\n",
    "pol = df['sentiment_polarity'].astype(float).copy()\n",
    "\n",
    "# normalize\n",
    "pol = df['sentiment_polarity'].astype(float).copy()\n",
    "pol_mean, pol_std = pol.mean(), pol.std() if pol.std() > 0 else 1.0\n",
    "df['sentiment_polarity_norm'] = (pol - pol_mean) / pol_std\n",
    "\n",
    "# MinMax scaling\n",
    "pol_min = pol.min()\n",
    "pol_max = pol.max()\n",
    "range_val = pol_max - pol_min if pol_max - pol_min > 0 else 1.0\n",
    "df['sentiment_polarity_min_max'] = (pol - pol_min) / range_val\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c81a58e-77d1-4bc1-8a89-90b2a0bcf2a6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Save preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ca060c6f-3eec-4b09-9a92-bac524b7b80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ./data/turkey_syria_earthquake_tweets/preprocessed_data.csv\n"
     ]
    }
   ],
   "source": [
    "output_path = \"./data/turkey_syria_earthquake_tweets/preprocessed_data.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "print(\"Saved:\", output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a9ab23-f031-44a3-88cd-4d08f1f6381c",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733ec020-a4bb-430f-b561-1f0367e5f486",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc7346fe-d763-4501-9a45-1c4c7a9ab363",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0869bbce-6ad5-4929-9439-c2f4bc6249f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "generator = torch.Generator()\n",
    "_ = generator.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f83365a1-379c-4bc6-bbcc-eb8ca273209f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 180915\n",
      "date is descending (future => past), reversing...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>like_count</th>\n",
       "      <th>rt_count</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>isVerified</th>\n",
       "      <th>language</th>\n",
       "      <th>source</th>\n",
       "      <th>sentiment_polarity</th>\n",
       "      <th>sentiment_polarity_norm</th>\n",
       "      <th>sentiment_polarity_min_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-02-06 00:01:03+00:00</td>\n",
       "      <td>from istanbul to new york – discovering the la...</td>\n",
       "      <td>['ottomanempire', 'ottoman', 'osman', 'osmanbe...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.578089</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-02-06 00:01:25+00:00</td>\n",
       "      <td>#earthquake (#sismo) m2.8 strikes 72 km ne of ...</td>\n",
       "      <td>['Earthquake', 'sismo', 'Calama', 'Chile']</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>44544.0</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>emsc-csem</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.689723</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-02-06 00:04:02+00:00</td>\n",
       "      <td>all they toys experimented in #syria for years...</td>\n",
       "      <td>['Syria', 'Ukraine', 'February2023']</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>679.0</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.689723</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-02-06 00:04:34+00:00</td>\n",
       "      <td>@kalllllenberger @jarrodjmorris the usa has it...</td>\n",
       "      <td>['propaganda', 'Turkey']</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>157308.0</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.578089</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-02-06 00:05:04+00:00</td>\n",
       "      <td>usgs reports a m1.16 earthquake, 14km n of bor...</td>\n",
       "      <td>['earthquake']</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>29666.0</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>everyEarthquake</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.689723</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        date  \\\n",
       "0  2023-02-06 00:01:03+00:00   \n",
       "1  2023-02-06 00:01:25+00:00   \n",
       "2  2023-02-06 00:04:02+00:00   \n",
       "3  2023-02-06 00:04:34+00:00   \n",
       "4  2023-02-06 00:05:04+00:00   \n",
       "\n",
       "                                             content  \\\n",
       "0  from istanbul to new york – discovering the la...   \n",
       "1  #earthquake (#sismo) m2.8 strikes 72 km ne of ...   \n",
       "2  all they toys experimented in #syria for years...   \n",
       "3  @kalllllenberger @jarrodjmorris the usa has it...   \n",
       "4  usgs reports a m1.16 earthquake, 14km n of bor...   \n",
       "\n",
       "                                            hashtags  like_count  rt_count  \\\n",
       "0  ['ottomanempire', 'ottoman', 'osman', 'osmanbe...         0.0       0.0   \n",
       "1         ['Earthquake', 'sismo', 'Calama', 'Chile']         3.0       2.0   \n",
       "2               ['Syria', 'Ukraine', 'February2023']         0.0       0.0   \n",
       "3                           ['propaganda', 'Turkey']         0.0       0.0   \n",
       "4                                     ['earthquake']         2.0       2.0   \n",
       "\n",
       "   followers_count  isVerified language               source  \\\n",
       "0             66.0       False       en  Twitter for Android   \n",
       "1          44544.0       False       en            emsc-csem   \n",
       "2            679.0       False       en   Twitter for iPhone   \n",
       "3         157308.0       False       en      Twitter Web App   \n",
       "4          29666.0       False       en      everyEarthquake   \n",
       "\n",
       "   sentiment_polarity  sentiment_polarity_norm  sentiment_polarity_min_max  \n",
       "0                 1.0                 1.578089                         1.0  \n",
       "1                -1.0                -0.689723                         0.0  \n",
       "2                -1.0                -0.689723                         0.0  \n",
       "3                 1.0                 1.578089                         1.0  \n",
       "4                -1.0                -0.689723                         0.0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_data(past_to_future = True):\n",
    "    output_path = \"./data/turkey_syria_earthquake_tweets/preprocessed_data.csv\"\n",
    "    df = pd.read_csv(output_path)\n",
    "    print(f\"N = {len(df)}\")\n",
    "\n",
    "    if past_to_future:\n",
    "        if df['date'].is_monotonic_decreasing:\n",
    "            print(\"date is descending (future => past), reversing...\")\n",
    "            df = df.iloc[::-1].reset_index(drop=True) # df = df.sort_values(\"date\").reset_index(drop=True)\n",
    "        assert df['date'].is_monotonic_increasing\n",
    "\n",
    "    return df\n",
    "\n",
    "df = load_data()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec095b8-b805-43d6-9d25-362e3cc168ac",
   "metadata": {},
   "source": [
    "## Anomaly Detection: Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9144abe8-0f20-4fde-9eca-4698c1a7342c",
   "metadata": {},
   "source": [
    "#### Implementation\n",
    "- An autoencoder neural network was designed and trained to detect anomalies based on deviations in tweet sentiment patterns.\n",
    "- The input data was structured into sequences of polarity scores.\n",
    "- The autoencoder was implemented as a fully connected feedforward network with a three-layer encoder and symmetric decoder.\n",
    "- The encoder consisted of a hidden layer with 64 neurons followed by a 16-neuron bottleneck, using rectified linear unit (ReLU) activations for encoding and decoding [42].\n",
    "- Reconstruction errors (mean squared error between actual and reconstructed sequences) were calculated, and tweets with errors above the 95th percentile threshold were flagged as anomalies.\n",
    "\n",
    "#### Common\n",
    "- Both models were trained for 10 epochs using the Adam optimizer (learning rate was set to 0.001), with a batch size of 32 and mean squared error (MSE) loss.\n",
    "- Sentiment polarity scores were normalized using MinMax scaling to the [0,1] range. The model’s output was a prediction of subsequent sentiment scores.\n",
    "- Anomalies were identified when prediction errors exceeded a threshold set at the 95th percentile, highlighting sudden or extreme shifts (changes) in sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2966f2e6-ea30-4534-afe4-bce55644533c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trained for 10 epochs using the Adam optimizer (learning rate was set to 0.001), with a batch size of 32 and mean squared error (MSE) loss.\n",
    "num_epochs = 10\n",
    "lr = 0.001 # 1e-3\n",
    "batch_size = 32\n",
    "\n",
    "# number of tweets in each sequence (not specified in the paper)\n",
    "WINDOW_SIZE = 32\n",
    "\n",
    "HIDDEN_DIM = 64\n",
    "BOTTLE_NECK_DIM = 16\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9fe0f83-d892-42b3-91f0-64f489b44441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=180915\n",
      "X shape: (180884, 32)\n",
      "len(end_indices): 180884\n"
     ]
    }
   ],
   "source": [
    "# The input data was structured into sequences of polarity scores.\n",
    "# Sentiment polarity scores were normalized using MinMax scaling to the [0,1] range. The model’s output was a prediction of subsequent sentiment scores.\n",
    "def sequences_by_window(window_size):\n",
    "    sentiment_polarity = df[\"sentiment_polarity_min_max\"].to_numpy(dtype=\"float32\")\n",
    "    \n",
    "    seqs = []\n",
    "    end_indices = [] # index of the last tweet in each sequence (map errors to this tweet)\n",
    "    for i in range(len(sentiment_polarity) - window_size + 1):\n",
    "        seqs.append(sentiment_polarity[i : i + window_size])\n",
    "        end_indices.append(i + window_size - 1)\n",
    "    \n",
    "    X = np.stack(seqs)\n",
    "    end_indices = np.array(end_indices)\n",
    "\n",
    "    print(f\"N={len(df)}\")\n",
    "    print(\"X shape:\", X.shape) # (num_sequences, WINDOW_SIZE)\n",
    "    print(\"len(end_indices):\", len(end_indices)) # [num_sequences]\n",
    "        \n",
    "    return X, end_indices\n",
    "\n",
    "X, end_indices = sequences_by_window(WINDOW_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f18965ba-298b-4777-801b-ecb03669a2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The autoencoder was implemented as a fully connected feedforward network with a three-layer encoder and symmetric decoder.\n",
    "# The encoder consisted of a hidden layer with 64 neurons followed by a 16-neuron bottleneck, using rectified linear unit (ReLU) activations for encoding and decoding [42].\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim = WINDOW_SIZE, hidden_dim = HIDDEN_DIM, bottleneck_dim = BOTTLE_NECK_DIM):\n",
    "        super().__init__()\n",
    "\n",
    "        # input_dim -> 64 -> 16\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, bottleneck_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # 16 -> 64 -> input_dim\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(bottleneck_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64a113b1-341c-4302-9dd3-922f7929f0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - loss=0.109235\n",
      "Epoch 2/10 - loss=0.088726\n",
      "Epoch 3/10 - loss=0.080445\n",
      "Epoch 4/10 - loss=0.071651\n",
      "Epoch 5/10 - loss=0.066949\n",
      "Epoch 6/10 - loss=0.065066\n",
      "Epoch 7/10 - loss=0.063888\n",
      "Epoch 8/10 - loss=0.063442\n",
      "Epoch 9/10 - loss=0.063062\n",
      "Epoch 10/10 - loss=0.062402\n",
      "\n",
      "Total training time: 16.83 sec\n"
     ]
    }
   ],
   "source": [
    "# trained for 10 epochs using the Adam optimizer (learning rate was set to 0.001), with a batch size of 32 and mean squared error (MSE) loss.\n",
    "\n",
    "# model & config\n",
    "num_epochs = 10\n",
    "lr = 0.001 # 1e-3\n",
    "batch_size = 32\n",
    "\n",
    "model = Autoencoder()\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# data\n",
    "X_tensor = torch.from_numpy(X)  # shape: (num_sequences, window_size)\n",
    "dataset = TensorDataset(X_tensor)\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, generator=generator) # NOTE: shuffle with seed=42 with generator\n",
    "\n",
    "# train\n",
    "model.train()\n",
    "train_start = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for (batch_x,) in loader:\n",
    "        batch_x = batch_x.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        recon = model(batch_x)\n",
    "        loss = criterion(recon, batch_x)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item() * batch_x.size(0)\n",
    "\n",
    "    epoch_loss /= len(dataset)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - loss={epoch_loss:.6f}\")\n",
    "\n",
    "train_end = time.time()\n",
    "print(f\"\\nTotal training time: {(train_end - train_start):.2f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8256baad-4537-4703-88a5-73fe0b014fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num anomalous sequences: 9045\n"
     ]
    }
   ],
   "source": [
    "# Reconstruction errors (mean squared error between actual and reconstructed sequences) were calculated,\n",
    "# and tweets with errors above the 95th percentile threshold were flagged as anomalies.\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    X_tensor = torch.from_numpy(X)\n",
    "    X_recon = model(X_tensor).numpy()\n",
    "\n",
    "# MSE per sequence\n",
    "recon_errors = ((X - X_recon) ** 2).mean(axis=1)  # shape: (num_sequences,)\n",
    "\n",
    "# compute 95th percentile threshold\n",
    "threshold = np.quantile(recon_errors, 0.95)\n",
    "\n",
    "# add anomaly flag on the tweets\n",
    "df[\"autoencoder_reconstruction_error\"] = np.nan\n",
    "df.loc[end_indices, \"autoencoder_reconstruction_error\"] = recon_errors\n",
    "df[\"autoencoder_is_anomaly\"] = df[\"autoencoder_reconstruction_error\"] > threshold\n",
    "\n",
    "# sequence level summary\n",
    "anomalous_seq_mask = recon_errors > threshold\n",
    "print(\"num anomalous sequences:\", anomalous_seq_mask.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f523f80-0254-42d0-b7bf-dfd22d828d7f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Anomaly Detection: LSTM with Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54eed613-cfe5-4c78-813e-8528fc665b80",
   "metadata": {},
   "source": [
    "#### Implementation\n",
    "- An LSTM neural network with an integrated attention mechanism was implemented to detect anomalies based on prediction errors.\n",
    "- Input sequences of polarity scores were processed through LSTM layers, and attention layers were applied to selectively weigh temporal dependencies within the sequences.\n",
    "- The LSTM with attention included a single-layer LSTM model with a hidden size of 32, followed by an attention mechanism.\n",
    "\n",
    "#### Common config\n",
    "- Both models were trained for 10 epochs using the Adam optimizer (learning rate was set to 0.001), with a batch size of 32 and mean squared error (MSE) loss.\n",
    "- Sentiment polarity scores were normalized using MinMax scaling to the [0,1] range. The model’s output was a prediction of subsequent sentiment scores.\n",
    "- Anomalies were identified when prediction errors exceeded a threshold set at the 95th percentile, highlighting sudden or extreme shifts (changes) in sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d95f286-78ab-4f63-a7e8-f4cb15d500ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trained for 10 epochs using the Adam optimizer (learning rate was set to 0.001), with a batch size of 32 and mean squared error (MSE) loss.\n",
    "num_epochs = 10\n",
    "lr = 0.001 # 1e-3\n",
    "batch_size = 32\n",
    "\n",
    "# number of tweets in each sequence (not specified in the paper)\n",
    "WINDOW_SIZE = 32\n",
    "\n",
    "HIDDEN_DIM = 32\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "beb7524b-0d3a-47e2-b4de-1818f67194ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=180915\n",
      "X shape: (180883, 32)\n",
      "y shape: (180883,)\n",
      "len(target_indices): 180883\n"
     ]
    }
   ],
   "source": [
    "# number of tweets in each sequence (not specified in the paper)\n",
    "WINDOW_SIZE = 32\n",
    "\n",
    "def sequenctial_prediction(window_size):\n",
    "    sentiment_polarity = df[\"sentiment_polarity_min_max\"].to_numpy(dtype=\"float32\")\n",
    "    \n",
    "    seqs = []\n",
    "    targets = []\n",
    "    target_indices = [] # predict target tweet index (map errors to this tweet)\n",
    "\n",
    "    # predict the next tweet(i+WINDOW_SIZE) based on the input (i ~ i+WINDOW_SIZE-1)\n",
    "    for i in range(len(sentiment_polarity) - WINDOW_SIZE):\n",
    "        seq = sentiment_polarity[i : i + WINDOW_SIZE] # input sequence\n",
    "        target = sentiment_polarity[i + WINDOW_SIZE]# next tweet polarity\n",
    "\n",
    "        seqs.append(seq)\n",
    "        targets.append(target)\n",
    "        target_indices.append(i + WINDOW_SIZE)\n",
    "\n",
    "    X = np.stack(seqs).astype(\"float32\") # (num_sequences, WINDOW_SIZE)\n",
    "    y = np.array(targets, dtype=\"float32\") # (num_sequences,)\n",
    "\n",
    "    print(f\"N={len(df)}\")\n",
    "    print(\"X shape:\", X.shape) # (num_sequences, WINDOW_SIZE)\n",
    "    print(\"y shape:\", y.shape) # (num_sequences,)\n",
    "    print(\"len(target_indices):\", len(target_indices))\n",
    "\n",
    "    return X, y, target_indices\n",
    "\n",
    "X, y, target_indices = sequenctial_prediction(WINDOW_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66e7d859-98a8-4b0e-ba23-b45a63acdc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input sequences of polarity scores were processed through LSTM layers, and attention layers were applied to selectively weigh temporal dependencies within the sequences.\n",
    "# The LSTM with attention included a single-layer LSTM model with a hidden size of 32, followed by an attention mechanism.\n",
    "\n",
    "class LSTMAttention(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=HIDDEN_DIM):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # single-layer LSTM\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, batch_first=True) # (batch, seq_len, input_size)\n",
    "\n",
    "        # attention: hidden state at each time step => scalar score\n",
    "        self.attn = nn.Linear(hidden_size, 1)\n",
    "\n",
    "        # context vector => scalar\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, input_size)\n",
    "        # lstm_out: (batch, seq_len, hidden_size)\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x) \n",
    "\n",
    "        # attention score\n",
    "        attn_scores = self.attn(lstm_out).squeeze(-1) # (batch, seq_len)\n",
    "        attn_weights = torch.softmax(attn_scores, dim=1) # (batch, seq_len)\n",
    "\n",
    "        # weighted sum context vector\n",
    "        context = torch.bmm(attn_weights.unsqueeze(1), lstm_out).squeeze(1) # (batch, hidden_size)\n",
    "\n",
    "        # predict next polarity\n",
    "        out = self.fc(context).squeeze(-1) # (batch,)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86bbcb08-a1f1-4a30-93bd-7f71c73a0b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LSTM] Epoch 1/10 - loss=0.194283\n",
      "[LSTM] Epoch 2/10 - loss=0.193981\n",
      "[LSTM] Epoch 3/10 - loss=0.193918\n",
      "[LSTM] Epoch 4/10 - loss=0.193867\n",
      "[LSTM] Epoch 5/10 - loss=0.193814\n",
      "[LSTM] Epoch 6/10 - loss=0.193843\n",
      "[LSTM] Epoch 7/10 - loss=0.193816\n",
      "[LSTM] Epoch 8/10 - loss=0.193814\n",
      "[LSTM] Epoch 9/10 - loss=0.193798\n",
      "[LSTM] Epoch 10/10 - loss=0.193774\n"
     ]
    }
   ],
   "source": [
    "# Both models were trained for 10 epochs using the Adam optimizer (learning rate was set to 0.001), with a batch size of 32 and mean squared error (MSE) loss.\n",
    "\n",
    "model_lstm = LSTMAttention().to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model_lstm.parameters(), lr=lr)\n",
    "\n",
    "# DataLoader\n",
    "X_tensor = torch.from_numpy(X).unsqueeze(-1) # (num_sequences, WINDOW_SIZE, 1)\n",
    "y_tensor = torch.from_numpy(y) # (num_sequences,)\n",
    "\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, generator=generator)\n",
    "\n",
    "# train\n",
    "model_lstm.train()\n",
    "train_start = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for batch_x, batch_y in loader:\n",
    "        batch_x = batch_x.to(device) # (B, T, 1)\n",
    "        batch_y = batch_y.to(device) # (B,)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        preds = model_lstm(batch_x) # (B,)\n",
    "        loss = criterion(preds, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item() * batch_x.size(0)\n",
    "\n",
    "    epoch_loss /= len(dataset)\n",
    "    print(f\"[LSTM] Epoch {epoch+1}/{num_epochs} - loss={epoch_loss:.6f}\")\n",
    "\n",
    "train_end = time.time()\n",
    "print(f\"\\nTotal training time: {(train_end - train_start):.2f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "732c0f9f-d91e-4f3a-947d-bca21b34adf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num anomalous LSTM sequences: 9045\n"
     ]
    }
   ],
   "source": [
    "# Anomalies were identified when prediction errors exceeded a threshold set at the 95th percentile, highlighting sudden or extreme shifts (changes) in sentiment.\n",
    "\n",
    "model_lstm.eval()\n",
    "with torch.no_grad():\n",
    "    X_tensor_full = torch.from_numpy(X).unsqueeze(-1).to(device)\n",
    "    preds_full = model_lstm(X_tensor_full).cpu().numpy() # (num_sequences,)\n",
    "\n",
    "true_targets = y # (num_sequences,)\n",
    "pred_errors = (true_targets - preds_full) ** 2 # (num_sequences,)\n",
    "\n",
    "# 95th percentile threshold\n",
    "pred_threshold = np.quantile(pred_errors, 0.95)\n",
    "df[\"lstm_prediction_error\"] = np.nan\n",
    "df.loc[target_indices, \"lstm_prediction_error\"] = pred_errors\n",
    "df[\"lstm_is_anomaly\"] = df[\"lstm_prediction_error\"] > pred_threshold\n",
    "\n",
    "# summary\n",
    "anomalous_seq_mask = pred_errors > pred_threshold\n",
    "print(\"num anomalous LSTM sequences:\", anomalous_seq_mask.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9039d85-8ead-451d-814e-1d14ad7d98ec",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4d5372-6fee-415e-b2d1-28d4256a973b",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1f714c-5899-4cd1-b315-67cde478e34d",
   "metadata": {},
   "source": [
    "\"Sentiment analysis was applied to a total of 189,626 English-language tweets with polarity scores ranging from −1 (most negative) to +1 (most positive). The tweets captured diverse emotional responses and reactions, including varying levels of public concern, panic, support and empathy during the crisis. The overall polarity indicates a generally negative sentiment of −0.28. Despite the dominance of negative sentiments in crisisrelated tweets, expressions of positivity were evident, reflecting community solidarity and supportive engagements alongside signals of distress.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3046f4-64d8-4254-94ed-0ccc57f62583",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"overall polarity = {df['sentiment_polarity'].mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db50e61a-742e-4613-bf47-a4ac4b547e5b",
   "metadata": {},
   "source": [
    "\"A visualization of daily average sentiment polarity scores reveals distinct temporal variations, illustrating fluctuations within predominantly negative sentiment. Sentiment scores showed clear peaks and fluctuations, reflecting public reactions as the crisis unfolded in real time. Periods exhibiting sharp declines suggest intensified negative emotional responses, perhaps triggered by escalating events or worsening news, while periods of relatively less negative sentiment may correspond to phases of crisis stabilization or expressions of community solidarity and relief.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff577bb-4312-4292-aec9-e627d73dd031",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "daily_df = df.groupby(df[\"date\"].dt.date)[\"sentiment_polarity\"].mean()\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(daily_df.index, daily_df.values, marker='o', linewidth=2)\n",
    "\n",
    "plt.title(\"Daily Average Sentiment Polarity Score Over Time\", fontsize=16)\n",
    "plt.xlabel(\"Date\", fontsize=12)\n",
    "plt.ylabel(\"Average Polarity Score\", fontsize=12)\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52495f4f-baee-47c9-8c95-3c4641dfbbd4",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a4aacb-81fa-471b-b917-a43c6a147d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TBA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2b12d0-922e-4c93-9d53-45c30cc8031f",
   "metadata": {},
   "source": [
    "## LSTM with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d134e39d-92d9-4edd-a645-a06572b18377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TBA"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V6E1",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
