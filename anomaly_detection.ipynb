{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2751f992",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "- [Dataset](https://www.kaggle.com/datasets/swaptr/turkey-earthquake-tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c00789c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/turkey_syria_earthquake_tweets/tweets.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# NOTE: unzip ./data/turkey_syria_earthquake_tweets/archive.zip before running the below code. `tweets.csv` file is too big.\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m full_data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./data/turkey_syria_earthquake_tweets/tweets.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m full_data\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/crisisbench-dl/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/crisisbench-dl/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/miniconda3/envs/crisisbench-dl/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/crisisbench-dl/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/miniconda3/envs/crisisbench-dl/lib/python3.10/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/turkey_syria_earthquake_tweets/tweets.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# NOTE: unzip ./data/turkey_syria_earthquake_tweets/archive.zip before running the below code. `tweets.csv` file is too big.\n",
    "full_data = pd.read_csv(\"./data/turkey_syria_earthquake_tweets/tweets.csv\")\n",
    "full_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21a6824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "478052\n",
      "189626\n"
     ]
    }
   ],
   "source": [
    "# TODO: filter dataset to only include English tweets => 189,626 tweets\n",
    "\n",
    "# 1) Filtering English language: by using pandas, a Python data analysis library, in the language column, only rows where the language field had the value “en” were filtered.\n",
    "# This step was necessary to increase the reliability of the pre-trained BERT model for sentiment analysis [ 36 ]. \n",
    "# After this filtering, 189,626 tweets out of 472,399 tweets were filtered as English text.\n",
    "\n",
    "print(len(full_data))\n",
    "en_filtered_data = full_data[full_data[\"language\"] == \"en\"]\n",
    "print(len(en_filtered_data))\n",
    "en_filtered_data.to_csv(\"./data/turkey_syria_earthquake_tweets/tweets_en.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4e11d15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>like_count</th>\n",
       "      <th>rt_count</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>isVerified</th>\n",
       "      <th>language</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>place</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-21 03:29:07+00:00</td>\n",
       "      <td>New search &amp;amp; rescue work is in progress in...</td>\n",
       "      <td>['Hatay', 'earthquakes', 'Türkiye', 'TurkiyeQu...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5697.0</td>\n",
       "      <td>True</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Twitter Web App</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-21 03:29:04+00:00</td>\n",
       "      <td>Can't imagine those who still haven't recovere...</td>\n",
       "      <td>['Turkey', 'earthquake', 'turkeyearthquake2023...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Twitter for Android</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-02-21 03:28:06+00:00</td>\n",
       "      <td>its a highkey sign for all of us to ponder ove...</td>\n",
       "      <td>['turkeyearthquake2023', 'earthquake', 'Syria']</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Twitter for Android</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>2023-02-21 03:27:27+00:00</td>\n",
       "      <td>See how strong was the #Earthquake of Feb 20, ...</td>\n",
       "      <td>['Earthquake', 'Hatay', 'Turkey', 'turkeyearth...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21836.0</td>\n",
       "      <td>True</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Twitter for Android</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>2023-02-21 03:27:11+00:00</td>\n",
       "      <td>More difficult news today on top of struggles ...</td>\n",
       "      <td>['Türkiye', 'Syria', 'earthquake', 'Canadians']</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>675.0</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                       date  \\\n",
       "0           1  2023-02-21 03:29:07+00:00   \n",
       "1           2  2023-02-21 03:29:04+00:00   \n",
       "2           3  2023-02-21 03:28:06+00:00   \n",
       "3           5  2023-02-21 03:27:27+00:00   \n",
       "4           6  2023-02-21 03:27:11+00:00   \n",
       "\n",
       "                                             content  \\\n",
       "0  New search &amp; rescue work is in progress in...   \n",
       "1  Can't imagine those who still haven't recovere...   \n",
       "2  its a highkey sign for all of us to ponder ove...   \n",
       "3  See how strong was the #Earthquake of Feb 20, ...   \n",
       "4  More difficult news today on top of struggles ...   \n",
       "\n",
       "                                            hashtags  like_count  rt_count  \\\n",
       "0  ['Hatay', 'earthquakes', 'Türkiye', 'TurkiyeQu...         1.0       0.0   \n",
       "1  ['Turkey', 'earthquake', 'turkeyearthquake2023...         0.0       0.0   \n",
       "2    ['turkeyearthquake2023', 'earthquake', 'Syria']         0.0       0.0   \n",
       "3  ['Earthquake', 'Hatay', 'Turkey', 'turkeyearth...         0.0       0.0   \n",
       "4    ['Türkiye', 'Syria', 'earthquake', 'Canadians']         1.0       0.0   \n",
       "\n",
       "   followers_count  isVerified language coordinates place               source  \n",
       "0           5697.0        True       en         NaN   NaN      Twitter Web App  \n",
       "1              1.0       False       en         NaN   NaN  Twitter for Android  \n",
       "2              3.0       False       en         NaN   NaN  Twitter for Android  \n",
       "3          21836.0        True       en         NaN   NaN  Twitter for Android  \n",
       "4            675.0       False       en         NaN   NaN   Twitter for iPhone  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/turkey_syria_earthquake_tweets/tweets_en.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f74ca052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Text lowercasing: all tweets were converted to lowercase; according to Hickman\n",
    "# et al. [37 ], lowercasing tends to be beneficial because it reduces data dimensionality,\n",
    "# thereby increasing statistical power, and usually does not reduce validity.\n",
    "df['content'] = df['content'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5800ed7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/jade/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /Users/jade/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>like_count</th>\n",
       "      <th>rt_count</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>isVerified</th>\n",
       "      <th>language</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>place</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-21 03:29:07+00:00</td>\n",
       "      <td>new search &amp; amp ; rescue work progress # hata...</td>\n",
       "      <td>['Hatay', 'earthquakes', 'Türkiye', 'TurkiyeQu...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5697.0</td>\n",
       "      <td>True</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Twitter Web App</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-21 03:29:04+00:00</td>\n",
       "      <td>ca n't imagine still n't recovered previous tr...</td>\n",
       "      <td>['Turkey', 'earthquake', 'turkeyearthquake2023...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Twitter for Android</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-02-21 03:28:06+00:00</td>\n",
       "      <td>highkey sign us ponder actions return merciful...</td>\n",
       "      <td>['turkeyearthquake2023', 'earthquake', 'Syria']</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Twitter for Android</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>2023-02-21 03:27:27+00:00</td>\n",
       "      <td>see strong # earthquake feb 20 , 2023 # hatay ...</td>\n",
       "      <td>['Earthquake', 'Hatay', 'Turkey', 'turkeyearth...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21836.0</td>\n",
       "      <td>True</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Twitter for Android</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>2023-02-21 03:27:11+00:00</td>\n",
       "      <td>difficult news today top struggles already fac...</td>\n",
       "      <td>['Türkiye', 'Syria', 'earthquake', 'Canadians']</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>675.0</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                       date  \\\n",
       "0           1  2023-02-21 03:29:07+00:00   \n",
       "1           2  2023-02-21 03:29:04+00:00   \n",
       "2           3  2023-02-21 03:28:06+00:00   \n",
       "3           5  2023-02-21 03:27:27+00:00   \n",
       "4           6  2023-02-21 03:27:11+00:00   \n",
       "\n",
       "                                             content  \\\n",
       "0  new search & amp ; rescue work progress # hata...   \n",
       "1  ca n't imagine still n't recovered previous tr...   \n",
       "2  highkey sign us ponder actions return merciful...   \n",
       "3  see strong # earthquake feb 20 , 2023 # hatay ...   \n",
       "4  difficult news today top struggles already fac...   \n",
       "\n",
       "                                            hashtags  like_count  rt_count  \\\n",
       "0  ['Hatay', 'earthquakes', 'Türkiye', 'TurkiyeQu...         1.0       0.0   \n",
       "1  ['Turkey', 'earthquake', 'turkeyearthquake2023...         0.0       0.0   \n",
       "2    ['turkeyearthquake2023', 'earthquake', 'Syria']         0.0       0.0   \n",
       "3  ['Earthquake', 'Hatay', 'Turkey', 'turkeyearth...         0.0       0.0   \n",
       "4    ['Türkiye', 'Syria', 'earthquake', 'Canadians']         1.0       0.0   \n",
       "\n",
       "   followers_count  isVerified language coordinates place               source  \n",
       "0           5697.0        True       en         NaN   NaN      Twitter Web App  \n",
       "1              1.0       False       en         NaN   NaN  Twitter for Android  \n",
       "2              3.0       False       en         NaN   NaN  Twitter for Android  \n",
       "3          21836.0        True       en         NaN   NaN  Twitter for Android  \n",
       "4            675.0       False       en         NaN   NaN   Twitter for iPhone  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Stop word removal: common English (function) words such as “and”, “is”, “I”, “am”,\n",
    "# “what”, “of”, etc. were removed by using the Natural Language Toolkit (NLTK).\n",
    "# Stop word removal has the advantages of reducing the size of the stored dataset and\n",
    "# improving the overall efficiency and effectiveness of the analysis [38].\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Get English stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to remove stop words from text\n",
    "def remove_stopwords(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    \n",
    "    # Tokenize the text\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Remove stop words and return as string\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    \n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "df['content'] = df['content'].apply(remove_stopwords)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71530407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. URLs removal: all URLs were removed from tweets, since the text of URL strings does\n",
    "# not necessarily convey any relevant information, and can therefore be removed [39].\n",
    "\n",
    "import re\n",
    "df['content'] = df['content'].str.replace(r'http\\S+', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f7b55b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Duplicate removal: all duplicate tweets were removed to eliminate redundancy and\n",
    "# possible skewing of the results.\n",
    "\n",
    "df = df.drop_duplicates(subset='content', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5866eb9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>like_count</th>\n",
       "      <th>rt_count</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>isVerified</th>\n",
       "      <th>language</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-02-21 03:29:07+00:00</td>\n",
       "      <td>new search &amp; amp ; rescue work progress # hata...</td>\n",
       "      <td>['Hatay', 'earthquakes', 'Türkiye', 'TurkiyeQu...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5697.0</td>\n",
       "      <td>True</td>\n",
       "      <td>en</td>\n",
       "      <td>Twitter Web App</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-02-21 03:29:04+00:00</td>\n",
       "      <td>ca n't imagine still n't recovered previous tr...</td>\n",
       "      <td>['Turkey', 'earthquake', 'turkeyearthquake2023...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>Twitter for Android</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-02-21 03:28:06+00:00</td>\n",
       "      <td>highkey sign us ponder actions return merciful...</td>\n",
       "      <td>['turkeyearthquake2023', 'earthquake', 'Syria']</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>Twitter for Android</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-02-21 03:27:27+00:00</td>\n",
       "      <td>see strong # earthquake feb 20 , 2023 # hatay ...</td>\n",
       "      <td>['Earthquake', 'Hatay', 'Turkey', 'turkeyearth...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21836.0</td>\n",
       "      <td>True</td>\n",
       "      <td>en</td>\n",
       "      <td>Twitter for Android</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-02-21 03:27:11+00:00</td>\n",
       "      <td>difficult news today top struggles already fac...</td>\n",
       "      <td>['Türkiye', 'Syria', 'earthquake', 'Canadians']</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>675.0</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        date  \\\n",
       "0  2023-02-21 03:29:07+00:00   \n",
       "1  2023-02-21 03:29:04+00:00   \n",
       "2  2023-02-21 03:28:06+00:00   \n",
       "3  2023-02-21 03:27:27+00:00   \n",
       "4  2023-02-21 03:27:11+00:00   \n",
       "\n",
       "                                             content  \\\n",
       "0  new search & amp ; rescue work progress # hata...   \n",
       "1  ca n't imagine still n't recovered previous tr...   \n",
       "2  highkey sign us ponder actions return merciful...   \n",
       "3  see strong # earthquake feb 20 , 2023 # hatay ...   \n",
       "4  difficult news today top struggles already fac...   \n",
       "\n",
       "                                            hashtags  like_count  rt_count  \\\n",
       "0  ['Hatay', 'earthquakes', 'Türkiye', 'TurkiyeQu...         1.0       0.0   \n",
       "1  ['Turkey', 'earthquake', 'turkeyearthquake2023...         0.0       0.0   \n",
       "2    ['turkeyearthquake2023', 'earthquake', 'Syria']         0.0       0.0   \n",
       "3  ['Earthquake', 'Hatay', 'Turkey', 'turkeyearth...         0.0       0.0   \n",
       "4    ['Türkiye', 'Syria', 'earthquake', 'Canadians']         1.0       0.0   \n",
       "\n",
       "   followers_count  isVerified language               source  \n",
       "0           5697.0        True       en      Twitter Web App  \n",
       "1              1.0       False       en  Twitter for Android  \n",
       "2              3.0       False       en  Twitter for Android  \n",
       "3          21836.0        True       en  Twitter for Android  \n",
       "4            675.0       False       en   Twitter for iPhone  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exclude location info (96% of the tweets lacked geolocation)\n",
    "# drop if exists\n",
    "df = df.drop(columns=['coordinates', 'place', 'Unnamed: 0'], errors='ignore') \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c75d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: assign sentiment labels using pre-trained BERT sentiment model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a653ab",
   "metadata": {},
   "source": [
    "# Neural Network Models\n",
    "\n",
    "- Sentiment Analysis\n",
    "  - pre-trained transformer-based `BERT` model\n",
    "\n",
    "- Anomaly Detection\n",
    "  - `autoencoder`\n",
    "  - `LSTM with Attention`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7180724",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "- `nlptown/bert-base-multilingual-uncased-sentiment` :  fine-tuned version of `bert-base-multilingual-uncased`, which is optimized for sentiment analysis across six languages: English, Dutch, German, French, Spanish and Italian.\n",
    "- Reference: Lakhanpal, S.; Gupta, A.; Agrawal, R. Leveraging Explainable AI to Analyze Researchers’ Aspect-Based Sentiment About ChatGPT. In Proceedings of the 15th International Conference on Intelligent Human Computer Interaction (IHCI 2023), Daegu, Republic of Korea, 8–10 November 2023; pp. 281–290.\n",
    "\n",
    "- Can be seen as part of preprocessing???\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bde8d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Tweets were tokenized using the AutoTokenizer from HuggingFace Transformers, truncated to a maximum length of 512 tokens [41].\n",
    "\n",
    "# TODO: The model predicted sentiment scores across five classes representing very negative to very positive sentiments.\n",
    "# These categorical outputs were then converted to a continuous polarity scale ranging from −1 (strongly negative) to +1 (strongly positive) to facilitate the temporal analysis of sentiment fluctuations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a523e0",
   "metadata": {},
   "source": [
    "## Anomaly Detection\n",
    "\n",
    "- `autoencoder`\n",
    "  - An autoencoder neural network was designed and trained to detect anomalies based on deviations in tweet sentiment patterns.\n",
    "  - The input data was structured into sequences of polarity scores. \n",
    "  - The autoencoder was implemented as a fully connected feedforward network with a three-layer encoder and symmetric decoder.\n",
    "  - The encoder consisted of a hidden layer with 64 neurons followed by a 16-neuron bottleneck, using rectified linear unit (ReLU) activations for encoding and decoding [ 42 ]. \n",
    "  - Reconstruction errors (mean squared error between actual and reconstructed sequences) were calculated, and tweets with errors above the 95th percentile threshold were flagged as anomalies. \n",
    "\n",
    "- `LSTM with Attention`\n",
    "  - An LSTM neural network with an integrated attention mechanism was implemented to detect anomalies based on prediction errors.\n",
    "  - Input sequences of polarity scores were processed through LSTM layers, and attention layers were applied to selectively weigh temporal dependencies within the sequences.\n",
    "  - The LSTM with attention included a single-layer LSTM model with a hidden size of 32, followed by an attention mechanism.\n",
    "\n",
    "- Common config\n",
    "  - Both models were trained for 10 epochs using the Adam optimizer (learning rate was set to 0.001), with a batch size of 32 and mean squared error (MSE) loss. \n",
    "  - Sentiment polarity scores were normalized using MinMax scaling to the [0,1] range. The model’s output was a prediction of subsequent sentiment scores.\n",
    "  - Anomalies were identified when prediction errors exceeded a threshold set at the 95th percentile, highlighting sudden or extreme shifts (changes) in sentiment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crisisbench-dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
