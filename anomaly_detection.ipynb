{"cells":[{"cell_type":"markdown","id":"2751f992","metadata":{"id":"2751f992"},"source":["# Data Preprocessing\n","\n","- [Dataset](https://www.kaggle.com/datasets/swaptr/turkey-earthquake-tweets)"]},{"cell_type":"code","source":[],"metadata":{"id":"rY2-qYLMEn2T"},"id":"rY2-qYLMEn2T","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":1,"id":"c00789c8","metadata":{"id":"c00789c8","colab":{"base_uri":"https://localhost:8080/","height":365},"collapsed":true,"executionInfo":{"status":"error","timestamp":1762268303360,"user_tz":300,"elapsed":1099,"user":{"displayName":"Jade Kim","userId":"13751362502685406790"}},"outputId":"ff74e3cc-cde7-40fa-a9c5-b64b76cb40c3"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: './data/turkey_syria_earthquake_tweets/tweets.csv'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3848397465.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# NOTE: unzip ./data/turkey_syria_earthquake_tweets/archive.zip before running the below code. `tweets.csv` file is too big.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfull_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data/turkey_syria_earthquake_tweets/tweets.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mfull_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/turkey_syria_earthquake_tweets/tweets.csv'"]}],"source":["import pandas as pd\n","\n","# NOTE: unzip ./data/turkey_syria_earthquake_tweets/archive.zip before running the below code. `tweets.csv` file is too big.\n","full_data = pd.read_csv(\"./data/turkey_syria_earthquake_tweets/tweets.csv\")\n","full_data.head()"]},{"cell_type":"code","source":["# optional block for mounting to google drive directory (needed when running things in colab)\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qxQQzdMgj2lA","executionInfo":{"status":"ok","timestamp":1762294694410,"user_tz":300,"elapsed":555,"user":{"displayName":"Jade Kim","userId":"13751362502685406790"}},"outputId":"e2da3c33-0630-40ee-f292-88110b74fbc8"},"id":"qxQQzdMgj2lA","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# optional block for mounting to google drive directory\n","import os\n","import pandas as pd\n","import numpy as np\n","import sys\n","\n","GOOGLE_DRIVE_PATH_POST_MYDRIVE = 'dl-twitter-crisis'\n","GOOGLE_DRIVE_PATH = os.path.join('/content', 'drive', 'MyDrive', GOOGLE_DRIVE_PATH_POST_MYDRIVE)\n","print(os.listdir(GOOGLE_DRIVE_PATH))\n","\n","if 'google.colab' in sys.modules:\n","  print(f'Running in google colab. Our path is `{GOOGLE_DRIVE_PATH}`')\n","else:\n","  GOOGLE_DRIVE_PATH = '.'\n","  print('Running locally.')\n","\n","print(GOOGLE_DRIVE_PATH)\n","sys.path.append(GOOGLE_DRIVE_PATH)\n","df = pd.read_csv(GOOGLE_DRIVE_PATH + \"/data/turkey_syria_earthquake_tweets/tweets_en.csv\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lyy12Lqwln-n","executionInfo":{"status":"ok","timestamp":1762294698036,"user_tz":300,"elapsed":2077,"user":{"displayName":"Jade Kim","userId":"13751362502685406790"}},"outputId":"3a61b6bc-d4d9-4be3-8418-10e84862783d"},"id":"Lyy12Lqwln-n","execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["['anomaly_detection.ipynb', '.gitignore', 'README.md', 'environment.yml', '.DS_Store', 'paper', '.git', 'data']\n","Running in google colab. Our path is `/content/drive/MyDrive/dl-twitter-crisis`\n","/content/drive/MyDrive/dl-twitter-crisis\n"]}]},{"cell_type":"code","execution_count":null,"id":"f21a6824","metadata":{"id":"f21a6824"},"outputs":[],"source":["# NOTE: commenting this out as we already have the csv file\n","\n","# 1) Filtering English language: by using pandas, a Python data analysis library, in the language column, only rows where the language field had the value ‚Äúen‚Äù were filtered.\n","# This step was necessary to increase the reliability of the pre-trained BERT model for sentiment analysis [ 36 ].\n","# After this filtering, 189,626 tweets out of 472,399 tweets were filtered as English text.\n","\n","# print(len(full_data))\n","# en_filtered_data = full_data[full_data[\"language\"] == \"en\"]\n","# print(len(en_filtered_data))\n","# en_filtered_data.to_csv(GOOGLE_DRIVE_PATH + \"./data/turkey_syria_earthquake_tweets/tweets_en.csv\")"]},{"cell_type":"code","execution_count":null,"id":"a4e11d15","metadata":{"id":"a4e11d15"},"outputs":[],"source":["# skip if using google colab\n","df = pd.read_csv(\"./data/turkey_syria_earthquake_tweets/tweets_en.csv\")\n","df.head()"]},{"cell_type":"code","execution_count":4,"id":"f74ca052","metadata":{"id":"f74ca052","executionInfo":{"status":"ok","timestamp":1762294699522,"user_tz":300,"elapsed":153,"user":{"displayName":"Jade Kim","userId":"13751362502685406790"}}},"outputs":[],"source":["# 2. Text lowercasing: all tweets were converted to lowercase; according to Hickman\n","# et al. [37 ], lowercasing tends to be beneficial because it reduces data dimensionality,\n","# thereby increasing statistical power, and usually does not reduce validity.\n","df['content'] = df['content'].str.lower()"]},{"cell_type":"code","execution_count":5,"id":"5800ed7d","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5800ed7d","executionInfo":{"status":"ok","timestamp":1762294724071,"user_tz":300,"elapsed":21153,"user":{"displayName":"Jade Kim","userId":"13751362502685406790"}},"outputId":"d3dc1c32-711a-4e81-d3c0-d607c1c5439d"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n"]}],"source":["# 3. Stop word removal: common English (function) words such as ‚Äúand‚Äù, ‚Äúis‚Äù, ‚ÄúI‚Äù, ‚Äúam‚Äù,\n","# ‚Äúwhat‚Äù, ‚Äúof‚Äù, etc. were removed by using the Natural Language Toolkit (NLTK).\n","# Stop word removal has the advantages of reducing the size of the stored dataset and\n","# improving the overall efficiency and effectiveness of the analysis [38].\n","\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","import re\n","\n","# Download required NLTK data\n","nltk.download('stopwords')\n","nltk.download('punkt_tab')\n","\n","# Get English stop words\n","stop_words = set(stopwords.words('english'))\n","\n","# Function to remove stop words from text\n","def remove_stopwords(text):\n","    if pd.isna(text):\n","        return text\n","\n","    # Tokenize the text\n","    words = word_tokenize(text)\n","\n","    # Remove stop words and return as string\n","    filtered_words = [word for word in words if word.lower() not in stop_words]\n","\n","    return ' '.join(filtered_words)\n","\n","df['content'] = df['content'].apply(remove_stopwords)\n"]},{"cell_type":"code","execution_count":6,"id":"71530407","metadata":{"id":"71530407","executionInfo":{"status":"ok","timestamp":1762294724216,"user_tz":300,"elapsed":142,"user":{"displayName":"Jade Kim","userId":"13751362502685406790"}}},"outputs":[],"source":["# 4. URLs removal: all URLs were removed from tweets, since the text of URL strings does\n","# not necessarily convey any relevant information, and can therefore be removed [39].\n","\n","import re\n","df['content'] = df['content'].str.replace(r'http\\S+', '', regex=True)"]},{"cell_type":"code","execution_count":7,"id":"6f7b55b3","metadata":{"id":"6f7b55b3","executionInfo":{"status":"ok","timestamp":1762294724300,"user_tz":300,"elapsed":81,"user":{"displayName":"Jade Kim","userId":"13751362502685406790"}}},"outputs":[],"source":["# 5. Duplicate removal: all duplicate tweets were removed to eliminate redundancy and\n","# possible skewing of the results.\n","\n","df = df.drop_duplicates(subset='content', keep='first')"]},{"cell_type":"code","execution_count":8,"id":"5866eb9e","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":293},"id":"5866eb9e","executionInfo":{"status":"ok","timestamp":1762294724336,"user_tz":300,"elapsed":33,"user":{"displayName":"Jade Kim","userId":"13751362502685406790"}},"outputId":"254ef374-e187-4e3e-db31-5541da746211"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                        date  \\\n","0  2023-02-21 03:29:07+00:00   \n","1  2023-02-21 03:29:04+00:00   \n","2  2023-02-21 03:28:06+00:00   \n","3  2023-02-21 03:27:27+00:00   \n","4  2023-02-21 03:27:11+00:00   \n","\n","                                             content  \\\n","0  new search & amp ; rescue work progress # hata...   \n","1  ca n't imagine still n't recovered previous tr...   \n","2  highkey sign us ponder actions return merciful...   \n","3  see strong # earthquake feb 20 , 2023 # hatay ...   \n","4  difficult news today top struggles already fac...   \n","\n","                                            hashtags  like_count  rt_count  \\\n","0  ['Hatay', 'earthquakes', 'T√ºrkiye', 'TurkiyeQu...         1.0       0.0   \n","1  ['Turkey', 'earthquake', 'turkeyearthquake2023...         0.0       0.0   \n","2    ['turkeyearthquake2023', 'earthquake', 'Syria']         0.0       0.0   \n","3  ['Earthquake', 'Hatay', 'Turkey', 'turkeyearth...         0.0       0.0   \n","4    ['T√ºrkiye', 'Syria', 'earthquake', 'Canadians']         1.0       0.0   \n","\n","   followers_count  isVerified language               source  \n","0           5697.0        True       en      Twitter Web App  \n","1              1.0       False       en  Twitter for Android  \n","2              3.0       False       en  Twitter for Android  \n","3          21836.0        True       en  Twitter for Android  \n","4            675.0       False       en   Twitter for iPhone  "],"text/html":["\n","  <div id=\"df-7c0c6523-5831-409a-bcc4-66068aaef49c\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>date</th>\n","      <th>content</th>\n","      <th>hashtags</th>\n","      <th>like_count</th>\n","      <th>rt_count</th>\n","      <th>followers_count</th>\n","      <th>isVerified</th>\n","      <th>language</th>\n","      <th>source</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2023-02-21 03:29:07+00:00</td>\n","      <td>new search &amp; amp ; rescue work progress # hata...</td>\n","      <td>['Hatay', 'earthquakes', 'T√ºrkiye', 'TurkiyeQu...</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>5697.0</td>\n","      <td>True</td>\n","      <td>en</td>\n","      <td>Twitter Web App</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2023-02-21 03:29:04+00:00</td>\n","      <td>ca n't imagine still n't recovered previous tr...</td>\n","      <td>['Turkey', 'earthquake', 'turkeyearthquake2023...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>False</td>\n","      <td>en</td>\n","      <td>Twitter for Android</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2023-02-21 03:28:06+00:00</td>\n","      <td>highkey sign us ponder actions return merciful...</td>\n","      <td>['turkeyearthquake2023', 'earthquake', 'Syria']</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>3.0</td>\n","      <td>False</td>\n","      <td>en</td>\n","      <td>Twitter for Android</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2023-02-21 03:27:27+00:00</td>\n","      <td>see strong # earthquake feb 20 , 2023 # hatay ...</td>\n","      <td>['Earthquake', 'Hatay', 'Turkey', 'turkeyearth...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>21836.0</td>\n","      <td>True</td>\n","      <td>en</td>\n","      <td>Twitter for Android</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2023-02-21 03:27:11+00:00</td>\n","      <td>difficult news today top struggles already fac...</td>\n","      <td>['T√ºrkiye', 'Syria', 'earthquake', 'Canadians']</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>675.0</td>\n","      <td>False</td>\n","      <td>en</td>\n","      <td>Twitter for iPhone</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7c0c6523-5831-409a-bcc4-66068aaef49c')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-7c0c6523-5831-409a-bcc4-66068aaef49c button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-7c0c6523-5831-409a-bcc4-66068aaef49c');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-4f4114ff-f7db-42d9-ba9c-157a615873b6\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4f4114ff-f7db-42d9-ba9c-157a615873b6')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-4f4114ff-f7db-42d9-ba9c-157a615873b6 button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df"}},"metadata":{},"execution_count":8}],"source":["# exclude location info (96% of the tweets lacked geolocation)\n","# drop if exists\n","df = df.drop(columns=['coordinates', 'place', 'Unnamed: 0'], errors='ignore')\n","df.head()"]},{"cell_type":"markdown","id":"50a653ab","metadata":{"id":"50a653ab"},"source":["# Neural Network Models\n","\n","- Sentiment Analysis\n","  - pre-trained transformer-based `BERT` model\n","\n","- Anomaly Detection\n","  - `autoencoder`\n","  - `LSTM with Attention`"]},{"cell_type":"markdown","id":"c7180724","metadata":{"id":"c7180724"},"source":["## Sentiment Analysis\n","- `nlptown/bert-base-multilingual-uncased-sentiment` :  fine-tuned version of `bert-base-multilingual-uncased`, which is optimized for sentiment analysis across six languages: English, Dutch, German, French, Spanish and Italian.\n","- Reference: Lakhanpal, S.; Gupta, A.; Agrawal, R. Leveraging Explainable AI to Analyze Researchers‚Äô Aspect-Based Sentiment About ChatGPT. In Proceedings of the 15th International Conference on Intelligent Human Computer Interaction (IHCI 2023), Daegu, Republic of Korea, 8‚Äì10 November 2023; pp. 281‚Äì290.\n","\n","- Can be seen as part of preprocessing???\n"]},{"cell_type":"code","execution_count":9,"id":"20c75d25","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"20c75d25","executionInfo":{"status":"ok","timestamp":1762294735008,"user_tz":300,"elapsed":10640,"user":{"displayName":"Jade Kim","userId":"13751362502685406790"}},"outputId":"7e06ead2-ed3e-4af0-faa0-90a0aeee00be","collapsed":true},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.10.23)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n","Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.9.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/jax/_src/cloud_tpu_init.py:86: UserWarning: Transparent hugepages are not enabled. TPU runtime startup and shutdown time should be significantly improved on TPU v5e and newer. If not already set, you may need to enable transparent hugepages in your VM image (sudo sh -c \"echo always > /sys/kernel/mm/transparent_hugepage/enabled\")\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]}],"source":["# Tokenize inputs\n","!pip install -U transformers\n","\n","from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n","import torch\n","\n","# pipe = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n","tokenizer = AutoTokenizer.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n","model = AutoModelForSequenceClassification.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n","\n","inputs = df['content'].tolist()\n","\n","inputs = inputs[:2500]\n","model_inputs = tokenizer(inputs, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")"]},{"cell_type":"code","source":["# Call BERT and get predicted labels\n","outputs = model(**model_inputs)\n","logits = outputs.logits\n","probabilities = torch.softmax(logits, dim=-1)\n","predicted_labels = torch.argmax(probabilities, dim=-1)\n","\n","# Convert to polarity scale as stated in the paper\n","star_ratings = predicted_labels + 1\n","polarity_scores = (star_ratings - 3) / 2.0\n","# Now each tweet has a sentiment polarity ‚àà [-1, +1]\n","# Note that I had to create a subset because of resource limitations\n","df_subset = df.iloc[:2500].copy()\n","df_subset['sentiment_polarity'] = polarity_scores.numpy()\n","\n","\n","# clean and normalize polarity data\n","pol = df_subset['sentiment_polarity'].astype(float).copy()\n","pol = pol.interpolate(limit_direction=\"both\")  # fill occasional gaps\n","pol_mean, pol_std = pol.mean(), pol.std() if pol.std() > 0 else 1.0\n","pol_norm = (pol - pol_mean) / pol_std\n"],"metadata":{"id":"d-XBPpxs99DR","executionInfo":{"status":"ok","timestamp":1762294778119,"user_tz":300,"elapsed":43109,"user":{"displayName":"Jade Kim","userId":"13751362502685406790"}},"collapsed":true},"id":"d-XBPpxs99DR","execution_count":10,"outputs":[]},{"cell_type":"code","source":["# Create Autoencoder neural network\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader, random_split\n","\n","# hyperparameters\n","SEQ_LEN   = 64\n","STRIDE    = 1\n","BATCH_SIZE= 128\n","EPOCHS    = 20\n","LR        = 1e-3\n","DEVICE    = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","RNG       = np.random.default_rng(42)\n","\n"],"metadata":{"id":"70xxm9V0bYT2","executionInfo":{"status":"ok","timestamp":1762294778134,"user_tz":300,"elapsed":4,"user":{"displayName":"Jade Kim","userId":"13751362502685406790"}}},"id":"70xxm9V0bYT2","execution_count":11,"outputs":[]},{"cell_type":"code","source":["# construct input data as sequences of polarity scores. Note that I used GPT\n","# for this section\n","\n","series = pol_norm.to_numpy().astype(np.float32)\n","N = len(series)\n","windows = []\n","indices = []  # store ending index of each window for mapping anomalies back\n","for start in range(0, N - SEQ_LEN + 1, STRIDE):\n","    end = start + SEQ_LEN\n","    windows.append(series[start:end])\n","    indices.append(end - 1)  # align anomaly decision at window end\n","X = np.stack(windows, axis=0)  # (num_windows, SEQ_LEN)\n","idx_map = np.array(indices)\n","\n","class SeqDataset(Dataset):\n","    def __init__(self, X):\n","        self.X = torch.from_numpy(X)\n","    def __len__(self):\n","        return self.X.shape[0]\n","    def __getitem__(self, i):\n","        x = self.X[i]\n","        return x, x  # autoencoder: input == target\n","\n","dataset = SeqDataset(X)\n","\n","# train/test split the dataset\n","val_size = int(0.2 * len(dataset))\n","train_size = len(dataset) - val_size\n","train_ds, val_ds = random_split(dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n","train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n","val_dl   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)"],"metadata":{"id":"yWs_nrwH5VNv","executionInfo":{"status":"ok","timestamp":1762294778138,"user_tz":300,"elapsed":2,"user":{"displayName":"Jade Kim","userId":"13751362502685406790"}}},"id":"yWs_nrwH5VNv","execution_count":12,"outputs":[]},{"cell_type":"code","source":["# declare AutoEncoder class with layers specified in the paper\n","class AE(nn.Module):\n","    def __init__(self, in_dim=SEQ_LEN, h1=128, h2=64, bottleneck=16):\n","        super().__init__()\n","        self.encoder = nn.Sequential(\n","            nn.Linear(in_dim, h1),\n","            nn.ReLU(),\n","            nn.Linear(h1, h2),\n","            nn.ReLU(),\n","            nn.Linear(h2, bottleneck),\n","            nn.ReLU()\n","        )\n","        self.decoder = nn.Sequential(\n","            nn.Linear(bottleneck, h2),\n","            nn.ReLU(),\n","            nn.Linear(h2, h1),\n","            nn.ReLU(),\n","            nn.Linear(h1, in_dim)  # final layer (no activation for regression)\n","        )\n","    def forward(self, x):\n","        z = self.encoder(x)\n","        out = self.decoder(z)\n","        return out\n","\n","model = AE(in_dim=SEQ_LEN, h1=128, h2=64, bottleneck=16).to(DEVICE)\n","criterion = nn.MSELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=LR)"],"metadata":{"id":"vG0ZSCXkuS-3","executionInfo":{"status":"ok","timestamp":1762294778142,"user_tz":300,"elapsed":1,"user":{"displayName":"Jade Kim","userId":"13751362502685406790"}}},"id":"vG0ZSCXkuS-3","execution_count":13,"outputs":[]},{"cell_type":"code","source":["# train loop\n","def run_epoch(dl, train=True):\n","    model.train(train)\n","    total, count = 0.0, 0\n","    for xb, yb in dl:\n","        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n","        if train:\n","            optimizer.zero_grad(set_to_none=True)\n","        preds = model(xb)\n","        loss = criterion(preds, yb)\n","        if train:\n","            loss.backward()\n","            optimizer.step()\n","        total += loss.item() * xb.size(0)\n","        count += xb.size(0)\n","    return total / max(count, 1)\n","\n","best_val = float(\"inf\")\n","patience, bad = 5, 0 # used to prevent overfitting, tracks if validation loss stops decreasing or starts increasing\n","for epoch in range(1, EPOCHS+1):\n","    tr = run_epoch(train_dl, train=True)\n","    va = run_epoch(val_dl, train=False)\n","    print(f\"Epoch {epoch:02d} | train MSE: {tr:.6f} | val MSE: {va:.6f}\")\n","    if va + 1e-6 < best_val:\n","        best_val = va\n","        bad = 0\n","        best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n","    else:\n","        bad += 1\n","        if bad >= patience:\n","            print(\"Early stopping.\")\n","            break\n","\n","# Restore best model\n","model.load_state_dict(best_state)\n","model.to(DEVICE)\n","model.eval()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"UmsAK2mCn-2l","executionInfo":{"status":"ok","timestamp":1762294778715,"user_tz":300,"elapsed":571,"user":{"displayName":"Jade Kim","userId":"13751362502685406790"}},"outputId":"e4b09677-1115-4c26-8f85-2141d5ab6148"},"id":"UmsAK2mCn-2l","execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 01 | train MSE: 0.986858 | val MSE: 0.978194\n","Epoch 02 | train MSE: 0.975218 | val MSE: 0.961202\n","Epoch 03 | train MSE: 0.944968 | val MSE: 0.920235\n","Epoch 04 | train MSE: 0.896460 | val MSE: 0.878052\n","Epoch 05 | train MSE: 0.853866 | val MSE: 0.846852\n","Epoch 06 | train MSE: 0.821692 | val MSE: 0.824133\n","Epoch 07 | train MSE: 0.798710 | val MSE: 0.810321\n","Epoch 08 | train MSE: 0.785143 | val MSE: 0.804307\n","Epoch 09 | train MSE: 0.771090 | val MSE: 0.791600\n","Epoch 10 | train MSE: 0.753499 | val MSE: 0.780109\n","Epoch 11 | train MSE: 0.737869 | val MSE: 0.769325\n","Epoch 12 | train MSE: 0.723167 | val MSE: 0.756426\n","Epoch 13 | train MSE: 0.706743 | val MSE: 0.746167\n","Epoch 14 | train MSE: 0.693631 | val MSE: 0.741966\n","Epoch 15 | train MSE: 0.682895 | val MSE: 0.732714\n","Epoch 16 | train MSE: 0.669968 | val MSE: 0.723534\n","Epoch 17 | train MSE: 0.658414 | val MSE: 0.714663\n","Epoch 18 | train MSE: 0.647138 | val MSE: 0.710504\n","Epoch 19 | train MSE: 0.637369 | val MSE: 0.701968\n","Epoch 20 | train MSE: 0.624954 | val MSE: 0.692304\n"]},{"output_type":"execute_result","data":{"text/plain":["AE(\n","  (encoder): Sequential(\n","    (0): Linear(in_features=64, out_features=128, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=128, out_features=64, bias=True)\n","    (3): ReLU()\n","    (4): Linear(in_features=64, out_features=16, bias=True)\n","    (5): ReLU()\n","  )\n","  (decoder): Sequential(\n","    (0): Linear(in_features=16, out_features=64, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=64, out_features=128, bias=True)\n","    (3): ReLU()\n","    (4): Linear(in_features=128, out_features=64, bias=True)\n","  )\n",")"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["# compute recon errors\n","with torch.no_grad():\n","    X_tensor = torch.from_numpy(X).to(DEVICE)\n","    recon = model(X_tensor)\n","    mse = ((recon - X_tensor) ** 2).mean(dim=1).detach().cpu().numpy()  # per-window MSE\n","\n","# 95th percentile threshold\n","threshold = np.percentile(mse, 95.0)\n","\n","# Flag anomalies: windows with error above threshold\n","is_anom = mse > threshold\n","\n","# Map window anomalies back to tweet-level indices\n","anom_indices = idx_map[is_anom]  # indices in the original df (row positions)\n","df_subset['ae_recon_error'] = np.nan\n","df_subset.loc[idx_map, 'ae_recon_error'] = mse  # assign errors to window-end rows\n","df_subset['ae_anomaly_95p'] = False\n","df_subset.loc[anom_indices, 'ae_anomaly_95p'] = True\n","\n","\n","print(f\"Detected anomalies: {is_anom.sum()} / {len(is_anom)} windows\")\n","anomalies = df_subset.loc[df_subset['ae_anomaly_95p'] == True, ['content', 'sentiment_polarity']]\n","# non_anomalies = df_subset.loc[df_subset['ae_anomaly_95p'] == False, ['content', 'sentiment_polarity']]\n","print(anomalies.head(10))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3ne-kXE27LL-","executionInfo":{"status":"ok","timestamp":1762295616630,"user_tz":300,"elapsed":16,"user":{"displayName":"Jade Kim","userId":"13751362502685406790"}},"outputId":"3f7ae055-a465-4386-f697-c11f1a3e38bf"},"id":"3ne-kXE27LL-","execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Threshold (95th pct): 1.322654\n","Detected anomalies: 122 / 2437 windows\n","                                               content  sentiment_polarity\n","166  @ lefkosaturkbld , present information disaste...                -1.0\n","168  africa also reported splitting two . # turkey ...                -1.0\n","317  heart aching feel helpless # helpsyria # syria...                -1.0\n","319  idlib health directorate : dozens emergency ca...                -1.0\n","320  üîî # earthquake ( # deprem ) m2.7 occurred 20 k...                -1.0\n","329  best option choose btw ‚û°Ô∏è # livrma match  : //...                 1.0\n","336  footage shaking adana city hospital magnitude ...                -1.0\n","339  # almayadeen 's correspondent # aleppo said 3 ...                -1.0\n","356  friend vietnam predicted another # earthquake ...                -1.0\n","628  interior minister soylu : 3 people lost lives ...                -1.0\n","                                             content  sentiment_polarity\n","0  new search & amp ; rescue work progress # hata...                 0.5\n","1  ca n't imagine still n't recovered previous tr...                -1.0\n","2  highkey sign us ponder actions return merciful...                -1.0\n","3  see strong # earthquake feb 20 , 2023 # hatay ...                -1.0\n","4  difficult news today top struggles already fac...                -1.0\n","5  another # earthquake southern # turkey .  : //...                -1.0\n","6  three people confirmed died 6.4-magnitude # ea...                -1.0\n","7  6.4 magnitude quake shakes turkey -syria borde...                -1.0\n","8  new 6.3 # earthquake hits # turkey , # syria b...                -1.0\n","9  latest # earthquake # turkey impact # disaster...                -1.0\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"4vfrrhCwmcw6"},"id":"4vfrrhCwmcw6"},{"cell_type":"markdown","source":[],"metadata":{"id":"ikBMCRoemcuT"},"id":"ikBMCRoemcuT"},{"cell_type":"markdown","source":[],"metadata":{"id":"B_r_He9PmbB5"},"id":"B_r_He9PmbB5"},{"cell_type":"markdown","id":"d0a523e0","metadata":{"id":"d0a523e0"},"source":["## Anomaly Detection\n","\n","- `autoencoder`\n","  - An autoencoder neural network was designed and trained to detect anomalies based on deviations in tweet sentiment patterns.\n","  - The input data was structured into sequences of polarity scores.\n","  - The autoencoder was implemented as a fully connected feedforward network with a three-layer encoder and symmetric decoder.\n","  - The encoder consisted of a hidden layer with 64 neurons followed by a 16-neuron bottleneck, using rectified linear unit (ReLU) activations for encoding and decoding [ 42 ].\n","  - Reconstruction errors (mean squared error between actual and reconstructed sequences) were calculated, and tweets with errors above the 95th percentile threshold were flagged as anomalies.\n","\n","- `LSTM with Attention`\n","  - An LSTM neural network with an integrated attention mechanism was implemented to detect anomalies based on prediction errors.\n","  - Input sequences of polarity scores were processed through LSTM layers, and attention layers were applied to selectively weigh temporal dependencies within the sequences.\n","  - The LSTM with attention included a single-layer LSTM model with a hidden size of 32, followed by an attention mechanism.\n","\n","- Common config\n","  - Both models were trained for 10 epochs using the Adam optimizer (learning rate was set to 0.001), with a batch size of 32 and mean squared error (MSE) loss.\n","  - Sentiment polarity scores were normalized using MinMax scaling to the [0,1] range. The model‚Äôs output was a prediction of subsequent sentiment scores.\n","  - Anomalies were identified when prediction errors exceeded a threshold set at the 95th percentile, highlighting sudden or extreme shifts (changes) in sentiment."]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.19"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"V6E1"},"accelerator":"TPU"},"nbformat":4,"nbformat_minor":5}