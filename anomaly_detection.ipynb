{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2751f992",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21a6824",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: filter dataset to only include English tweets => 189,626 tweets\n",
    "\n",
    "# 1. Filtering English language: by using pandas, a Python data analysis library, in the\n",
    "language column, only rows where the language field had the value “en” were filtered.\n",
    "This step was necessary to increase the reliability of the pre-trained BERT model for\n",
    "sentiment analysis [ 36 ]. After this filtering, 189,626 tweets out of 472,399 tweets were\n",
    "filtered as English text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74ca052",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 2. Text lowercasing: all tweets were converted to lowercase; according to Hickman\n",
    "# et al. [37 ], lowercasing tends to be beneficial because it reduces data dimensionality,\n",
    "# thereby increasing statistical power, and usually does not reduce validity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5800ed7d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 3. Stop word removal: common English (function) words such as “and”, “is”, “I”, “am”,\n",
    "# “what”, “of”, etc. were removed by using the Natural Language Toolkit (NLTK).\n",
    "# Stop word removal has the advantages of reducing the size of the stored dataset and\n",
    "# improving the overall efficiency and effectiveness of the analysis [38]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71530407",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 4. URLs removal: all URLs were removed from tweets, since the text of URL strings does\n",
    "# not necessarily convey any relevant information, and can therefore be removed [39]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7b55b3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 5. Duplicate removal: all duplicate tweets were removed to eliminate redundancy and\n",
    "# possible skewing of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5866eb9e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: exclude location info (96% of the tweets lacked geolocation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c75d25",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: assign sentiment labels using pre-trained BERT sentiment model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a653ab",
   "metadata": {},
   "source": [
    "# Neural Network Models\n",
    "\n",
    "- Sentiment Analysis\n",
    "  - pre-trained transformer-based `BERT` model\n",
    "\n",
    "- Anomaly Detection\n",
    "  - `autoencoder`\n",
    "  - `LSTM with Attention`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7180724",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "- `nlptown/bert-base-multilingual-uncased-sentiment` :  fine-tuned version of `bert-base-multilingual-uncased`, which is optimized for sentiment analysis across six languages: English, Dutch, German, French, Spanish and Italian.\n",
    "- Reference: Lakhanpal, S.; Gupta, A.; Agrawal, R. Leveraging Explainable AI to Analyze Researchers’ Aspect-Based Sentiment About ChatGPT. In Proceedings of the 15th International Conference on Intelligent Human Computer Interaction (IHCI 2023), Daegu, Republic of Korea, 8–10 November 2023; pp. 281–290.\n",
    "\n",
    "- Can be seen as part of preprocessing???\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bde8d06",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Tweets were tokenized using the AutoTokenizer from HuggingFace Transformers, truncated to a maximum length of 512 tokens [41].\n",
    "\n",
    "# TODO: The model predicted sentiment scores across five classes representing very negative to very positive sentiments.\n",
    "# These categorical outputs were then converted to a continuous polarity scale ranging from −1 (strongly negative) to +1 (strongly positive) to facilitate the temporal analysis of sentiment fluctuations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a523e0",
   "metadata": {},
   "source": [
    "## Anomaly Detection\n",
    "\n",
    "- `autoencoder`\n",
    "  - An autoencoder neural network was designed and trained to detect anomalies based on deviations in tweet sentiment patterns.\n",
    "  - The input data was structured into sequences of polarity scores. \n",
    "  - The autoencoder was implemented as a fully connected feedforward network with a three-layer encoder and symmetric decoder.\n",
    "  - The encoder consisted of a hidden layer with 64 neurons followed by a 16-neuron bottleneck, using rectified linear unit (ReLU) activations for encoding and decoding [ 42 ]. \n",
    "  - Reconstruction errors (mean squared error between actual and reconstructed sequences) were calculated, and tweets with errors above the 95th percentile threshold were flagged as anomalies. \n",
    "\n",
    "- `LSTM with Attention`\n",
    "  - An LSTM neural network with an integrated attention mechanism was implemented to detect anomalies based on prediction errors.\n",
    "  - Input sequences of polarity scores were processed through LSTM layers, and attention layers were applied to selectively weigh temporal dependencies within the sequences.\n",
    "  - The LSTM with attention included a single-layer LSTM model with a hidden size of 32, followed by an attention mechanism.\n",
    "\n",
    "- Common config\n",
    "  - Both models were trained for 10 epochs using the Adam optimizer (learning rate was set to 0.001), with a batch size of 32 and mean squared error (MSE) loss. \n",
    "  - Sentiment polarity scores were normalized using MinMax scaling to the [0,1] range. The model’s output was a prediction of subsequent sentiment scores.\n",
    "  - Anomalies were identified when prediction errors exceeded a threshold set at the 95th percentile, highlighting sudden or extreme shifts (changes) in sentiment."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
